Task: 针对 C2 的 2.1 PyTorch backend selection，设计并提交实验

Output:
## 实验设计完成

### 实验设计
- **目的**：验证 PyTorch SDPA backend selection 的边界条件，特别是 `head_dim % 8 == 0` 对 FlashAttention 可用性的影响
- **配置**：
  - 测试 head_dims: 104-128 范围（覆盖 PaLU 压缩后常见的 114-125）
  - 4 个 backend 对比: AUTO, FLASH, MEM_EFFICIENT, MATH
  - 2 种 shape 配置: `(4, 2048, 32)` 和 `(1, 4096, 32)`
  - warmup=50, measure=200, trials=3
- **预期结果**：
  - `head_dim % 8 == 0` (112, 120, 128) → FlashAttention 可用
  - `head_dim % 8 != 0` (113-119, 121-127) → FlashAttention 不可用，回退到 MEM_EFFICIENT 或 MATH

### 创建的文件
1. **实验配置**: `experiments/night_sweep.yaml` - 添加了 `C21_backend_selection` 实验
2. **实验运行器**: `src/experiment_runner.py` - 添加了 `run_c21_backend_selection()` 函数
3. **Slurm 脚本**: `slurm/run_c21_backend_selection.sbatch`

### 提交结果
- **Job ID**: 18421
- **预计运行时间**: ~1-2 小时
- **结果路径**: `results/C21/`

### 下一步
作业完成后需要分析：
1. 每个 head_dim 实际选择的 backend
2. 确认 `head_dim % 8 != 0` 时 Flash 不可用的假设
3. 比较各 backend 在不同维度下的性能差距
4. 更新 findings.yaml 中的 C2_probe.layer_2_1_pytorch 发现


