# Literature Search Results
# 由 Literature Agent 维护

# 搜索历史
searches:
  - date: "2026-01-27"
    topic: "technical_verification"
    query: "FlashAttention head dimension alignment requirements"
    findings:
      - title: "FlashAttention Head Dimension Support"
        source: "https://github.com/Dao-AILab/flash-attention"
        relevance: "核心技术验证 - 确认我们论文的 head_dim 约束声明"
        key_points:
          - "FlashAttention-2 支持所有 head_dim <= 256"
          - "head_dim > 192 backward 需要 A100/A800 或 H100/H800"
          - "vLLM/xformers 限制特定维度: [64, 80, 96, 112, 128, 256]"
          - "某些构建要求 head_dim 必须是 32 的倍数"

      - title: "PyTorch SDPA Backend Selection"
        source: "https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html"
        relevance: "核心技术验证 - SDPA fallback 到 Math backend 的条件"
        key_points:
          - "三种 backend: FlashAttention-2, Memory-Efficient (xformers), Math"
          - "自动选择基于硬件、输入形状、数据类型"
          - "head_dim 必须是 8 的倍数 (fp16/bf16) 或 4 的倍数 (fp32)"
          - "不满足 Flash/Efficient 约束时 fallback 到 Math"
          - "Math backend 性能差约 40x (87ms vs 2.3ms 示例)"

      - title: "NVIDIA Tensor Core Alignment Requirements"
        source: "https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html"
        relevance: "核心技术验证 - Tensor Core 对齐要求"
        key_points:
          - "cuBLAS < 11.0: 维度必须是 8 的倍数才能用 Tensor Core"
          - "cuBLAS >= 11.0: 任意维度可用，但倍数性能更好"
          - "A100 最优: 维度是 64 的倍数 (128 bytes / 2 bytes per fp16)"
          - "mma.sync.aligned.m16n8k16 是 A100 常用指令"
          - "不对齐导致 tile/wave quantization，性能下降 1.5-2x"
    action_items:
      - "引用 NVIDIA Matrix Multiplication Guide"
      - "在论文中明确说明 SDPA backend fallback 条件"
      - "添加 vLLM 支持的 head_dim 列表作为参考"

  - date: "2026-01-27"
    topic: "competitive_analysis"
    query: "LLM compression methods latency memory tradeoff"
    findings:
      - title: "GPTQ Quantization"
        source: "https://arxiv.org/abs/2210.17323"
        relevance: "主流量化方法，与我们的 SVD 压缩互补"
        key_points:
          - "Layer-wise post-training quantization"
          - "使用 Hessian-based optimization"
          - "4-bit 量化保持大部分精度"
          - "不改变维度，不产生 dimensional collapse"

      - title: "AWQ (Activation-aware Weight Quantization)"
        source: "https://github.com/mit-han-lab/llm-awq"
        relevance: "MLSys 2024 Best Paper，高效量化方法"
        key_points:
          - "只有 1% weights 是 salient"
          - "RTX 4090 上 2.7x speedup vs FP16"
          - "支持 <4-bit 量化"
          - "同样不改变维度结构"

      - title: "Palu: KV-Cache Compression with Low-Rank Projection"
        source: "https://arxiv.org/abs/2407.21118"
        relevance: "最相关竞争方法 - 同样使用 SVD，会产生 irregular dimensions"
        key_points:
          - "ICLR 2025 paper"
          - "使用 SVD 分解: W = UΣV^T"
          - "50% KV-Cache 压缩，up to 1.89x speedup"
          - "group_size=4 的 G-LRD 方案"
          - "SVD 引入 outliers，影响量化"
          - "使用 Walsh-Hadamard transform 消除 outliers"
          - "没有讨论 irregular dimension 导致的 GPU 性能问题"

      - title: "SVD-LLM"
        source: "https://arxiv.org/abs/2403.07378"
        relevance: "另一个 SVD 压缩方法"
        key_points:
          - "ICLR 2025 paper"
          - "Truncation-aware SVD"
          - "压缩 weight matrices"
          - "Palu 使用其 SVD 分解方法"
    action_items:
      - "在 Related Work 中对比 GPTQ/AWQ vs SVD approaches"
      - "强调 Palu 没有考虑 dimensional collapse 问题"
      - "引用 SVD-LLM 作为 truncation-aware SVD 的来源"

  # ===== 2026-01-28 最新文献调研 =====
  - date: "2026-01-28"
    topic: "related_work_comprehensive"
    query: "LLM compression SVD low-rank, FlashAttention head dimension, Tensor Core alignment"
    purpose: "系统性文献调研，支持 Related Work 撰写"
    findings:
      # ===== SVD-Based LLM 压缩方法 =====
      - title: "SVD-LLM: Truncation-aware Singular Value Decomposition"
        source: "https://arxiv.org/abs/2403.07378"
        venue: "ICLR 2025"
        authors: "Xin Wang, Yu Zheng, Zhongwei Wan, Mi Zhang"
        relevance: "核心相关工作 - SVD 压缩方法"
        key_points:
          - "Data whitening 技术确保 singular values 直接映射到 compression loss"
          - "Sequential low-rank approximation 补偿 accuracy degradation"
          - "解决了两个核心问题: (1) 小奇异值截断不一定 loss 最小 (2) 截断后缺乏权重更新"
          - "高压缩率下优于 ASVD 等方法"
          - "在 10 datasets, 7 models, 3 LLM families 上验证"
          - "未讨论 irregular dimension 导致的 GPU 性能问题"

      - title: "SVD-LLM V2: Optimizing Singular Value Truncation"
        source: "https://arxiv.org/abs/2503.12340"
        venue: "NAACL 2025"
        relevance: "SVD-LLM 改进版本"
        key_points:
          - "计算每个 weight matrix 的 theoretical truncation loss"
          - "为每个 weight matrix 分配 unique compression ratio"
          - "解决了 homogeneous compression 导致的高 truncation loss 问题"
          - "不同层使用不同压缩率"

      - title: "Fisher-Aligned Subspace Compression (FASC)"
        source: "https://arxiv.org/abs/2601.07197"
        venue: "arXiv 2026"
        relevance: "SVD 的替代方法，使用 Fisher information"
        key_points:
          - "SVD 假设 activation variance = importance (可能不正确)"
          - "FASC 使用 gradient information 识别关键维度"
          - "在 50% rank reduction 下保留 6-8% 更多 accuracy"
          - "7B 模型可达到 13B 模型的 factual recall"

      - title: "Dobi-SVD: Differentiable SVD for LLM Compression"
        source: "https://arxiv.org/abs/2502.02723"
        venue: "arXiv 2025"
        relevance: "可微分 SVD 方法"
        key_points:
          - "可微分优化 truncation positions"
          - "压缩率 0.4 时保持 40% accuracy (ASVD/SVD-LLM 只有 29-31%)"

      - title: "ResSVD: Residual Compensated SVD"
        source: "https://arxiv.org/abs/2505.20112"
        venue: "arXiv 2025"
        relevance: "残差补偿 SVD"
        key_points:
          - "解决现有方法忽略 residual matrix 的问题"
          - "减少 truncation loss"

      # ===== KV Cache 压缩 =====
      - title: "Palu: KV-Cache Compression with Low-Rank Projection"
        source: "https://arxiv.org/abs/2407.21118"
        venue: "ICLR 2025"
        authors: "Chi-Chih Chang, Wei-Cheng Lin, Chien-Yu Lin, et al."
        relevance: "最相关竞争方法 - 同样使用 SVD，会产生 irregular dimensions"
        key_points:
          - "使用 truncation-aware SVD 分解 KV projection matrices"
          - "50% KV-Cache 压缩，up to 1.89x speedup (RoPE attention)"
          - "64K 序列 + 4-bit quantization: 6.17x speedup over FP16"
          - "Medium-grained group-head low-rank decomposition (G-LRD)"
          - "使用 Walsh-Hadamard transform 消除 SVD 引入的 outliers"
          - "论文未讨论 irregular dimension 的 GPU 性能影响"
          - "代码开源: https://github.com/shadowpa0327/Palu"

      - title: "KVQuant: Towards 10 Million Context Length LLM Inference"
        source: "https://arxiv.org/abs/2401.18079"
        venue: "NeurIPS 2024"
        relevance: "KV cache 量化方法"
        key_points:
          - "3-bit KV cache quantization"
          - "自定义 CUDA kernels 实现 ~1.7x speedup"
          - "支持 10M context length"
          - "不改变维度，不产生 dimensional collapse"

      - title: "KV Cache Compression for Inference Efficiency in LLMs: A Review"
        source: "https://arxiv.org/abs/2508.06297"
        venue: "arXiv 2025"
        relevance: "KV cache 压缩综述"
        key_points:
          - "分类: Selective Token, Quantization, Layer-wise, Attention-Aware"
          - "核心问题: KV cache 随 sequence length 线性增长"
          - "低秩方法是主要研究方向之一"

      # ===== FlashAttention 和 GPU Attention 优化 =====
      - title: "FlashAttention: Fast and Memory-Efficient Exact Attention"
        source: "https://openreview.net/pdf?id=H4DqfPSibmx"
        venue: "NeurIPS 2022"
        authors: "Tri Dao et al."
        relevance: "核心 baseline，IO-aware attention"
        key_points:
          - "利用 GPU 内存层次结构减少 HBM 访问"
          - "线性内存复杂度 (vs 二次)"
          - "2-4x speedup vs optimized baselines"
          - "head_dim <= 128"

      - title: "FlashAttention-2: Faster Attention with Better Parallelism"
        source: "https://arxiv.org/abs/2307.08691"
        venue: "ICLR 2024"
        authors: "Tri Dao"
        relevance: "核心 baseline，我们实验的主要参照"
        key_points:
          - "减少 non-matmul FLOPs"
          - "改进 parallelization across thread blocks"
          - "改进 warp 间工作分配"
          - "比 FlashAttention-1/xformers 快约 2x"
          - "A100 上可达理论 FLOPs 的 50-73%"
          - "支持 head_dim <= 256"
          - "head_dim > 192 backward 需要 A100/H100"

      - title: "FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision"
        source: "https://arxiv.org/abs/2407.08608"
        venue: "NeurIPS 2024 Spotlight"
        authors: "Jay Shah et al."
        relevance: "最新 FlashAttention 版本，Hopper 优化"
        key_points:
          - "针对 H100 GPU 的异步和低精度优化"
          - "三种技术: warp-specialization, interleaved matmul/softmax, FP8 quantization"
          - "BF16: 840 TFLOPs/s (85% utilization)"
          - "FP8: 1.3 PFLOPs/s, 2.6x lower numerical error"
          - "比 FlashAttention-2 快 1.5-2x"

      # ===== PyTorch SDPA =====
      - title: "PyTorch SDPA Backend Selection"
        source: "https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html"
        venue: "PyTorch 官方文档"
        relevance: "核心技术验证 - backend fallback"
        key_points:
          - "四种 backend: FlashAttention, Memory-Efficient, Math, cuDNN"
          - "自动选择基于硬件、输入形状、数据类型"
          - "Flash 要求: head_dim % 8 == 0, head_dim <= 128 (built-in)"
          - "Efficient 要求: head_dim % 8 == 0 (fp16) / % 4 (fp32)"
          - "不满足约束自动 fallback 到 Math backend"
          - "Math backend 比 Flash 慢约 40x (benchmark: 87ms vs 2.3ms)"

      # ===== Tensor Core 和 GEMM 优化 =====
      - title: "NVIDIA Deep Learning Performance Guide - Matrix Multiplication"
        source: "https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html"
        venue: "NVIDIA 官方文档"
        relevance: "核心技术验证 - Tensor Core 对齐"
        key_points:
          - "TF32: 倍数 4 最优"
          - "FP16: 倍数 8 最优 (16 bytes)"
          - "INT8: 倍数 16 最优"
          - "A100 最优: 64 elements (128 bytes)"
          - "cuBLAS >= 11.0: 放宽硬性要求但效率仍受影响"
          - "Tile quantization: 不对齐可导致 1.5x 额外操作"
          - "Wave quantization: 可导致 GFLOPS 减半"
          - "WMMA 标准 tile: 16×16×16"

      - title: "The Power of 8: Getting the most out of Tensor Cores"
        source: "https://medium.com/@michael.diggin/the-power-of-8-getting-the-most-out-of-tensor-cores-c7704ae0c5c1"
        venue: "Medium Article"
        relevance: "通俗解释 Tensor Core 对齐"
        key_points:
          - "不规则矩阵维度无法达到最优 GPU 利用率"
          - "维度对齐是 GEMM 性能的关键"
          - "旧版 cuBLAS 要求 16 bytes 对齐才能使用 Tensor Cores"
          - "新版放宽但性能仍受影响"

      - title: "TMA-Adaptive FP8 Grouped GEMM"
        source: "https://arxiv.org/abs/2508.16584"
        venue: "arXiv 2025"
        relevance: "最新消除 padding 的研究 (Hopper)"
        key_points:
          - "Hopper TMA 约束: 16-byte global, 128-byte shared alignment"
          - "消除 padding 到固定 alignment 的需求"
          - "针对 MoE 的动态 group sizes"
          - "23.8% memory overhead 减少"
          - "K mod 16 = 0 满足基本对齐"

      # ===== 量化方法 (对比 baseline) =====
      - title: "GPTQ: Accurate Post-Training Quantization"
        source: "https://arxiv.org/abs/2210.17323"
        venue: "ICLR 2023"
        authors: "Elias Frantar et al."
        relevance: "量化 baseline，不产生 dimensional collapse"
        key_points:
          - "Layer-wise post-training quantization"
          - "Hessian-based optimization"
          - "4-bit 量化保持大部分精度"
          - "不改变维度结构"
          - "Perplexity: ~6.90 (4-bit)"

      - title: "AWQ: Activation-aware Weight Quantization"
        source: "https://github.com/mit-han-lab/llm-awq"
        venue: "MLSys 2024 Best Paper"
        authors: "Ji Lin et al."
        relevance: "量化 baseline，与 SVD 方法对比"
        key_points:
          - "只有 1% weights 是 salient，保护这些权重"
          - "RTX 4090 上 2.7x speedup vs FP16"
          - "Perplexity: ~6.84 (4-bit), 优于 GPTQ"
          - "不改变维度结构"
          - "AWQ vs GPTQ: AWQ 在 coding tasks 更好 (51.83% vs 46%)"

      - title: "Marlin: GPTQ/AWQ Optimized Kernel"
        source: "https://github.com/IST-DASLab/marlin"
        venue: "IST Austria"
        relevance: "量化加速 kernel"
        key_points:
          - "同样的 GPTQ 权重，2.6x speedup (712 vs 276 tok/s)"
          - "说明 kernel 优化的重要性"

      # ===== LLM 推理分析 =====
      - title: "FlashDecoding++: Faster LLM Inference on GPUs"
        source: "https://proceedings.mlsys.org/paper_files/paper/2024/file/5321b1dabcd2be188d796c21b733e8c7-Paper-Conference.pdf"
        venue: "MLSys 2024"
        authors: "Ke Hong et al."
        relevance: "说明 GEMM shape 敏感性"
        key_points:
          - "单一静态 dataflow 可导致 50.25% 性能损失"
          - "不同 GEMM shapes 需要不同 dataflow"
          - "异步 softmax + double buffering"
          - "NVIDIA GPU 上 4.86x speedup"
          - "支持我们的论点: GEMM shape 对性能至关重要"

      - title: "vLLM: Efficient Memory Management for LLM Serving"
        source: "https://arxiv.org/abs/2309.06180"
        venue: "SOSP 2023"
        authors: "Woosuk Kwon et al."
        relevance: "生产推理框架，head_dim 白名单"
        key_points:
          - "PagedAttention 减少 KV cache 碎片"
          - "FlashAttentionBackend 只支持: [64, 80, 96, 112, 128, 256]"
          - "不支持的 head_dim 触发 fallback 或 ValueError"
          - "说明 head_dim 约束在生产系统中的重要性"

      # ===== LoRA (低秩适配) =====
      - title: "LoRA: Low-Rank Adaptation of Large Language Models"
        source: "https://arxiv.org/abs/2106.09685"
        venue: "ICLR 2022"
        authors: "Edward J. Hu et al."
        relevance: "低秩方法的经典工作"
        key_points:
          - "训练时使用低秩矩阵 A, B"
          - "推理时可合并: W = W₀ + BA，无额外延迟"
          - "与 SVD 压缩不同: LoRA 用于微调，不改变基础模型维度"
          - "可学习 rank 通常较小 (r=4, 8, 16)"

    action_items:
      - "在 Related Work 分 3 个子节: LLM Compression, Attention Optimization, GPU Performance"
      - "强调 SVD 方法关注 accuracy preservation，忽视 dimensional collapse"
      - "引用 NVIDIA 文档说明 Tensor Core 对齐的重要性"
      - "对比我们的系统性分析与现有 ad-hoc 方案"

# 需要引用的论文 (must_cite)
papers_to_cite:
  # ===== 核心必引用 =====
  - title: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
    authors: "Tri Dao et al."
    venue: "NeurIPS 2022"
    arxiv: "2205.14135"
    relevance: "FlashAttention 原论文"
    cited: true

  - title: "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"
    authors: "Tri Dao"
    venue: "ICLR 2024"
    arxiv: "2307.08691"
    relevance: "我们实验的主要 baseline"
    cited: true

  - title: "Palu: KV-Cache Compression with Low-Rank Projection"
    authors: "Chi-Chih Chang et al."
    venue: "ICLR 2025"
    arxiv: "2407.21118"
    relevance: "最相关竞争方法，产生 irregular dimensions"
    cited: false
    note: "需要在 Related Work 重点讨论"

  - title: "SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression"
    authors: "Xin Wang et al."
    venue: "ICLR 2025"
    arxiv: "2403.07378"
    relevance: "Truncation-aware SVD 方法"
    cited: false

  - title: "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"
    authors: "Ji Lin et al."
    venue: "MLSys 2024 Best Paper"
    relevance: "量化 baseline，不产生 dimensional collapse"
    cited: false

  - title: "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"
    authors: "Elias Frantar et al."
    venue: "ICLR 2023"
    relevance: "量化 baseline"
    cited: false

  - title: "vLLM: Efficient Memory Management for Large Language Model Serving with PagedAttention"
    authors: "Woosuk Kwon et al."
    venue: "SOSP 2023"
    relevance: "生产推理框架，有 head_dim 白名单约束"
    cited: false

  - title: "FlashDecoding++: Faster Large Language Model Inference on GPUs"
    authors: "Ke Hong et al."
    venue: "MLSys 2024"
    relevance: "说明 GEMM shape 敏感性 - 50% 性能差异"
    cited: false

  - title: "LoRA: Low-Rank Adaptation of Large Language Models"
    authors: "Edward J. Hu et al."
    venue: "ICLR 2022"
    arxiv: "2106.09685"
    relevance: "低秩方法经典工作，对比 SVD 压缩"
    cited: false

  # ===== 可选引用 =====
  - title: "FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision"
    authors: "Jay Shah et al."
    venue: "NeurIPS 2024 Spotlight"
    arxiv: "2407.08608"
    relevance: "可选引用，Hopper 优化"
    cited: false

  - title: "KVQuant: Towards 10 Million Context Length LLM Inference"
    authors: "Coleman Hooper et al."
    venue: "NeurIPS 2024"
    relevance: "KV cache 量化方法"
    cited: false

  - title: "SVD-LLM V2: Optimizing Singular Value Truncation"
    authors: "Xin Wang et al."
    venue: "NAACL 2025"
    arxiv: "2503.12340"
    relevance: "SVD-LLM 改进版"
    cited: false

# 竞争方法对比数据
competitive_methods:
  - name: "GPTQ"
    type: "Quantization"
    compression: "4-bit weights"
    speedup: "2-3x (with vLLM optimizations)"
    memory_reduction: "4x (FP16 -> INT4)"
    accuracy_loss: "Perplexity +0.4 (6.5 → 6.9)"
    dimensional_collapse: false
    note: "不改变维度结构"

  - name: "AWQ"
    type: "Quantization"
    compression: "4-bit weights, protect 1% salient"
    speedup: "2.7x on RTX 4090"
    memory_reduction: "4x+"
    accuracy_loss: "Perplexity +0.34 (6.5 → 6.84)"
    dimensional_collapse: false
    note: "MLSys 2024 Best Paper，coding tasks 更好"

  - name: "Palu"
    type: "Low-rank SVD (KV-Cache)"
    compression: "50% KV-Cache"
    speedup: "1.89x (RoPE attention), 6.17x (64K + Q4)"
    memory_reduction: "2x KV-Cache"
    accuracy_loss: "Small (with WHT)"
    dimensional_collapse: true
    note: "产生 irregular dimensions，但论文未讨论 GPU 性能影响"

  - name: "SVD-LLM"
    type: "Low-rank SVD (Weights)"
    compression: "Truncated weights"
    speedup: "Varies by rank"
    memory_reduction: "Depends on rank"
    accuracy_loss: "Minimized by truncation-aware"
    dimensional_collapse: true
    note: "遵循 NVIDIA guideline round 到 8 的倍数，但缺乏系统分析"

  - name: "LoRA"
    type: "Low-rank Adaptation"
    compression: "Trainable low-rank A, B"
    speedup: "No inference overhead (merge W = W₀ + BA)"
    memory_reduction: "Checkpoint size only"
    accuracy_loss: "Task-dependent"
    dimensional_collapse: false
    note: "用于微调，不改变基础模型维度"

# 技术文档引用
technical_references:
  - source: "NVIDIA Matrix Multiplication Performance Guide"
    url: "https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html"
    topic: "GEMM alignment for Tensor Cores"
    key_info: |
      - FP16: 8 的倍数 (16 bytes)
      - A100 最优: 64 的倍数 (128 bytes)
      - cuBLAS >= 11.0 放宽硬性要求，但效率仍受影响
      - Tile quantization 可导致 1.5x 额外操作
      - Wave quantization 可导致 GFLOPS 减半

  - source: "PyTorch SDPA Documentation"
    url: "https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html"
    topic: "Backend selection logic"
    key_info: |
      - 四种 backend: Flash, Efficient, Math, cuDNN
      - 优先级: flash > efficient > math
      - head_dim 必须是 8 的倍数 (fp16) 或 4 的倍数 (fp32)
      - 不满足约束自动 fallback 到 Math
      - Math 性能比 Flash 差约 40x

  - source: "FlashAttention GitHub Repository"
    url: "https://github.com/Dao-AILab/flash-attention"
    topic: "Supported head dimensions"
    key_info: |
      - FlashAttention-2 支持所有 head_dim <= 256
      - vLLM/xformers 限制: [64, 80, 96, 112, 128, 256]
      - head_dim > 192 backward 需要 A100+
      - 某些构建要求 head_dim 是 32 的倍数

  - source: "vLLM GitHub Issues"
    url: "https://github.com/vllm-project/vllm/issues/3359"
    topic: "FlashAttention head size constraints"
    key_info: |
      - FlashAttentionBackend 受限于特定 head sizes
      - 不支持的 head_dim 触发 ValueError 或 fallback
      - Multimodal 模型的 ViT 部分常有非标准 head_dim

# 关键发现总结
key_findings:
  - finding: "FlashAttention 有明确的 head_dim 约束"
    evidence: "vLLM 限制 [64, 80, 96, 112, 128, 256]，某些构建要求 32 的倍数"
    implication: "SVD 压缩后的 irregular head_dim 可能无法使用 Flash backend"

  - finding: "PyTorch SDPA 自动 fallback 到 Math backend"
    evidence: "head_dim 不是 8 的倍数时触发 fallback，性能差 40x"
    implication: "解释了我们观察到的 SDPA 性能悬崖"

  - finding: "Tensor Core 效率与维度对齐强相关"
    evidence: "A100 最优 64 的倍数，tile/wave quantization 可导致 2x 性能损失"
    implication: "支持我们的 dimensional collapse 核心论点"

  - finding: "Palu 等 SVD 方法未讨论 dimensional collapse"
    evidence: "Palu 论文只关注 accuracy 和 memory，未提 irregular dim 的 GPU 性能影响"
    implication: "我们的论文填补了这个重要空白"

  - finding: "量化方法不产生 dimensional collapse"
    evidence: "GPTQ, AWQ 保持原始维度结构"
    implication: "SVD compression 有独特的 dimensional collapse 问题"

  - finding: "FlashDecoding++ 发现单一 GEMM dataflow 导致 50% 性能损失"
    evidence: "不同 GEMM shapes 需要不同 dataflow 策略"
    implication: "佐证我们的论点: GEMM shape (包括维度) 对性能有显著影响"

  - finding: "LoRA 推理无额外延迟因为可合并权重"
    evidence: "W = W₀ + BA 合并后推理，无需维护分解形式"
    implication: "与 SVD 压缩对比: SVD 可能保留分解形式 (U, Σ, V) 导致维度变化"

# 文献调研关键结论
literature_review_summary:
  main_finding: |
    现有 LLM 压缩文献（特别是 SVD-based 方法）主要关注 accuracy preservation 和
    memory reduction，对 dimensional collapse 导致的 GPU 性能问题缺乏系统性分析。
    我们的论文首次量化这个问题，填补了重要的研究空白。

  supporting_evidence:
    - evidence: "SVD-LLM 遵循 NVIDIA guideline 将维度 round 到 8 的倍数，但未分析不对齐的代价"
      source: "SVD-LLM paper"

    - evidence: "Palu 论文展示 1.89x speedup，但未讨论 head_dim 不规则时的性能退化"
      source: "Palu paper"

    - evidence: "vLLM 硬编码支持的 head_dim 列表，不支持的维度触发 fallback"
      source: "vLLM GitHub issues"

    - evidence: "FlashDecoding++ 发现不同 GEMM shapes 需要不同 dataflow，单一策略损失 50%"
      source: "FlashDecoding++ MLSys 2024"

    - evidence: "NVIDIA 文档明确 tile/wave quantization 可导致 1.5-2x 性能损失"
      source: "NVIDIA Deep Learning Performance Guide"

  research_gap: |
    1. SVD 压缩方法: 关注 accuracy，ad-hoc 处理维度对齐
    2. 推理框架 (vLLM, TRT-LLM): 白名单策略，缺乏灵活修复
    3. GPU 优化研究: 关注 sequence padding，未讨论 head_dim collapse
    我们的 GAC 方案填补这个空白，提供第一个系统性的维度修复策略。

# Related Work 段落建议
related_work_suggestions:
  svd_compression_methods:
    topic: "SVD-based LLM Compression Methods"
    suggested_text: |
      Recent SVD-based LLM compression methods, including SVD-LLM [Wang et al., ICLR 2025],
      Palu [Chang et al., ICLR 2025], and related approaches, have made significant progress
      in reducing model size while preserving accuracy. SVD-LLM uses truncation-aware data
      whitening to minimize compression loss, while Palu applies group-wise low-rank
      decomposition to KV-cache with up to 50% compression. However, these methods focus
      primarily on accuracy preservation and memory reduction, treating dimension alignment
      as a secondary concern. SVD-LLM follows NVIDIA's guideline to round dimensions to
      multiples of 8, but without analyzing the performance implications of different
      alignment choices. Our work reveals that this ad-hoc approach is insufficient:
      non-aligned dimensions can cause up to 2× performance degradation due to Tensor Core
      tile quantization and attention backend fallback—a phenomenon we term "dimensional collapse."
    key_citations:
      - "SVD-LLM: Truncation-aware SVD (ICLR 2025)"
      - "Palu: KV-Cache Compression with Low-Rank Projection (ICLR 2025)"

  attention_optimization:
    topic: "Attention Mechanism Optimization"
    suggested_text: |
      FlashAttention [Dao et al., NeurIPS 2022] and its successors [Dao, ICLR 2024;
      Shah et al., NeurIPS 2024] have revolutionized attention computation by exploiting
      GPU memory hierarchy. PyTorch's scaled_dot_product_attention (SDPA) integrates
      multiple backends with automatic selection based on input properties. However,
      these optimizations impose strict dimension constraints: FlashAttention requires
      head dimensions to be multiples of 8, and production frameworks like vLLM further
      restrict to specific values [64, 80, 96, 112, 128, 256]. When these constraints
      are not met, systems fall back to slower Math backends with up to 40× performance
      degradation. Our work systematically characterizes these constraints and proposes
      GAC dimension repair strategies to ensure compressed models remain compatible
      with optimized attention backends.
    key_citations:
      - "FlashAttention (NeurIPS 2022)"
      - "FlashAttention-2 (ICLR 2024)"
      - "vLLM (SOSP 2023)"

  quantization_comparison:
    topic: "Quantization vs. Low-Rank Compression"
    suggested_text: |
      Quantization methods like GPTQ [Frantar et al., ICLR 2023] and AWQ [Lin et al.,
      MLSys 2024] achieve significant compression by reducing weight precision without
      altering tensor dimensions. In contrast, SVD-based compression fundamentally
      changes the matrix structure, producing intermediate dimensions determined by
      the chosen rank. While quantization preserves dimension alignment automatically,
      SVD compression can inadvertently create irregular dimensions that violate
      Tensor Core and attention backend requirements. Our analysis shows this is a
      fundamental distinction: quantization trades precision for efficiency, while
      SVD compression trades FLOPs for memory—but may inadvertently sacrifice GPU
      efficiency through dimensional collapse.
    key_citations:
      - "GPTQ (ICLR 2023)"
      - "AWQ (MLSys 2024 Best Paper)"

# 技术规格汇总
technical_specs_summary:
  flashattention:
    supported_head_dims: "all <= 256"
    optimal_head_dims: "[64, 128]"
    vllm_whitelist: "[64, 80, 96, 112, 128, 256]"
    trtllm_whitelist: "[32, 40, 64, 80, 96, 104, 128, 160, 256]"
    common_constraint: "head_dim % 8 == 0 (some builds require % 32)"
    backward_constraint: "head_dim > 192 needs A100/H100"

  pytorch_sdpa:
    backends: ["FLASH_ATTENTION", "EFFICIENT_ATTENTION", "MATH", "CUDNN"]
    flash_constraint: "head_dim % 8 == 0, head_dim <= 128"
    efficient_constraint: "head_dim % 8 == 0"
    math_constraint: "universal fallback, supports fp64"
    fallback_penalty: "~40x slower than Flash"

  tensor_core_alignment:
    cublas_pre_11: "dims must be multiple of 16 bytes for TC"
    cublas_11_plus: "any dims work, but aligned is faster"
    a100_optimal: "dims multiple of 64 elements (128 bytes for fp16)"
    tile_quantization: "up to 1.5x overhead for misaligned"
    wave_quantization: "can halve GFLOPS"

# 最后更新时间
last_updated: "2026-01-28T16:00:00"
