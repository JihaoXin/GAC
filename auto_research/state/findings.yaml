# G-Compress 研究发现汇总
# 每次实验后由 researcher agent 更新

# C1: 量化现象
C1_quantify:
  - id: "F1.1"
    date: "2026-01-19"
    experiment: "S1_sdpa_dense_sweep"
    finding: "head_dim=107 导致 SDPA 延迟 2.147ms, 比 head_dim=96 (1.140ms) 增加 88%"
    evidence_path: "results/S1/20260119_224805_S1_sdpa_dense_sweep/"
    confidence: "high"

  - id: "F1.2"
    date: "2026-01-19"
    experiment: "S2_sdpa_backend_forced"
    finding: "head_dim=107 时 Math backend 延迟 26.995ms, Flash backend 2.139ms, 相差 12.6x"
    evidence_path: "results/S2/"
    confidence: "high"

  - id: "F1.3"
    date: "2026-01-21"
    experiment: "palu_dim_distribution"
    finding: "PaLU 压缩后 head_dim 分布不规则，大量维度不是 8/16/32 的倍数"
    evidence_path: "results/palu_dim_dist/"
    confidence: "high"

  - id: "F1.4"
    date: "2026-01-24"
    experiment: "palu_dim_distribution (extended)"
    finding: "Mistral-7B 和 Llama-3-8B 使用 PaLU r=0.8 压缩后，per-head dim 集中在 114-125 范围，其中只有 120 是 8 的倍数（~3%），其余全部不对齐"
    evidence_path: "results/palu_dim_dist/llama3_r0.8/"
    confidence: "high"
    details:
      mistral_dims: [114, 115, 116, 118, 119, 120, 121, 122, 123, 124, 125]
      llama3_dims: [114, 116, 117, 118, 120, 121, 122, 123, 124, 125]
      aligned_8_count: "1 out of 11 unique values"

  - id: "F1.5"
    date: "2026-01-19"
    experiment: "G3_gemm_k_dense"
    finding: "GEMM K 维度不对齐时性能下降，K=64-160 范围的 dense sweep 显示周期性性能波动"
    evidence_path: "results/G3/"
    confidence: "high"

  - id: "F1.6"
    date: "2026-01-19"
    experiment: "P1_padding_rescue"
    finding: "将 head_dim=107 padding 到 112 或 128 可恢复大部分性能"
    evidence_path: "results/P1/"
    confidence: "high"

# C2: 原因探究
C2_probe:
  layer_2_1_pytorch:
    - id: "F2.1.1"
      date: "2026-01-24"
      experiment: "C21_backend_selection"
      finding: "FlashAttention 在 104-128 范围内对所有维度都可用，包括非 8-aligned 维度"
      evidence_path: "results/C21/20260124_212113_C21_backend_selection/"
      confidence: "high"
      details:
        tested_dims: [104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128]
        flash_available: "50/50 (100%)"
        detected_backend: "FLASH (all dims)"
      implication: "PyTorch 后端选择不是性能差距的根因，FlashAttention 并未回退到 MATH"

    - id: "F2.1.2"
      date: "2026-01-24"
      experiment: "C21_backend_selection"
      finding: "MEM_EFFICIENT 后端仅对 8-aligned 维度可用，非 8-aligned 维度完全不可用"
      evidence_path: "results/C21/20260124_212113_C21_backend_selection/"
      confidence: "high"
      details:
        mem_efficient_8_aligned: "8/8 (100%)"
        mem_efficient_non_aligned: "0/42 (0%)"
      implication: "xFormers/MEM_EFFICIENT 有严格的 8-aligned 要求"

    - id: "F2.1.3"
      date: "2026-01-24"
      experiment: "C21_backend_selection"
      finding: "FlashAttention 内部对非 8-aligned 输入有 30-45% 的性能惩罚"
      evidence_path: "results/C21/20260124_212113_C21_backend_selection/"
      confidence: "high"
      details:
        avg_latency_8_aligned: "1.553 ms"
        avg_latency_non_aligned: "2.033 ms"
        slowdown: "+30.9%"
        palu_range_114_119:
          latency: "2.22-2.24 ms"
          vs_d120: "+38-39%"
      implication: "性能差距发生在 FlashAttention 内部，而非后端选择层"

    - id: "F2.1.4"
      date: "2026-01-24"
      experiment: "C21_backend_selection"
      finding: "非对齐维度的延迟呈阶梯状分布，与 padding 到下一个 8-aligned 的开销无关"
      evidence_path: "results/C21/20260124_212113_C21_backend_selection/"
      confidence: "high"
      details:
        example: "D=113-119 延迟约 2.23ms，远高于 D=120 的 1.61ms"
        pattern: "非对齐维度延迟几乎恒定，与具体数值无关"
      implication: "FlashAttention 对非对齐输入使用了不同的执行路径"

  layer_2_2_cuda: []

  layer_2_3_hardware:
    - id: "F2.3.1"
      date: "2026-01-24"
      experiment: "C23_hardware_layer_analysis"
      finding: "Tensor Core 利用率在 16-aligned K 维度时显著高于非对齐维度"
      evidence_path: "results/C23/20260124_220005_C23_hardware_layer/"
      confidence: "high"
      hypothesis_status: "CONFIRMED"
      details:
        gemm_shape: "8192 x 8192 x K"
        aligned_16_tflops: 91.1
        aligned_8_only_tflops: 76.8
        non_aligned_tflops: 58.1
        slowdown_non_vs_16: "58.1%"
        tc_utilization_aligned: "~30%"
        tc_utilization_non_aligned: "~12%"
        key_observation: "K=105,107,109... 等奇数维度表现最差 (~37-55 TFLOPS)"
      implication: "GEMM 内核对 K 维度有 16-alignment 优化路径，是性能差距的主因之一"
      impact: "HIGH - 58% slowdown"

    - id: "F2.3.2"
      date: "2026-01-24"
      experiment: "C23_hardware_layer_analysis"
      finding: "L2 cache sector 浪费对带宽效率影响有限 (~5.8%)，不是主要瓶颈"
      evidence_path: "results/C23/20260124_220005_C23_hardware_layer/"
      confidence: "high"
      hypothesis_status: "NOT CONFIRMED"
      details:
        aligned_16_bandwidth_gbs: 209.3
        non_aligned_bandwidth_gbs: 214.6
        avg_sector_waste_pct: 5.8
        observation: "非对齐维度的带宽反而略高，说明 sector 浪费不是主因"
      implication: "L2 cache 不是性能差距的根因"
      impact: "LOW - 5.8% waste, negligible"

    - id: "F2.3.3"
      date: "2026-01-24"
      experiment: "C23_hardware_layer_analysis"
      finding: "向量化加载模式对 GEMM 性能有显著影响"
      evidence_path: "results/C23/20260124_220005_C23_hardware_layer/"
      confidence: "high"
      hypothesis_status: "CONFIRMED"
      details:
        float4_aligned_16:
          K_values: [112, 128]
          tflops: "73-83"
        float4_aligned_8:
          K_values: [104, 120]
          tflops: "68-77"
        float2_aligned_4:
          K_values: [108, 116, 124]
          tflops: "61-71"
        scalar_non_aligned:
          K_values: [105, 107]
          tflops: "39-40"
      implication: "float4 向量化加载需要 8-aligned，非对齐维度降级为 scalar 导致性能下降"
      impact: "HIGH - 50% throughput loss"

    - id: "F2.3.4"
      date: "2026-01-24"
      experiment: "C23_hardware_layer_analysis"
      finding: "SDPA 内存带宽效率与 8-alignment 强相关"
      evidence_path: "results/C23/20260124_220005_C23_hardware_layer/"
      confidence: "high"
      hypothesis_status: "CONFIRMED"
      details:
        sdpa_d112_latency_ms: 1.529
        sdpa_d113_latency_ms: 2.208
        sdpa_d120_latency_ms: 1.571
        sdpa_d121_latency_ms: 2.142
        bandwidth_8_aligned: "153-160 GB/s"
        bandwidth_non_aligned: "107-118 GB/s"
        bandwidth_ratio: "~1.4x"
      implication: "SDPA 对非 8-aligned 维度有 ~40% 的带宽效率损失"
      impact: "HIGH - 40% efficiency loss"

  layer_2_3_summary:
    status: "COMPLETED"
    date: "2026-01-24"
    hypotheses_tested: 4
    confirmed: 3
    not_confirmed: 1
    root_causes_by_impact:
      - rank: 1
        cause: "Tensor Core tile alignment"
        impact: "58% slowdown"
        requirement: "K % 16 == 0"
      - rank: 2
        cause: "向量化加载降级"
        impact: "50% throughput loss"
        requirement: "K % 8 == 0 for float4"
      - rank: 3
        cause: "SDPA 带宽效率"
        impact: "40% efficiency loss"
        requirement: "head_dim % 8 == 0"
      - rank: 4
        cause: "L2 cache sector 浪费"
        impact: "5.8% (negligible)"
        status: "非主因"
    alignment_constraints:
      minimal: "head_dim % 8 == 0"
      optimal: "head_dim % 16 == 0"
      recommended_values: [64, 96, 112, 128]

# C3: 形式化
C3_formulate:
  - id: "F3.1"
    date: "2026-01-24"
    finding: "ShapeContract 数据结构定义完成，形式化了维度对齐约束"
    implementation: "src/gcompress_bench/dimension_repair.py"
    details:
      minimal_alignment: 8
      optimal_alignment: 16
      recommended_values: [32, 64, 96, 112, 128, 160, 192, 224, 256]
      max_overhead_pct: 20.0
    confidence: "high"

  - id: "F3.2"
    date: "2026-01-24"
    finding: "repair_dimension() 函数实现 4 种修复策略"
    implementation: "src/gcompress_bench/dimension_repair.py:repair_dimension()"
    details:
      strategies:
        minimal: "对齐到 8，最小内存开销"
        optimal: "对齐到 16，最优 Tensor Core"
        predefined: "对齐到预定义快速路径"
        tradeoff: "根据开销阈值自动选择"
    confidence: "high"

# C4: 解决方案
C4_solver:
  - id: "F4.1"
    date: "2026-01-24"
    finding: "DimensionRepairer 类实现了模型级别的维度修复"
    implementation: "src/gcompress_bench/dimension_repair.py:DimensionRepairer"
    details:
      methods:
        - analyze_model: 识别需要修复的维度
        - compute_repair_plan: 计算修复方案
        - repair_linear_layer: 修复单个 Linear 层
        - repair_model: 修复整个模型
    confidence: "high"

  - id: "F4.2"
    date: "2026-01-24"
    finding: "PaLU 压缩模型维度修复验证成功"
    experiment: "C4_dimension_repair (Job 18424)"
    status: "completed"
    evidence_path: "results/C4/20260124_221749_C4_dimension_repair/"
    details:
      palu_dims_distribution:
        total_heads: 512
        originally_aligned: 3.1%
        misaligned_pct: 96.9%
      actual_overhead:
        minimal_strategy: 3.72%
        optimal_strategy: 7.2%
      benchmark_speedups:
        dim_107: "2.064ms → 1.490ms (+27.8%)"
        dim_114: "2.049ms → 1.549ms (+24.4%)"
        dim_117: "2.054ms → 1.567ms (+23.7%)"
        dim_121: "1.964ms → 1.430ms (+27.2%)"
        dim_125: "1.975ms → 1.439ms (+27.1%)"
      key_observation: "D=120 已对齐无提升 (1.557ms)，验证了对齐假设"
    confidence: "high"

  - id: "F4.3"
    date: "2026-01-24"
    finding: "Dimension repair 算法正确性验证通过 (30/30 tests)"
    experiment: "C4_dimension_repair"
    evidence_path: "results/C4/20260124_221749_C4_dimension_repair/results.json"
    details:
      repair_dimension_tests: 22/22
      shape_contract_tests: 8/8
      palu_dimension_repairs:
        114: 120
        116: 120
        117: 120
        118: 120
        120: 120
        121: 128
        122: 128
        123: 128
        124: 128
        125: 128
    confidence: "high"

# C5: 验证
C5_validation:
  - id: "F5.1"
    date: "2026-01-25"
    experiment: "C5_e2e_comparison (Job 18425)"
    finding: "C5 端到端验证实验存在方法论问题，维度分析代码检测到 0% misalignment"
    status: "FAILED - needs re-run"
    evidence_path: "results/C5/20260125_000525_C5_e2e_comparison/"
    confidence: "high"
    issue_analysis:
      observed: |
        - PaLU 模型 unique_dims: [320, 384, 448, 512, 576, 704, 768, 832, 896, 960, 1024]
        - misaligned_pct: 0.0%
        - total_heads: 192
      expected: |
        - 根据 C4 和 palu_dim_dist 数据，per-head dims 应为 [114-125]
        - misaligned_pct: ~97%
        - total_heads: 512
      root_cause: |
        analyze_palu_dimensions() 函数查看的是 k_proj/v_proj 的 out_features，
        这是 PaLU 分解后的 GROUP 级别维度（已是 64 的倍数），
        而不是 per-HEAD 级别的维度（114-125 范围内大部分不对齐）
      impact: |
        由于 PaLU 模型在 C5 实验中被错误识别为「已对齐」，
        dimension repair 实际上没有生效 (affected_layers: 0)，
        导致 palu 和 palu_repair 的性能几乎相同

  - id: "F5.2"
    date: "2026-01-25"
    experiment: "C5_e2e_comparison"
    finding: "尽管 C5 方法有问题，baseline/decode 对比仍有价值"
    evidence_path: "results/C5/20260125_000525_C5_e2e_comparison/"
    confidence: "medium"
    details:
      baseline_prefill_tok_s: 9870
      baseline_decode_tok_s: 119
      palu_prefill_tok_s: 9672
      palu_decode_tok_s: 1371
      palu_decode_speedup: "11.5x"
      observation: |
        PaLU 在 decode 阶段相比 baseline 有 11.5x 的吞吐量提升。
        这是因为 PaLU 压缩减少了 KV cache 大小，decode 阶段是 memory-bound。
        但这不能证明 dimension repair 的有效性。
    implication: |
      C5 实验验证了 PaLU 压缩对 decode 阶段有显著加速（11.5x），
      但由于维度分析错误，无法验证 dimension repair 的端到端效果

  - id: "F5.3"
    date: "2026-01-25"
    experiment: "C5_e2e_comparison"
    finding: "内存测量异常：palu_repair 内存开销 81%"
    evidence_path: "results/C5/20260125_000525_C5_e2e_comparison/comparison.json"
    confidence: "high"
    details:
      baseline_mb: 19003
      palu_mb: 18896
      palu_repair_mb: 34233
      repair_overhead_pct: 81.2
    issue_analysis: |
      palu_repair 内存开销 81% 远超预期的 <5%。
      可能原因：
      1. repair 创建了新的模型副本（inplace=False）
      2. 两个模型同时在 GPU 上
      这是实验脚本问题，非 dimension repair 算法问题

  - id: "F5.4"
    date: "2026-01-25"
    finding: "C5 验证需要修复的问题清单"
    action_items:
      - item: "修复 analyze_palu_dimensions() 函数"
        description: |
          需要深入 PaLU 模型结构，找到 per-head 的压缩维度。
          可能需要查看 PaluLlamaConfig 中的 low_rank_* 参数，
          或者 k_proj/v_proj 模块的 U/VT 分解维度
      - item: "修复 DimensionRepairer 对 PaLU 模型的适配"
        description: |
          当前 DimensionRepairer 可能没有正确识别 PaLU 的低秩分解层
      - item: "修复内存测量逻辑"
        description: |
          在 repair 后应释放原模型，避免两个模型同时占用显存
      - item: "重新运行 C5 实验"
        description: |
          修复上述问题后，使用正确的维度分析和 repair 逻辑重新验证

  - id: "F5.5"
    date: "2026-01-27"
    experiment: "M2_E2E_RAP_SVD_Compression (Job 18533)"
    finding: "RAP SVD 压缩成功生成 100% 不对齐维度的 Llama-3-8B 模型"
    status: "COMPLETED"
    evidence_path: "results/rap_svd_misaligned/"
    confidence: "high"
    details:
      job_id: 18533
      model_path: "results/rap_svd_misaligned/llama3_8b_svd_r0.8.pt"
      model_size_gb: 15.2
      dims_json_path: "results/rap_svd_misaligned/dims.json"
      compression_method: "RAP SVD (no alignment constraint)"
      retain_ratio: 0.8
      original_head_dim: 128
      compressed_head_dim: 102
      alignment_check: "102 % 8 = 6 (NOT aligned)"
      total_layers: 32
      k_aligned_8_count: 0
      k_aligned_8_pct: 0.0
      k_misaligned_8_pct: 100.0
      unique_k_ranks: [102]
      unique_v_ranks: [102]
    code_analysis:
      rap_svd_formula: "rank_per_head = round(head_dim * retain_ratio)"
      code_location: "third_party/RAP/prune.py:775"
      no_alignment_constraint: true
    contrast_with_palu:
      palu_formula: "rounding_search_result(config, block_size=32)"
      palu_always_aligned: true
    supports_claim: "非量化 SVD 压缩会产生不对齐维度，导致 GPU 性能下降"
    paper_section: "Section 5 (E2E Validation)"
    figure_ref: "Figure 6"
    next_steps:
      - "运行 E2E benchmark: RAP_SVD vs RAP_SVD+Repair"
      - "测量 speedup 并更新论文 Figure 6"

c5_status_summary:
  date: "2026-01-27"
  status: "PARTIALLY_COMPLETE - RAP SVD model ready, E2E benchmark pending"
  previous_blocker: |
    所有可用的 PaLU checkpoint（ratio 0.5-0.9）都使用了 32-倍数量化约束，
    维度 100% 对齐。维度坍塌问题不存在于当前可用模型中。
  solution_found: "2026-01-26"
  solution: |
    **发现 RAP SVD 方案不强制对齐！**

    对比三种压缩方案的对齐逻辑：
    - PaLU: `rounding_search_result(config, block_size=32)` → 强制 32 对齐
    - RAP SVD: `round(head_dim * retain_ratio)` → 任意整数，无对齐约束
    - RAP: 只要求偶数（RoPE pair 需要）

    代码位置：
    - PaLU 对齐: third_party/palu/palu/rank_search.py:11-17, block_size=32
    - RAP SVD 无对齐: third_party/RAP/prune.py:775

    示例：retain_ratio=0.8, head_dim=128
    - RAP SVD: round(128 * 0.8) = 102 ← 不是 8 的倍数！
    - PaLU: round(102 / 32) * 32 = 96 ← 强制对齐
  validated: true
  validation_date: "2026-01-27"
  next_action: |
    ✅ COMPLETED:
    1. 使用 RAP SVD 压缩模型生成不对齐维度 - Job 18533 完成
    2. 验证生成的模型 rank 确实不对齐 - 100% 不对齐 (head_dim=102)

    REMAINING:
    3. 运行 dimension repair 并测量 speedup
    4. 更新论文 Figure 6 展示 repair 效果

  # 2026-01-27 M2 实验成功记录
  m2_experiment_success:
    job_id: 18533
    date: "2026-01-27"
    status: "SUCCESS"
    model_path: "results/rap_svd_misaligned/llama3_8b_svd_r0.8.pt"
    dims_json_path: "results/rap_svd_misaligned/dims.json"
    compression_method: "RAP SVD (no alignment constraint)"
    retain_ratio: 0.8
    original_head_dim: 128
    compressed_head_dim: 102
    alignment_analysis:
      total_layers: 32
      k_aligned_8_count: 0
      k_aligned_8_pct: 0.0
      k_misaligned_8_pct: 100.0
      unique_k_ranks: [102]
      unique_v_ranks: [102]
    key_finding: |
      RAP SVD 压缩成功生成了 100% 不对齐的维度！
      - 所有 32 层的 k_rank_per_head = 102 (102 % 8 = 6, 不对齐)
      - 所有 32 层的 v_rank_per_head = 102
      这验证了 findings.yaml 中关于 RAP SVD 不强制对齐的发现。
    code_fixes_applied:
      - "ALL_ATTENTION_FUNCTIONS 导入兼容性修复 (transformers < 4.44)"
      - "Unpack/TransformersKwargs 导入兼容性修复"
      - "LlamaRotaryEmbedding 签名兼容性修复"
      - "lm_eval 懒加载以避免 peft 依赖冲突"
      - "torch_dtype 替代 dtype 参数修复"
    files_modified:
      - "third_party/RAP/src/RAPAttention.py"
      - "third_party/RAP/src/SVDAttention.py"
      - "third_party/RAP/src/PaLUAttention.py"
      - "third_party/RAP/src/LlamaModel.py"
      - "third_party/RAP/src/ops.py"
    next_steps:
      - "运行 E2E benchmark: RAP_SVD vs RAP_SVD+Repair"
      - "测量 speedup 并更新论文 Figure 6"

  # 2026-01-27 M2 实验失败记录 (历史)
  m2_experiment_attempt_1:
    job_id: 18530
    date: "2026-01-27"
    status: "FAILED"
    error_type: "ImportError"
    error_message: |
      cannot import name 'ALL_ATTENTION_FUNCTIONS' from 'transformers.modeling_utils'
    root_cause: |
      RAP 代码使用了 transformers 4.44+ 新增的 ALL_ATTENTION_FUNCTIONS API，
      但当前环境 transformers 版本为 4.39.3，导致导入失败。
    evidence:
      slurm_out: "slurm_logs/18530_rap_svd_compress.out"
      slurm_err: "slurm_logs/18530_rap_svd_compress.err"
      result_dir_empty: true
    impact: |
      - 无法生成不对齐维度的 RAP SVD 压缩模型
      - M2 E2E validation 无法进行
      - 论文 C5 验证仍处于阻塞状态
    resolution_options:
      option_a:
        description: "升级 transformers 到 4.44+"
        complexity: LOW
        risk: "可能破坏其他依赖"
      option_b:
        description: "修改 RAP 代码移除 ALL_ATTENTION_FUNCTIONS 依赖"
        complexity: MEDIUM
        time_estimate: "1-2 hours"
      option_c:
        description: "使用替代方法：手动构造不对齐维度测试"
        complexity: LOW
        note: "已有 C4 微基准数据可复用"

  # 2026-01-26 第三轮分析 - 最终确认
  detailed_analysis_v3:
    date: "2026-01-26"
    analyst: "researcher_agent"

    definitive_finding: |
      经过全面检查所有可用 PaLU 模型（24 个 checkpoint，ratio 0.5-0.9），
      所有模型的 head_wise_ranks 都是 32 的倍数，100% 8-aligned。

      "~97% 维度不对齐" 的数据来源于 RAP 的理论分析
      (third_party/RAP/results/attention_size/*_retain0.8.json)，
      这是 Fisher 信息计算的"理想" rank 分配，
      而非实际 PaLU 压缩结果。

    verified_data:
      all_palu_checkpoints_checked: 24
      all_checkpoints_8_aligned: "100%"
      rap_theoretical_analysis_aligned: "3.1%"
      source_of_97_percent_claim: "RAP 理论分析，非实际模型"

    code_correctness:
      analyze_palu_dimensions: |
        代码逻辑正确。检测 HeadwiseLowRankModule.ranks 并报告 0% misalignment
        是因为实际模型维度确实对齐。不需要修复代码。

    experiment_design_issue:
      description: |
        C5 实验设计假设：PaLU 压缩产生不对齐维度
        实际情况：PaLU 使用量化约束，强制 rank 对齐到 32 的倍数
        结论：C5 实验的前提条件不成立

    resolution:
      recommended: "文字修改方案"
      actions:
        - "更新论文，明确 scope：维度坍塌发生在非量化 SVD 压缩场景"
        - "保留 C4 微基准（使用人工构造的不对齐维度）作为主要证据"
        - "C5 数据用于展示 PaLU 的 decode 加速 (11.5x)，但不用于 repair 效果验证"
        - "在 limitations 中说明：当前 PaLU 实现已内置对齐约束"

  # 2026-01-26 深入分析 (第一轮 - 已过时)
  detailed_analysis_v1:
    date: "2026-01-26"
    analyst: "researcher_agent"
    status: "SUPERSEDED by v2"
    note: "原分析认为是导入问题，实际问题更根本"

  # 2026-01-26 深入分析 (第二轮 - 最终)
  detailed_analysis_v2:
    date: "2026-01-26"
    analyst: "researcher_agent"

    critical_finding: |
      当前使用的 PaLU 模型 (ratio=0.7, gs=4) 的维度本身就是完全对齐的！
      这不是代码 bug，而是实验设计问题。

    evidence:
      - source: "PaLU config.json"
        path: "/home/xinj/rap/submodules/palu/Meta-Llama-3-8B-Instruct_ratio-0.7_gs-4-fisher_uniform-svd/config.json"
        observation: |
          head_wise_ranks 中所有值都是 32 的倍数:
          [160, 192, 224, 256, 288, 352, 384, 416, 448, 480, 512]
          8-aligned: 100%, 16-aligned: 100%, 32-aligned: 100%
      - source: "C5 experiment output"
        path: "results/C5/20260125_000525_C5_e2e_comparison/report.md"
        observation: |
          Before/After Repair 完全相同:
          - Misaligned dimensions: 0.0%
          - Unique dims: [320, 384, 448, 512, 576, 704, 768, 832, 896, 960, 1024]
          - Affected layers: 0
      - source: "palu_dim_dist 数据 (不同模型)"
        path: "results/palu_dim_dist/llama3_r0.8/dims.json"
        observation: |
          retain_ratio=0.8 的模型有不对齐维度:
          per_head_dims: [114, 116, 117, 118, 120, 121, 122, 123, 124, 125]
          约 97% 不是 8 的倍数

    root_cause_analysis:
      issue_1_model_already_aligned:
        description: "ratio=0.7 的 PaLU 模型产生对齐的 rank 值"
        detail: |
          PaLU 的 Fisher 信息 + 均匀量化策略在 ratio=0.7 时
          产生的 rank 值恰好都是 32 的倍数
        impact: "C5 实验的前提条件不存在"

      issue_2_data_source_mismatch:
        description: "findings.yaml 中 ~97% 不对齐的数据来自不同模型"
        detail: |
          - palu_dim_dist 分析的是 retain_ratio=0.8
          - C5 实验使用的是 ratio=0.7
          这是两个不同的压缩配置
        impact: "数据不一致导致错误预期"

      issue_3_ranks_semantic:
        description: "HeadwiseLowRankModule.ranks 是 GROUP-level 而非 per-head"
        detail: |
          ranks=[160, 160] 表示 2 个 group 各有 rank=160
          不是 per-head 的维度，per-head dim = rank / heads_per_group
        impact: "代码逻辑正确但解释有误"

    difficulty_assessment:
      level: "HARD"
      rationale:
        - "不是代码 bug，是实验设计和模型选择问题"
        - "需要获取或重新压缩 retain_ratio=0.8 的 PaLU 模型"
        - "或者修改 PaLU 压缩参数禁用内部对齐约束"

    recommended_action: "降级到文字修改方案"

    text_modification_plan:
      - "更新论文表述，明确 C5 使用的是 ratio=0.7（已对齐）"
      - "将 dimension repair 验证限制在 C4 微基准测试"
      - "强调 ratio=0.8 等高压缩比场景下维度坍塌更严重"
      - "保留 C4 的 +27% speedup 结论作为主要证据"

    alternative_if_e2e_required:
      - "获取 retain_ratio=0.8 的完整 PaLU checkpoint"
      - "或使用 third_party/RAP/results 中的维度数据重新压缩模型"
      - "预计需要额外 2-4 小时 GPU 时间"

# 关键洞察（跨阶段）
key_insights:
  - id: "I1"
    insight: "维度对齐的影响是非线性的：减少 FLOPs 反而增加延迟"
    supporting_findings: ["F1.1", "F1.2"]
    implications: "压缩算法必须考虑硬件对齐约束"

  - id: "I2"
    insight: "低秩 SVD 压缩理论上会产生不对齐维度，但实际实现可能有对齐约束"
    supporting_findings: ["F1.3", "F1.4"]
    implications: "需要在压缩后添加维度修复 pass，或在压缩算法中引入对齐约束"
    caveat_added: "2026-01-26"
    status: "REQUIRES CLARIFICATION"
    caveat: |
      【重要澄清】2026-01-26 最终确认：
      - ~97% 不对齐数据来自 RAP 的理论分析，非实际 PaLU 压缩结果
      - 实际 PaLU checkpoint (ratio 0.5-0.9) 全部 100% 对齐
      - PaLU 使用 Fisher + 均匀量化策略，强制 rank 对齐到 32 的倍数
      - 维度坍塌问题存在于：(1) 未量化的 SVD 压缩，(2) 理论 rank 分配
      - 论文 scope 需要明确这一点

  - id: "I5"
    date: "2026-01-26"
    insight: "PaLU 实现使用量化约束，强制 rank 对齐到 32 的倍数"
    supporting_findings: ["F5.1", "F5.4"]
    status: "CORRECTED"
    original_hypothesis: |
      (错误) ratio=0.8 产生 ~97% 不对齐维度
    corrected_understanding: |
      所有 PaLU checkpoint (ratio 0.5-0.9) 都是 100% 对齐的。
      ~97% 不对齐的数据来自 RAP 的理论分析，非实际压缩结果。
      PaLU 的 Fisher + 均匀量化策略会将 rank 对齐到 32 的倍数。
    implications: |
      1. 当前 PaLU 实现不会产生维度坍塌
      2. 维度坍塌发生在未量化的 SVD 压缩场景
      3. 论文需要明确说明这一限制
      4. C4 微基准（人工不对齐维度）仍是有效证据

  - id: "I3"
    date: "2026-01-24"
    insight: "性能差距的根因不在 PyTorch 后端选择层，而在 FlashAttention 内核层"
    supporting_findings: ["F2.1.1", "F2.1.3", "F2.1.4"]
    implications: |
      1. 之前假设的「FlashAttention 回退到 Math」是错误的
      2. FlashAttention 对非 8-aligned 输入使用了慢速路径
      3. 需要深入 FlashAttention/CUTLASS 源码分析具体原因
      4. 优化方向应聚焦于 CUDA kernel 层而非 PyTorch 层

  - id: "I4"
    date: "2026-01-24"
    insight: "硬件层根因已验证：Tensor Core 对齐 + 向量化加载是性能差距的主因"
    supporting_findings: ["F2.3.1", "F2.3.3", "F2.3.4"]
    implications: |
      1. Tensor Core 需要 K % 16 == 0 达到最优利用率（~30% vs ~12%）
      2. 向量化加载需要 K % 8 == 0 使用 float4（vs scalar）
      3. L2 cache sector 浪费（5.8%）不是主因
      4. 解决方案：显式 padding 到 8/16 对齐
      5. 可进入 C3 形式化阶段定义 Shape Contract

# C1 阶段总结
C1_summary:
  status: "COMPLETED"
  date: "2026-01-24"
  key_numbers:
    - "88% 延迟增加 (head_dim 107 vs 96)"
    - "12.6x 性能差距 (Math vs Flash backend)"
    - "~97% PaLU 压缩维度不对齐 (not % 8 == 0)"
  conclusion: "维度坍塌现象已充分量化，可进入 C2 原因探究阶段"
