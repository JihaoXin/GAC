experiments:
  S1_sdpa_dense_sweep:
    type: sdpa_dense
    dtype: fp16
    shapes:
      - {batch: 4, seq_len: 2048, n_heads: 32}
      - {batch: 1, seq_len: 4096, n_heads: 32}
    head_dim_range: [64, 160]
    head_dim_step_1: 1  # step for 64..128
    head_dim_step_2: 2  # step for 128..160
    backend: AUTO
    warmup: 50
    measure: 200
    trials: 3

  S1_sdpa_extended:
    type: sdpa_dense
    dtype: fp16
    shapes:
      - {batch: 4, seq_len: 2048, n_heads: 32}
    head_dim_range: [129, 256]
    head_dim_boundary: 160  # step1 up to 160, step2 above
    head_dim_step_1: 1  # step for 129..160
    head_dim_step_2: 2  # step for 162..256
    backend: AUTO
    warmup: 50
    measure: 200
    trials: 3

  S2_sdpa_backend_forced:
    type: sdpa_backend_forced
    dtype: fp16
    shape: {batch: 4, seq_len: 2048, n_heads: 32}
    head_dims: [96, 104, 107, 112, 120, 128]
    backends: [AUTO, FLASH, MEM_EFFICIENT, MATH]
    warmup: 50
    measure: 200
    trials: 3

  G3_gemm_k_dense:
    type: gemm_k_dense
    dtypes: [fp16, bf16]
    shape: {M: 4096, N: 4096}
    K_range: [64, 160]
    K_step_1: 1  # step for 64..128
    K_step_2: 2  # step for 128..160
    warmup: 50
    measure: 200
    trials: 3

  G4_gemm_n_dense_projectionlike:
    type: gemm_n_dense
    dtypes: [fp16, bf16]
    M_values: [1024, 4096, 16384]
    K: 4096
    N_range: [64, 160]
    N_step_1: 1  # step for 64..128
    N_step_2: 2  # step for 128..160
    warmup: 50
    measure: 200
    trials: 3

  P1_padding_rescue:
    type: padding_rescue
    dtype: fp16
    logical_head_dim: 107
    pad_options: [107, 112, 128]
    sdpa_shape: {batch: 4, seq_len: 2048, n_heads: 32}
    gemm_shapes:
      - {M: 4096, N: 4096, K: 107}  # reduction
      - {M: 4096, K: 4096, N: 107}  # projection
    warmup: 50
    measure: 200
    trials: 3

  HET1_head_hetero_batching_penalty:
    type: hetero_batching
    dtype: fp16
    total_N: 4096
    H: 32
    patterns:
      uniform:
        groups: [{dim: 128, count: 32}]
      mild:
        groups: [{dim: 128, count: 28}, {dim: 112, count: 4}]
      medium:
        groups: [{dim: 128, count: 16}, {dim: 112, count: 8}, {dim: 96, count: 8}]
      severe:
        groups: [{dim: 128, count: 8}, {dim: 112, count: 8}, {dim: 107, count: 8}, {dim: 96, count: 8}]
    warmup: 50
    measure: 200
    trials: 3

  # C2.1 PyTorch Backend Selection Verification
  C21_backend_selection:
    type: sdpa_backend_selection
    description: "Verify PyTorch SDPA backend selection boundaries"
    dtype: fp16
    shapes:
      - {batch: 4, seq_len: 2048, n_heads: 32}
      - {batch: 1, seq_len: 4096, n_heads: 32}
    # Test all head_dims from 112-128 (PaLU common range)
    # plus boundary cases around 8-alignment
    head_dims: [
      # 8-aligned (should use Flash)
      112, 120, 128,
      # PaLU typical dims (non-8-aligned, should fallback)
      113, 114, 115, 116, 117, 118, 119,
      121, 122, 123, 124, 125, 126, 127,
      # Additional boundary tests
      104, 105, 106, 107, 108, 109, 110, 111
    ]
    backends: [AUTO, FLASH, MEM_EFFICIENT, MATH]
    warmup: 50
    measure: 200
    trials: 3
    # Record which backend is actually selected for AUTO mode
    detect_backend: true

  # C2.3 Hardware Layer Analysis
  C23_hardware_layer:
    type: hardware_analysis
    description: "Analyze hardware-level factors: Tensor Core, L2 cache, bandwidth"
    dtype: fp16
    hypotheses:
      H1: "Tensor Core requires K % 16 == 0 for optimal FP16 performance"
      H2: "L2 cache 32-byte sector causes over-fetch for non-aligned strides"
      H3: "Non-aligned dims cause bandwidth efficiency loss"
      H4: "Vectorized loads (float4) require head_dim % 8 == 0"
    tests:
      tensor_core:
        M: 8192
        N: 8192
        K_range: [104, 128]
        metric: TFLOPS
      l2_cache:
        M: 2048
        head_dims: [104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128]
        metric: bandwidth_efficiency
      vectorized_loads:
        M: 4096
        N: 4096
        K_values: [104, 105, 107, 108, 112, 116, 120, 124, 128]
        metric: TFLOPS
    warmup: 30
    measure: 100
