@inproceedings{flashattention,
  title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{flashattention2,
  title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  booktitle={ICLR},
  year={2024}
}

@article{flashattention3,
  title={FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision},
  author={Shah, Jay and Bikshandi, Ganesh and Zhang, Ying and Thakkar, Vijay and Ramani, Pradeep and Dao, Tri},
  journal={arXiv preprint arXiv:2407.08608},
  year={2024}
}

@misc{nvidia_hopper_whitepaper,
  title={NVIDIA H100 Tensor Core GPU Architecture},
  author={{NVIDIA Corporation}},
  howpublished={White Paper},
  year={2022},
  note={\url{https://resources.nvidia.com/en-us-tensor-core}}
}

@inproceedings{palu,
  title={PaLU: Compressing KV-Cache with Low-Rank Projection},
  author={Chang, Chi-Chih and Huang, Wei-Cheng and Liu, Chien-Yu and Lin, Chun-Feng and Chen, Kai-Chiang and Wu, An-Yeu},
  booktitle={EMNLP},
  year={2024}
}

@inproceedings{sparsegpt,
  title={SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={ICML},
  year={2023}
}

@inproceedings{gptq,
  title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and others},
  booktitle={ICLR},
  year={2023}
}

@misc{cutlass,
  title={CUTLASS: CUDA Templates for Linear Algebra Subroutines},
  author={NVIDIA},
  howpublished={\url{https://github.com/NVIDIA/cutlass}},
  year={2023}
}

@misc{llama3,
  title={Llama 3 Model Card},
  author={{Meta AI}},
  howpublished={\url{https://github.com/meta-llama/llama3}},
  year={2024}
}

@inproceedings{awq,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  booktitle={MLSys},
  year={2024}
}

@article{mqa,
  title={Fast Transformer Decoding: One Write-Head is All You Need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}

@inproceedings{gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  booktitle={EMNLP},
  year={2023}
}

@inproceedings{streaminglm,
  title={Efficient Streaming Language Models with Attention Sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  booktitle={ICLR},
  year={2024}
}

@misc{tensorrt,
  title={NVIDIA TensorRT: Programmable Inference Accelerator},
  author={NVIDIA},
  howpublished={\url{https://developer.nvidia.com/tensorrt}},
  year={2024}
}

@inproceedings{vllm,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph E and Zhang, Hao and Stoica, Ion},
  booktitle={SOSP},
  year={2023}
}

@inproceedings{rap,
  title={Reducing Transformer Key-Value Cache Size with Cross-Layer Attention},
  author={Liu, William and Zhou, Hao and Zhang, Jiaxuan and Zou, James},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{svdllm,
  title={SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression},
  author={Wang, Xin and Zheng, Yu and Wan, Zhongwei and Zhang, Mi},
  booktitle={ICLR},
  year={2025}
}

@inproceedings{lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{qlora,
  title={QLoRA: Efficient Finetuning of Quantized LLMs},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  booktitle={NeurIPS},
  year={2023}
}

@misc{gguf,
  title={GGML: Tensor Library for Machine Learning},
  author={Gerganov, Georgi},
  howpublished={\url{https://github.com/ggerganov/ggml}},
  year={2024}
}

@inproceedings{flashdecoding,
  title={FlashDecoding++: Faster Large Language Model Inference on GPUs},
  author={Hong, Ke and Dai, Guohao and Xu, Jiaming and Mao, Qiuli and Li, Xiuhong and Liu, Jun and Chen, Kangdi and Dong, Yuhan and Wang, Yu},
  booktitle={MLSys},
  year={2024}
}

@misc{nvidia_perf_guide,
  title={Matrix Multiplication Background User's Guide},
  author={NVIDIA},
  howpublished={\url{https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/}},
  year={2024}
}

@inproceedings{flashinfer,
  title={FlashInfer: Efficient and Customizable Attention Engine for LLM Inference},
  author={Ye, Zihao and Chen, Lianmin and Zheng, Ruihang and others},
  booktitle={MLSys},
  year={2025}
}

@article{caldera,
  title={CALDERA: Compressing LLMs using Low Rank and Low Precision Decomposition},
  author={Wei, Jiwei and others},
  journal={arXiv preprint arXiv:2408.15285},
  year={2024}
}

@misc{tgi,
  title={Text Generation Inference},
  author={Hugging Face},
  howpublished={\url{https://github.com/huggingface/text-generation-inference}},
  year={2024}
}

@misc{pytorch_sdpa,
  title={Scaled Dot Product Attention Documentation},
  author={PyTorch},
  howpublished={\url{https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html}},
  year={2024}
}

@article{mixtral,
  title={Mixtral of Experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@inproceedings{squeezellm,
  title={SqueezeLLM: Dense-and-Sparse Quantization},
  author={Kim, Sehoon and Hooper, Coleman and Gholami, Amir and Dong, Zhen and Li, Xiuyu and Shen, Sheng and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={ICML},
  year={2024}
}

@inproceedings{llmint8,
  title={LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{speculative_decoding,
  title={Fast Inference from Transformers via Speculative Decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  booktitle={ICML},
  year={2023}
}

@inproceedings{medusa,
  title={Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads},
  author={Cai, Tianle and Li, Yuhong and Geng, Zhengyang and Peng, Hongwu and Lee, Jason D and Chen, Deming and Dao, Tri},
  booktitle={ICML},
  year={2024}
}

% ===== HIGH PRIORITY (80+) =====

@article{quest,
  title={QUEST: Query-Aware Sparsity for Efficient Long-Context LLM Inference},
  author={Tang, Jiaming and Tang, Yilong and Yang, Shang and Vora, Ashish and Han, Song},
  journal={arXiv preprint arXiv:2406.10774},
  year={2024}
}

% ===== COMPRESSION METHODS =====

@inproceedings{smoothquant,
  title={SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={ICML},
  year={2023}
}

@article{h2o,
  title={H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and RÃ©, Christopher and Barrett, Clark and others},
  journal={arXiv preprint arXiv:2306.14048},
  year={2023}
}

@article{pyramidkv,
  title={PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling},
  author={Cai, Zefan and Zhang, Yichi and Gao, Bofei and Liu, Yuliang and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Chang, Baobao and Hu, Junjie and others},
  journal={arXiv preprint arXiv:2406.02069},
  year={2024}
}

@article{snapkv,
  title={SnapKV: LLM Knows What You are Looking for Before Generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={arXiv preprint arXiv:2404.14469},
  year={2024}
}

@inproceedings{atom,
  title={ATOM: Low-bit Quantization for Efficient and Accurate LLM Serving},
  author={Zhao, Yilong and Xu, Chien-Yu and Gu, Yuhui and Guo, Song and Yang, Jingwen and Chen, Kan and Yu, Zhaoting and Zhou, Xingcheng and Mei, Hao and Guo, Xiaotao and others},
  booktitle={MLSys},
  year={2024}
}

@article{slimgpt,
  title={SlimGPT: Layer-wise Structured Pruning for Large Language Models},
  author={Li, Yilin and Peng, Bo and Guan, Dongxu and Gao, Yue and Li, Zheng and Huang, Jingjing},
  journal={arXiv preprint arXiv:2401.12153},
  year={2024}
}

% ===== GPU OPTIMIZATION =====

@misc{volta_whitepaper,
  title={NVIDIA Tesla V100 GPU Architecture},
  author={{NVIDIA Corporation}},
  howpublished={White Paper},
  year={2017},
  note={\url{https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf}}
}

@misc{ampere_whitepaper,
  title={NVIDIA A100 Tensor Core GPU Architecture},
  author={{NVIDIA Corporation}},
  howpublished={White Paper},
  year={2020},
  note={\url{https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf}}
}

@inproceedings{cutlass_paper,
  title={CUTLASS: Fast Linear Algebra in CUDA C++},
  author={Kerr, Andrew and Campbell, Dan and Trevithick, Mark},
  booktitle={IEEE High Performance Extreme Computing Conference (HPEC)},
  year={2017}
}

@inproceedings{triton,
  title={Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations},
  author={Tillet, Philippe and Kung, H. T. and Cox, David},
  booktitle={MAPL},
  year={2019}
}

% ===== EVALUATION & BENCHMARKING =====

@article{llm_perf,
  title={Full Stack Optimization of Transformer Inference: a Survey},
  author={Kim, Sehoon and Mangalam, Karttikeya and Moon, Suhong and others},
  journal={arXiv preprint arXiv:2302.14017},
  year={2023}
}

@software{lmeval,
  author = {Gao, Leo and
            Tow, Jonathan and
            Abbasi, Baber and
            Biderman, Stella and
            Black, Sid and
            DiPofi, Anthony and
            Foster, Charles and
            Golding, Laurence and
            Hsu, Jeffrey and
            Le Noac'h, Alain and others},
  title = {A framework for few-shot language model evaluation},
  month = sep,
  year = 2023,
  publisher = {Zenodo},
  version = {v0.4.0},
  doi = {10.5281/zenodo.10256836},
  url = {https://zenodo.org/records/10256836}
}

% ===== INFERENCE SYSTEMS =====

@inproceedings{deepspeed,
  title={ZeRO: Memory Optimizations Toward Training Trillion Parameter Models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020}
}

@inproceedings{deepspeed_inference,
  title={DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale},
  author={Aminabadi, Reza Yazdani and Rajbhandari, Samyam and Awan, Ammar Ahmad and Li, Cheng and Li, Du and Zheng, Elton and Ruwase, Olatunji and Smith, Shaden and Zhang, Minjia and Rasley, Jeff and He, Yuxiong},
  booktitle={SC22: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2022}
}

@inproceedings{orca,
  title={ORCA: A Distributed Serving System for Transformer-Based Generative Models},
  author={Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  booktitle={OSDI},
  pages={521--538},
  year={2022}
}

% ===== ATTENTION OPTIMIZATION =====

@article{ring_attention,
  title={Ring Attention with Blockwise Transformers for Near-Infinite Context},
  author={Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2310.01889},
  year={2023}
}

@misc{xformers,
  title={xFormers: A modular and hackable Transformer modelling library},
  author={{Meta AI Research}},
  howpublished={\url{https://github.com/facebookresearch/xformers}},
  year={2022}
}

@inproceedings{efficient_transformers,
  title={Efficient Transformers: A Survey},
  author={Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  journal={ACM Computing Surveys},
  volume={55},
  number={6},
  pages={1--28},
  year={2022}
}

% ===== FOUNDATIONS =====

@article{lottery_ticket,
  title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={ICLR},
  year={2019}
}

@inproceedings{roofline,
  title={Roofline: An Insightful Visual Performance Model for Multicore Architectures},
  author={Williams, Samuel and Waterman, Andrew and Patterson, David},
  journal={Communications of the ACM},
  volume={52},
  number={4},
  pages={65--76},
  year={2009}
}


% ===== HARDWARE-AWARE COMPRESSION =====

@inproceedings{haloc2023,
  title={HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural Networks},
  author={Xiao, Jinqi and Zhang, Chengming and Gong, Yu and Yin, Miao and Sui, Yang and Xiang, Lizhi and Tao, Dingwen and Yuan, Bo},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={9},
  pages={10464--10472},
  year={2023}
}

@inproceedings{halp2021,
  title={HALP: Hardware-Aware Latency Pruning},
  author={Li, Meng and Liao, Rui and Wang, Mengchen and Ren, Kuangrong and Fan, Xin and Li, Can and Lin, Yufei and Niu, Wei and others},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{amc2018,
  title={AMC: AutoML for Model Compression and Acceleration on Mobile Devices},
  author={He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
  booktitle={European Conference on Computer Vision},
  pages={784--800},
  year={2018}
}

@article{hape2025,
  title={HAPE: Hardware-Aware LLM Pruning For Efficient On-Device Inference Optimization},
  author={Kim, Minseok and others},
  journal={ACM Transactions on Design Automation of Electronic Systems},
  year={2025}
}

@article{nas_llm_compression2024,
  title={Compressing Large Language Models with Automated Sub-Network Search},
  author={Sukthanker, Rhea Sanjay and Staffler, Benedikt and Hutter, Frank and Klein, Aaron},
  journal={arXiv preprint arXiv:2410.06479},
  year={2024}
}

% ===== SVD-BASED COMPRESSION =====

@inproceedings{svdllm2024,
  title={SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression},
  author={Wang, Xin and Zheng, Yu and Wan, Zhongwei and Zhang, Mi},
  booktitle={International Conference on Learning Representations},
  year={2025}
}

@inproceedings{fwsvd2022,
  title={Language Model Compression with Weighted Low-rank Factorization},
  author={Hsu, Yen-Chang and Hua, Ting and Chang, Sungen and Lou, Qian and Shen, Yilin and Jin, Hongxia},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={1323--1335},
  year={2022}
}

@article{gfwsvd2025,
  title={Generalized Fisher-Weighted SVD: Scalable Kronecker-Factored Fisher Approximation for Compressing LLMs},
  author={Li, Yuchen and others},
  journal={arXiv preprint arXiv:2505.17974},
  year={2025}
}

@article{lowrank_prehab2024,
  title={Low-Rank Prehab: Preparing Neural Networks for SVD Compression},
  author={Zhang, Yiming and others},
  journal={arXiv preprint arXiv:2512.01980},
  year={2024}
}

% ===== GPU ARCHITECTURE EVOLUTION =====

@misc{nvidia_tensor_core_evolution2024,
  title={NVIDIA Tensor Core Evolution: From Volta To Blackwell},
  author={{SemiAnalysis}},
  howpublished={\url{https://newsletter.semianalysis.com/p/nvidia-tensor-core-evolution-from-volta-to-blackwell}},
  year={2024}
}

@article{hopper_microbenchmark2024,
  title={Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and Multiple Level Analysis},
  author={Zhang, Yiming and Li, Chenhao and Sun, Jing and Huang, Xin and Wang, Yuxuan},
  journal={arXiv preprint arXiv:2501.12084},
  year={2024}
}

@article{tma_fp8_grouped_gemm2025,
  title={TMA-Adaptive FP8 Grouped GEMM: Eliminating Padding Requirements in Low-Precision Training},
  author={Chen, Yixin and others},
  journal={arXiv preprint arXiv:2508.16584},
  year={2025}
}

% ===== QUANTIZATION METHODS =====

@inproceedings{llmint8_2022,
  title={LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}

@article{int4_quantization2023,
  title={Understanding INT4 Quantization for Language Models: Latency Speedup, Composability, and Failure Cases},
  author={Wu, Xiaoxia and Yao, Zhewei and He, Yuxiong},
  journal={arXiv preprint arXiv:2301.12017},
  year={2023}
}

% ===== PRUNING METHODS =====

@inproceedings{structured_pruning_iclr2024,
  title={Dynamic Sparse Training with Structured Pruning},
  author={Zhou, Zixuan and others},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

@inproceedings{maskllm2024,
  title={MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models},
  author={Sun, Gongfan and Liang, Chengying and Song, Yurui and Wen, Zaixing and Chen, Shengji and Dong, Jing and Li, Donghai and Yuan, Yujie and Zhou, Dongsheng and others},
  booktitle={Advances in Neural Information Processing Systems},
  year={2024}
}

% ===== GPU MEMORY AND GEMM OPTIMIZATION =====

@article{memory_coalescing2024,
  title={Irregular Accesses Reorder Unit: Improving GPGPU Memory Coalescing},
  author={Pham, Binh and others},
  journal={The Journal of Supercomputing},
  volume={78},
  pages={5135--5161},
  year={2024}
}

% ===== INFERENCE SYSTEMS =====

@misc{vllm_dimension_handling2024,
  title={Inside vLLM: Anatomy of a High-Throughput LLM Inference System},
  author={{vLLM Team}},
  howpublished={\url{https://blog.vllm.ai/2025/09/05/anatomy-of-vllm.html}},
  year={2024}
}

@misc{tensorrt_padding2024,
  title={TensorRT-LLM Architecture Overview},
  author={{NVIDIA Corporation}},
  howpublished={\url{https://nvidia.github.io/TensorRT-LLM/architecture/overview.html}},
  year={2024}
}

% ===== GPU ARCHITECTURE EVOLUTION (Additional) =====

@article{nvidia_dl_perf2024,
  title={Deep Learning Performance: Matrix Multiplication},
  author={{NVIDIA Corporation}},
  howpublished={\url{https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/}},
  year={2024}
}

@misc{cutlass_alignment2024,
  title={CUTLASS 3.x: Orthogonal, Reusable, and Composable Abstractions for GEMM Kernel Design},
  author={{NVIDIA Corporation}},
  howpublished={\url{https://developer.nvidia.com/blog/cutlass-3-x-orthogonal-reusable-and-composable-abstractions}},
  year={2024}
}

% ===== QUANTIZATION METHODS (Additional) =====

% ===== SURVEYS (Comprehensive) =====

@article{hw_accel_survey2025,
  title={Hardware Acceleration for Neural Networks: A Comprehensive Survey},
  author={Various},
  journal={arXiv preprint arXiv:2512.23914},
  year={2025}
}

@article{llm_compression_survey2025,
  title={A Review of State-of-the-Art Techniques for Large Language Model Compression},
  author={Various},
  journal={Complex \& Intelligent Systems},
  publisher={Springer},
  year={2025},
  url={https://link.springer.com/article/10.1007/s40747-025-02019-z}
}

@article{model_compression_survey2025,
  title={A Survey of Model Compression Techniques: Past, Present, and Future},
  author={Various},
  journal={Frontiers in Robotics and AI},
  year={2025},
  url={https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1518965/full}
}

% ===== ASVD, LLM-Pruner, Wanda =====

@article{asvd,
  title={ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models},
  author={Yuan, Zhihang and Shang, Yuzhang and Song, Yue and Wu, Qiang and Yan, Yan and Sun, Guangyu},
  journal={arXiv preprint arXiv:2312.05821},
  year={2023}
}

@inproceedings{llmpruner,
  title={LLM-Pruner: On the Structural Pruning of Large Language Models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  booktitle={NeurIPS},
  year={2023}
}

@inproceedings{wanda,
  title={A Simple and Effective Pruning Approach for Large Language Models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J. Zico},
  booktitle={ICLR},
  year={2024}
}

% ===== END OF ADDITIONS =====
