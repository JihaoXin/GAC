@inproceedings{flashattention,
  title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{flashattention2,
  title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  booktitle={ICLR},
  year={2024}
}

@article{palu,
  title={PaLU: Compressing KV-Cache with Low-Rank Projection},
  author={Chang, Chi-Chih and others},
  journal={arXiv preprint arXiv:2407.21118},
  year={2024}
}

@inproceedings{sparsegpt,
  title={SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={ICML},
  year={2023}
}

@inproceedings{gptq,
  title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and others},
  booktitle={ICLR},
  year={2023}
}

@misc{cutlass,
  title={CUTLASS: CUDA Templates for Linear Algebra Subroutines},
  author={NVIDIA},
  howpublished={\url{https://github.com/NVIDIA/cutlass}},
  year={2023}
}

@article{llama3,
  title={Llama 3 Model Card},
  author={Meta AI},
  year={2024}
}
