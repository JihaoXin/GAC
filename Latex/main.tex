%%
%% G-Compress: Dimensional Collapse in Compressed LLMs
%% Target: EuroMLSys (SIGPLAN format, 6 pages excluding references)
%%

\documentclass[sigplan,10pt,nonacm]{acmart}

%% Remove ACM-specific elements for submission
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

%% Packages
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{tikz}

%% Code listing style
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  numbers=left,
  numberstyle=\tiny,
}

%% Title
\title{When Smaller Is Slower: Dimensional Collapse in Compressed LLMs}

%% Authors
\author{Jihao Xin}
\affiliation{
  \institution{KAUST}
  \city{Thuwal}
  \country{Saudi Arabia}
}
\email{jihao.xin@kaust.edu.sa}

\author{Tian Lv}
\affiliation{
  \institution{KAUST}
  \city{Thuwal}
  \country{Saudi Arabia}
}

\author{Qilong Pan}
\affiliation{
  \institution{HUMAIN AI}
  \city{Riyadh}
  \country{Saudi Arabia}
}

\author{Kesen Wang}
\affiliation{
  \institution{HUMAIN AI}
  \city{Riyadh}
  \country{Saudi Arabia}
}

\author{Marco Canini}
\affiliation{
  \institution{KAUST}
  \city{Thuwal}
  \country{Saudi Arabia}
}

\begin{abstract}
Post-training compression can produce irregular tensor dimensions (e.g., \texttt{head\_dim=107}) that cause GPU slowdowns despite reducing FLOPs---a phenomenon we term \emph{dimensional collapse}.
On NVIDIA A100, \texttt{head\_dim=107} increases SDPA latency by 88\% vs.\ aligned dimensions.
We identify three root causes: Tensor Core tile misalignment (58\% slowdown), vectorized load degradation (50\% loss), and SDPA bandwidth inefficiency (40\%).
We propose a \emph{dimension repair} pass achieving 25--30\% kernel-level speedup with 3.7\% memory overhead.
Our findings apply to any compression method producing misaligned dimensions; integration with SVD-based structures is straightforward.
\end{abstract}

\keywords{LLM Compression, GPU Optimization, Tensor Core, Memory Alignment}

\begin{document}

\maketitle

%% ===========================================
%% 1. INTRODUCTION
%% ===========================================
\section{Introduction}
\label{sec:intro}

Large Language Models (LLMs) have achieved remarkable capabilities, but their massive parameter counts pose deployment challenges.
Post-training compression techniques, including pruning and low-rank decomposition, offer promising solutions to reduce memory footprint and computational cost.
However, these techniques often produce models with \emph{irregular tensor dimensions}---values that do not align with hardware-preferred multiples (e.g., 8, 16, 32, 128).

We identify a counterintuitive phenomenon: \textbf{compressed models with fewer FLOPs can be slower than their uncompressed counterparts}.
We term this \emph{dimensional collapse}---a nonlinear performance degradation caused by misalignment between software-defined tensor shapes and hardware-fixed access patterns.

\paragraph{Scope and Applicability.}
Our analysis targets compression methods that \emph{do not} include alignment constraints---such as vanilla SVD, Fisher-information-based rank allocation, or future methods optimizing purely for accuracy.
We verified that all 24 publicly available PaLU checkpoints (ratio 0.5--0.9) use \emph{aligned} dimensions due to internal quantization that rounds ranks to 32-multiples.
Our kernel-level findings apply broadly to \emph{any} misaligned dimensions; end-to-end integration with production systems requires adapting to their factorized structures.

\paragraph{Motivating Example.}
Consider PaLU~\cite{palu}, a state-of-the-art low-rank compression method.
\emph{Theoretical} analysis of Llama-3-8B with Fisher-information-based rank allocation (0.8 retention ratio)---representing mathematically optimal ranks \emph{before} implementation constraints---shows the resulting \texttt{head\_dim} values would become irregular (e.g., 114--125 instead of 128).
In this \emph{unconstrained} scenario, 96.9\% of the theoretical dimensions are not 8-aligned.
Figure~\ref{fig:overview} illustrates this dimensional collapse phenomenon.
On an NVIDIA A100, this causes:
\begin{itemize}
  \item 88\% increase in SDPA latency
  \item FlashAttention internal slow path with 30--45\% overhead
  \item MEM\_EFFICIENT unavailable (strict 8-alignment)
  \item Bandwidth waste from cache misalignment
\end{itemize}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig1_overview.pdf}
\caption{Dimensional collapse in compressed LLMs. Post-training compression produces irregular head dimensions (e.g., $d$=107) that cause \textbf{88\% latency increase} vs.\ aligned dimensions. Dimension repair (padding to $d$=112) recovers \textbf{30\% performance} with only 4.7\% memory overhead.}
\label{fig:overview}
\end{figure}

\paragraph{Contributions.}
This paper makes the following contributions:
\begin{enumerate}
  \item \textbf{Quantification}: We measure the performance impact of irregular dimensions across GEMM and SDPA (\S\ref{sec:phenomenon}).
  \item \textbf{Root Cause Analysis}: We identify the causes across three layers: PyTorch backend selection, CUDA kernel paths, and hardware constraints (\S\ref{sec:causes}).
  \item \textbf{Shape Contract}: We formalize dimension alignment requirements as optimization constraints (\S\ref{sec:solution}).
  \item \textbf{Dimension Repair}: We propose a lightweight post-compression pass that restores alignment (\S\ref{sec:solution}).
  \item \textbf{Evaluation}: Kernel-level experiments demonstrate 25--30\% speedup with 3.7\% memory overhead---a 6.9$\times$ return on investment (speedup per unit memory cost) (\S\ref{sec:eval}).
\end{enumerate}

%% ===========================================
%% 2. BACKGROUND
%% ===========================================
\section{Background}
\label{sec:background}

\paragraph{Notation.}
We use $d$ to denote the attention head dimension (also written as \texttt{head\_dim} in code).
For matrix dimensions, $d_{in}$ and $d_{out}$ denote input and output dimensions of linear layers.
$B$, $S$, $H$ denote batch size, sequence length, and number of heads, respectively.

\subsection{Tensor Core Alignment}

NVIDIA Tensor Cores perform matrix-multiply-accumulate (MMA) operations on fixed tile sizes.
For FP16 on A100, the optimal tile requires $K \mod 16 = 0$.
Irregular dimensions force either padding (wasted compute) or fallback to scalar paths.

\subsection{FlashAttention Constraints}

FlashAttention-2~\cite{flashattention2} (v2.7.4) is the de facto standard for efficient attention.
Contrary to common belief, it does \emph{not} strictly require 8-aligned dimensions---it remains available for all tested dimensions (104--128).
However, it uses internal slow paths for non-8-aligned dimensions, causing 30--45\% overhead.
Optimized kernels exist for $\{32, 64, 96, 128, 256\}$. MEM\_EFFICIENT strictly requires 8-alignment.
\emph{Note: Results are specific to FlashAttention 2.7.4; future versions may implement internal alignment handling that changes these behaviors.}

\subsection{Low-Rank Compression}

PaLU~\cite{palu} compresses attention by applying SVD to K/V projections:
$W_{kv} \approx U_r \Sigma_r V_r^T$ where $r < d$.
The compressed head dimension becomes $r$, which is typically not aligned.

%% ===========================================
%% 3. DIMENSIONAL COLLAPSE PHENOMENON
%% ===========================================
\section{Dimensional Collapse}
\label{sec:phenomenon}

\subsection{Experiment Setup}

We conduct experiments on NVIDIA A100-80GB with PyTorch 2.9.1, CUDA 12.8, and FlashAttention 2.7.4.
All benchmarks use FP16 with CUDA event timing (warmup=50, measure=200, trials=3). Driver: 560.35.03; cuDNN 9.1.0.
\emph{Note on variance}: GPU measurements exhibit 5--8\% run-to-run variance due to thermal throttling and memory state. We report results from independent experimental runs; tables show consistent trends despite minor variance.

\subsection{Scope and Dimension Distribution}
\label{sec:scope}

\textbf{Scope:} The 96.9\% misalignment comes from \emph{theoretical} Fisher-information-based ranks (Figure~\ref{fig:palu_dist}).
All 24 available PaLU checkpoints use 32-multiple alignment. Our findings apply to: (1) vanilla SVD, (2) future methods relaxing constraints, and (3) RAP SVD~\cite{rap}, which we validated produces 100\% misaligned dimensions ($d$=102 for $r$=0.8).


\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig3_palu_dist.pdf}
\caption{\textbf{THEORETICAL (Unconstrained SVD)} dimension distribution from Fisher-information-based rank allocation (Llama-3-8B, $r$=0.8). 96.9\% of 512 KV head dimensions would be misaligned if compression used mathematically optimal ranks \emph{without alignment constraints}. \textbf{Production PaLU checkpoints enforce 32-multiple alignment}; this figure represents the \emph{motivation} for methods that may relax such constraints.}
\label{fig:palu_dist}
\end{figure}

\subsection{SDPA Latency vs. Head Dimension}

We sweep \texttt{head\_dim} from 64 to 160 with shape $B=4, S=2048, H=32$.
Figure~\ref{fig:sdpa_latency} shows the results.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig2_sdpa_latency.pdf}
\caption{SDPA latency across head dimensions. Shaded regions show $\pm$1 std over 3 trials $\times$ 200 iterations per trial. Clear alignment cliffs (``staircase effect'') visible at non-8-aligned values. $d$=107 shows 88\% increase vs $d$=96.}
\label{fig:sdpa_latency}
\end{figure}

8-aligned dimensions achieve 1.1--1.6ms while non-8-aligned incur 1.6--2.2ms. \texttt{head\_dim=107} shows 2.147ms (+88\% vs 96).

\subsection{Backend Selection Behavior}

Table~\ref{tab:backend} shows latency across different SDPA backends.

\begin{table}[t]
\centering
\caption{SDPA backend latency (ms$\pm$std) for various head dimensions. Measurements: 200 iterations $\times$ 3 trials.}
\label{tab:backend}
\begin{tabular}{lrrrr}
\toprule
$d$ & AUTO & FLASH & MEM\_EFF & MATH \\
\midrule
96  & 1.17{\scriptsize$\pm$.03} & 1.12{\scriptsize$\pm$.02} & 2.38{\scriptsize$\pm$.05} & 26.0{\scriptsize$\pm$.2} \\
104 & 1.54{\scriptsize$\pm$.04} & 1.54{\scriptsize$\pm$.04} & 2.75{\scriptsize$\pm$.06} & 26.5{\scriptsize$\pm$.2} \\
\textbf{107} & \textbf{2.14}{\scriptsize$\pm$.06} & \textbf{2.14}{\scriptsize$\pm$.06} & \multicolumn{1}{c}{N/A$^*$} & \textbf{27.0}{\scriptsize$\pm$.2} \\
112 & 1.53{\scriptsize$\pm$.04} & 1.53{\scriptsize$\pm$.04} & 2.60{\scriptsize$\pm$.05} & 27.1{\scriptsize$\pm$.2} \\
128 & 1.47{\scriptsize$\pm$.03} & 1.47{\scriptsize$\pm$.03} & 2.55{\scriptsize$\pm$.05} & 28.1{\scriptsize$\pm$.2} \\
\bottomrule
\multicolumn{5}{l}{\scriptsize $^*$MEM\_EFFICIENT unavailable: requires strict 8-alignment ($d$=107 is not 8-aligned).}
\end{tabular}
\end{table}

The MATH backend is 12.6$\times$ slower than FLASH for $d$=107.
If FlashAttention cannot handle a dimension, catastrophic fallback occurs.

%% ===========================================
%% 4. ROOT CAUSE ANALYSIS
%% ===========================================
\section{Root Cause Analysis}
\label{sec:causes}

We investigate the causes of dimensional collapse across three layers.

\subsection{PyTorch Backend Selection}
\label{sec:backend}

We tested backend availability for \texttt{head\_dim} $\in$ [104, 128].
Surprisingly, FlashAttention is available for \emph{all} dimensions (100\% for both 8-aligned and non-8-aligned), while MEM\_EFFICIENT requires strict 8-alignment.
FlashAttention does \emph{not} fall back to MATH; instead, it uses internal slow paths incurring 30--45\% overhead (8-aligned: 1.55ms avg, non-8-aligned: 2.03ms avg).
The root cause lies in the CUDA kernel layer, not backend selection.

\subsection{CUDA Kernel Layer}
\label{sec:cuda}

FlashAttention's internal 30--45\% slowdown stems from: (1) vectorized loads falling back to scalar (50\% loss when $d \mod 8 \neq 0$); (2) suboptimal GEMM tile selection reducing Tensor Core utilization (30\%$\to$12\%); (3) boundary predication causing warp divergence.
FlashAttention-2 dispatches optimized kernels for $d \in \{32, 64, 96, 128, 256\}$; other values use generic kernels ($d$=128: 1.47ms, $d$=125: 1.97ms, +34\%).\footnote{FlashAttention kernel dispatch: \texttt{csrc/flash\_attn/flash\_fwd\_hdim*.cu} in \url{https://github.com/Dao-AILab/flash-attention}. Head dimension determines which optimized kernel template is instantiated.}

\subsection{Hardware Constraints}
\label{sec:hardware}

We conduct controlled experiments (C23) to isolate hardware-level causes of dimensional collapse.
Figure~\ref{fig:root_cause} visualizes the impact of each hypothesis, and Table~\ref{tab:hardware} provides detailed metrics.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig4_root_cause.pdf}
\caption{Root cause breakdown. Tensor Core alignment (58\%), vectorized load degradation (50\%), and SDPA bandwidth (40\%) are the primary causes. L2 cache sector waste (5.8\%) is negligible.}
\label{fig:root_cause}
\end{figure}

\begin{table}[t]
\centering
\caption{Hardware layer root cause analysis (C23 experiment). Impact measured on A100 with FP16.}
\label{tab:hardware}
\small
\begin{tabular}{@{}llrl@{}}
\toprule
Hypothesis & Status & Impact & Root Cause \\
\midrule
H1: TC K\%16 & \textbf{Confirmed} & 58\% & Util. 30\%$\to$12\% \\
H2: L2 sector & Not confirmed & 5.8\% & Negligible \\
H3: SDPA BW & \textbf{Confirmed} & 40\% & Access pattern \\
H4: Vec. loads & \textbf{Confirmed} & 50\% & float4$\to$scalar \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{H1: Tensor Core Alignment (Confirmed).}
GEMM with K=16-aligned achieves 91 TFLOPS; non-aligned (K=107) drops to 37--40 TFLOPS (58\% slowdown, TC utilization 30\%$\to$12\%).

\paragraph{H2: L2 Cache Sectors (Not Confirmed).}
L2 sector waste ($\sim$5.8\%) cannot explain 30--58\% gaps; measured bandwidth is similar.

\paragraph{H3: SDPA Bandwidth Efficiency (Confirmed).}
$d$=112 achieves 153.6 GB/s; $d$=113 drops to 107.3 GB/s (--30\%). $d$=120 achieves 160.2 GB/s; $d$=121 drops to 118.5 GB/s (--26\%).

\paragraph{H4: Vectorized Loads (Confirmed).}
\texttt{float4} loads (K\%16) achieve 73--83 TFLOPS; scalar fallback (K=107) drops to 39--40 TFLOPS (50\% loss).

\textbf{Summary}: Tensor Core alignment (58\%), vectorized loads (50\%), and SDPA bandwidth (40\%) are primary causes; L2 cache (5.8\%) is negligible.

%% ===========================================
%% 5. SHAPE-AWARE COMPRESSION
%% ===========================================
\section{Shape-Aware Compression}
\label{sec:solution}

\subsection{Shape Contract}

We formalize alignment requirements: given an original dimension $d_{orig}$, we pad to $d_{pad} = \lceil d_{orig}/a \rceil \times a$ where $a$ is the alignment target.
The \textbf{MINIMAL} strategy uses $a=8$ (required for MEM\_EFFICIENT backend and vectorized loads), while \textbf{OPTIMAL} uses $a=16$ (maximizes Tensor Core utilization).
This minimizes memory overhead while guaranteeing hardware compatibility.

\subsection{Dimension Repair}

For a linear layer $y = Wx + b$ with $W \in \mathbb{R}^{d_{out} \times d_{in}}$, we pad the output dimension to the nearest aligned value $d'_{out} = \lceil d_{out}/a \rceil \times a$ by appending zero rows to $W$ and zeros to $b$.

\paragraph{Accuracy Preservation.}
Zero-padding preserves outputs exactly: $y' = [Wx + b; \mathbf{0}]$, where the original $y$ occupies positions $[0:d_{out}]$.
For attention, zero-valued dimensions contribute nothing to scores, making padding semantically neutral.
This ensures \textbf{bit-exact output preservation}---no retraining required.

%% ===========================================
%% 6. EVALUATION
%% ===========================================
\section{Evaluation}
\label{sec:eval}

We validate dimension repair at kernel level (SDPA/GEMM microbenchmarks), demonstrating 25--30\% recovery.
We also contextualize compression benefits with PaLU as an orthogonal study.

\subsection{Padding Rescue Experiment (P1)}

Table~\ref{tab:padding} shows the effect of padding $d$=107 to aligned values.

\begin{table}[t]
\centering
\caption{Padding rescue results for SDPA ($d$=107 logical). Measurements: 200 iterations $\times$ 3 trials. Data consistent with Table~\ref{tab:repair_perf}.}
\label{tab:padding}
\small
\begin{tabular}{lrrr}
\toprule
Phys. $d$ & Mem. Ovhd. & Latency (ms$\pm$std) & Speedup \\
\midrule
107 (base) & 0\% & 2.064{\scriptsize$\pm$.06} & 1.00$\times$ \\
112 & 4.7\% & 1.490{\scriptsize$\pm$.04} & 1.39$\times$ \\
128 & 19.6\% & 1.506{\scriptsize$\pm$.04} & 1.37$\times$ \\
\bottomrule
\end{tabular}
\end{table}

Padding to 112 achieves 27.8\% speedup with only 4.7\% memory overhead---an excellent tradeoff.

\subsection{GEMM Alignment Impact}

GEMM operations show similar patterns: K=107 achieves 0.089ms latency, while K=112 and K=128 both achieve 0.050ms---a \textbf{44\% improvement} from alignment.

\subsection{Dimension Repair Validation (C4)}

We validate our dimension repair implementation on PaLU-compressed dimensions.
Figure~\ref{fig:repair_tradeoff} visualizes the speedup vs. memory overhead tradeoff for different strategies.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig5_repair_tradeoff.pdf}
\caption{Speedup vs.\ memory overhead tradeoff for dimension repair. Points labeled with original dimensions; $d$=120 (already 8-aligned) shows 0\% MINIMAL speedup, validating alignment hypothesis. MINIMAL achieves 6.9$\times$ ROI; OPTIMAL provides 4.0$\times$ ROI.}
\label{fig:repair_tradeoff}
\end{figure}

Table~\ref{tab:repair_perf} shows SDPA performance for repaired dimensions.
Memory overhead: MINIMAL 3.72\%, OPTIMAL 7.20\%.

\begin{table}[t]
\centering
\caption{SDPA latency (ms$\pm$std) before and after dimension repair ($B$=4, $S$=2048, $H$=32). Run-to-run variance ($\sim$6\%) vs.\ Table~\ref{tab:padding} is within normal GPU measurement variability (see~\S\ref{sec:phenomenon}.1).}
\label{tab:repair_perf}
\begin{tabular}{lrrrrr}
\toprule
$d$ & Original & Minimal & Optimal & $\Delta$Min & $\Delta$Opt \\
\midrule
107 & 2.064{\scriptsize$\pm$.06} & 1.490{\scriptsize$\pm$.04} & 1.506{\scriptsize$\pm$.04} & \textbf{+27.8\%} & +27.0\% \\
114 & 2.049{\scriptsize$\pm$.06} & 1.549{\scriptsize$\pm$.04} & 1.432{\scriptsize$\pm$.04} & +24.4\% & \textbf{+30.1\%} \\
117 & 2.054{\scriptsize$\pm$.06} & 1.567{\scriptsize$\pm$.04} & 1.433{\scriptsize$\pm$.04} & +23.7\% & \textbf{+30.2\%} \\
120 & 1.557{\scriptsize$\pm$.04} & 1.557{\scriptsize$\pm$.04} & 1.428{\scriptsize$\pm$.04} & 0\% & +8.3\% \\
121 & 1.964{\scriptsize$\pm$.05} & 1.430{\scriptsize$\pm$.04} & 1.441{\scriptsize$\pm$.04} & \textbf{+27.2\%} & +26.6\% \\
125 & 1.975{\scriptsize$\pm$.05} & 1.439{\scriptsize$\pm$.04} & 1.439{\scriptsize$\pm$.04} & \textbf{+27.1\%} & +27.1\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Findings.}
MINIMAL achieves 23--28\% speedup with 3.72\% overhead (ROI = speedup/memory = 6.9$\times$).
$d$=120 validates alignment: 8-aligned (0\% MINIMAL gain) but OPTIMAL pads to 128 for +8.3\%.

\subsection{Orthogonal Study: PaLU Compression Benefits}

For context, we compare Llama-3-8B baseline vs.\ PaLU (ratio=0.7) on A100 80GB (Table~\ref{tab:e2e}, Figure~\ref{fig:e2e}).
PaLU achieves \textbf{11.5$\times$ decode speedup} via KV cache compression.
Since PaLU uses 32-multiple alignment, these results are orthogonal to dimension repair---for methods without alignment constraints, kernel-level experiments suggest additional 25--30\% improvement.

\begin{table}[t]
\centering
\caption{PaLU \emph{compression} benefit (orthogonal to repair). 11.5$\times$ decode speedup from reduced KV cache.}
\label{tab:e2e}
\small
\begin{tabular}{lrrr}
\toprule
Metric & Baseline & PaLU & $\Delta$ \\
\midrule
Prefill (tok/s) & 9870 & 9672 & --2.0\% \\
Decode (tok/s) & 119 & 1371 & +11.5$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig6_e2e.pdf}
\caption{PaLU compression benefit (orthogonal to repair). 11.5$\times$ decode speedup from KV cache compression.}
\label{fig:e2e}
\end{figure}

\subsection{Accuracy Preservation}

Zero-padding guarantees \textbf{bit-exact output preservation}: $y'[0:d_{out}] = y$ exactly.
Unit tests confirm identical outputs (30/30 passed).
WikiText-2 perplexity validation on RAP SVD ($r$=0.8, $d$=102, 100\% misaligned) confirms repair produces \textbf{identical perplexity}: baseline 11.08, RAP SVD 92.39, RAP SVD + repair 92.39 (higher PPL from compression, not repair).

\subsection{Limitations}
\label{sec:limitations}

\textbf{(1) E2E integration}: Kernel-level speedups (25--30\%) not yet integrated into PaLU's $W = U \cdot V^T$ structure.
\textbf{(2) Scope}: The 96.9\% figure is theoretical; production PaLU uses alignment constraints. RAP SVD validated as real-world misaligned example.
\textbf{(3) Downstream}: Perplexity validated; comprehensive task evaluation is future work.

%% ===========================================
%% 7. RELATED WORK
%% ===========================================
\section{Related Work}
\label{sec:related}

\paragraph{LLM Compression.}
Post-training compression methods---SparseGPT~\cite{sparsegpt} (pruning), GPTQ~\cite{gptq}/AWQ~\cite{awq} (quantization), and PaLU~\cite{palu} (low-rank)---optimize for accuracy-compression trade-offs but largely ignore hardware alignment.
\emph{Which methods produce misaligned dimensions?} SVD-based approaches (PaLU, vanilla SVD) can theoretically produce irregular dimensions. However, PaLU's production checkpoints use internal quantization that enforces 32-multiple alignment. GPTQ and AWQ operate on fixed-width groups (typically 128) and do not alter tensor dimensions. Unstructured pruning (SparseGPT) can create irregular sparsity patterns but typically preserves dimensions.
Our work targets compression methods that \emph{do not} include alignment constraints, including vanilla SVD and future methods that may relax constraints for better compression ratios.

\paragraph{KV Cache \& Attention Optimization.}
MQA~\cite{mqa}, GQA~\cite{gqa}, and StreamingLLM~\cite{streaminglm} reduce KV cache while preserving standard dimensions.
FlashAttention~\cite{flashattention,flashattention2} is tuned for $\{32, 64, 96, 128, 256\}$, with cliffs for other values.
SVD-based compression produces irregular dimensions that violate these implicit constraints.

\paragraph{Inference Frameworks.}
TensorRT~\cite{tensorrt}, vLLM~\cite{vllm}, and TGI apply runtime optimizations but typically assume aligned dimensions.
TensorRT may perform implicit runtime padding, but this is opaque and incurs per-inference overhead.
Our approach differs: compile-time repair (1) makes alignment explicit and controllable, (2) enables better kernel selection by the framework, and (3) avoids repeated runtime padding cost.
vLLM and TGI rely on the underlying attention implementation (FlashAttention/xformers); our findings on slow paths apply directly.

\paragraph{Positioning.}
Unlike prior accuracy-compression trade-off studies, \textbf{we focus on performance-alignment trade-offs}---compressed models with fewer FLOPs can run slower due to hardware misalignment.

%% ===========================================
%% 8. CONCLUSION
%% ===========================================
\section{Conclusion}
\label{sec:conclusion}

We identified \emph{dimensional collapse} as a critical but overlooked problem in LLM compression.
Root causes: FlashAttention slow paths (+30--45\%), Tensor Core alignment (58\% slowdown), and vectorized load degradation (50\% loss). L2 cache waste (5.8\%) is negligible.

Our dimension repair achieves 25--28\% kernel-level speedup with 3.72\% memory overhead (6.9$\times$ ROI).
RAP SVD validation confirms 100\% misaligned dimensions ($d$=102) in real compression; perplexity evaluation confirms bit-exact preservation.
PaLU's internal alignment makes its 11.5$\times$ decode speedup orthogonal to our repair contribution.
Integration into SVD structures and H100+ generalization remain future work.

%% ===========================================
%% REFERENCES
%% ===========================================
\clearpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
