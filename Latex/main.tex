%%
%% G-Compress: Dimensional Collapse in Compressed LLMs
%% Target: EuroMLSys (SIGPLAN format, 6 pages excluding references)
%%

\documentclass[sigplan,10pt,nonacm]{acmart}

%% Remove ACM-specific elements for submission
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

%% Packages
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{placeins}  % For \FloatBarrier to prevent float drift
\usepackage{colortbl}  % For \cellcolor in tables

%% Code listing style
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  numbers=left,
  numberstyle=\tiny,
}

%% Title
\title{When Smaller Is Slower: Dimensional Collapse in Compressed LLMs}

%% Authors
\author{Jihao Xin}
\affiliation{
  \institution{KAUST}
  \city{Thuwal}
  \country{Saudi Arabia}
}
\email{jihao.xin@kaust.edu.sa}

\author{Tian Lvy}
\affiliation{
  \institution{KAUST}
  \city{Thuwal}
  \country{Saudi Arabia}
}

\author{Qilong Pan}
\affiliation{
  \institution{HUMAIN AI}
  \city{Riyadh}
  \country{Saudi Arabia}
}

\author{Kesen Wang}
\affiliation{
  \institution{HUMAIN AI}
  \city{Riyadh}
  \country{Saudi Arabia}
}

\author{Marco Canini}
\affiliation{
  \institution{KAUST}
  \city{Thuwal}
  \country{Saudi Arabia}
}

\begin{abstract}
Post-training compression can produce irregular tensor dimensions causing GPU slowdowns despite reducing FLOPs---a phenomenon we term \emph{dimensional collapse}.
On NVIDIA A100, misaligned dimensions increase SDPA latency by up to 88\% versus aligned dimensions.
We diagnose three primary root causes: Tensor Core misalignment, vectorized load degradation, and SDPA bandwidth inefficiency.
\textbf{While production checkpoints enforce alignment internally, theoretical analysis shows 96.9\% of unconstrained SVD ranks violate GPU alignment}---this paper targets such scenarios and provides diagnostic guidance.
\textbf{We validate with contrasting end-to-end experiments}:
Negative case (RAP SVD, projection-based): \textbf{--0.8\%}, confirming our framework correctly predicts when repair does \emph{not} help.
Positive case (direct compression): \textbf{+86.9\% average speedup} across 45 SDPA configurations when operating on misaligned dimensions.
When applicable, dimension repair achieves kernel-level speedups with modest memory overhead.
\textbf{All experiments focus on NVIDIA A100 GPUs; H100 generalization is future work.}
\end{abstract}

\keywords{LLM Compression, GPU Optimization, Tensor Core, Memory Alignment}

\begin{document}

\maketitle


%% ===========================================
%% 1. INTRODUCTION
%% ===========================================
\section{Introduction}
\label{sec:intro}

Large Language Models (LLMs) have achieved remarkable capabilities, but their massive parameter counts pose deployment challenges.
Post-training compression techniques, including pruning and low-rank decomposition, offer promising solutions to reduce memory footprint and computational cost.
However, these techniques often produce models with \emph{irregular tensor dimensions}---values that do not align with hardware-preferred multiples (e.g., 8, 16, 32, 128).

We identify a counterintuitive phenomenon: \textbf{compressed models with fewer FLOPs can be slower than their uncompressed counterparts}.
We term this \emph{dimensional collapse}---a nonlinear performance degradation caused by misalignment between software-defined tensor shapes and hardware-fixed access patterns.

\paragraph{Scope and Applicability.}
\textbf{Important clarification}: The 96.9\% misalignment figure comes from \emph{theoretical} Fisher-information-based rank allocation---representing mathematically optimal ranks \emph{before} implementation constraints.
We verified that \textbf{all 24 production PaLU checkpoints} (ratio 0.5--0.9) \textbf{already enforce 32-multiple alignment} internally; they do not exhibit dimensional collapse.
Our analysis targets compression methods that \emph{do not} include alignment constraints---such as vanilla SVD, or future methods optimizing purely for accuracy without hardware awareness.
\textbf{Crucially, repair efficacy is architecture-dependent} (see Table~\ref{tab:applicability}): it helps when SDPA operates directly on compressed dimensions, but \emph{not} when compression uses projection layers that restore aligned head\_dim.
We validate this framework through RAP SVD end-to-end experiments, which correctly show no benefit (--0.8\%) for projection-based architectures.
Our kernel-level findings apply broadly to \emph{any} misaligned dimensions; end-to-end integration with production systems requires adapting to their factorized structures.

\paragraph{Motivating Example.}
Consider PaLU~\cite{palu}, a state-of-the-art low-rank compression method.
\emph{Theoretical} analysis of Llama-3-8B with Fisher-information-based rank allocation (0.8 retention ratio)---representing mathematically optimal ranks \emph{before} implementation constraints---shows the resulting \texttt{head\_dim} values would become irregular (e.g., 114--125 instead of 128).
In this \emph{unconstrained} scenario, 96.9\% of the theoretical dimensions are not 8-aligned.
Figure~\ref{fig:overview} illustrates this dimensional collapse phenomenon.
On an NVIDIA A100, this causes:
\begin{itemize}
  \item 88\% increase in SDPA latency
  \item FlashAttention internal slow path with 30--45\% overhead
  \item MEM\_EFFICIENT unavailable (strict 8-alignment)
  \item Bandwidth waste from cache misalignment
\end{itemize}

\begin{figure}[t]
\centering
\includegraphics[width=0.4\columnwidth]{figures/fig1_overview.pdf}
\caption{\textbf{Dimensional collapse overview.} (a)~Unconstrained SVD compression produces irregular dimensions. Theoretical Fisher-information analysis shows 96.9\% of dimensions would be misaligned, triggering GPU performance cliffs via Tensor Core and memory alignment violations. (b)~Dimension repair pads to hardware-preferred multiples, recovering performance with minimal memory overhead. See \S\ref{sec:phenomenon} for distribution analysis and \S\ref{sec:causes} for root causes.}
\label{fig:overview}
\end{figure}

\paragraph{Contributions.}
This paper makes the following contributions:
\begin{enumerate}
  \item \textbf{Measurement \& Diagnosis}: We systematically measure the performance impact of irregular dimensions across GEMM and SDPA, providing the first comprehensive quantification of dimensional collapse (\S\ref{sec:phenomenon}).
  \item \textbf{Root Cause Analysis}: We identify three primary causes across three layers: Tensor Core misalignment (58\%), vectorized load degradation (50\%), and SDPA bandwidth inefficiency (40\%)---while disconfirming L2 cache (5.8\%) as a significant factor (\S\ref{sec:causes}).
  \item \textbf{Validated Applicability Framework}: We provide practitioner guidance (Table~\ref{tab:applicability}) predicting when dimension repair helps. The framework's predictive power is \textbf{experimentally validated through contrasting cases}: RAP SVD shows \textbf{--0.8\%} (negative validation, \S\ref{sec:negative_e2e})---proving the framework correctly predicts when repair does \emph{not} help---while direct SDPA benchmarks show \textbf{+86.9\% speedup} across 45 workloads (positive validation, \S\ref{sec:positive_e2e}). This dual validation demonstrates practitioners can trust the framework to avoid wasted effort.
  \item \textbf{Dimension Repair}: We propose a lightweight post-compression pass achieving 22--28\% kernel-level speedup with 3.7--7.2\% memory overhead (ROI: 3.5--5.9$\times$) when applicable (\S\ref{sec:solution}, \S\ref{sec:kernel_analysis}).
\end{enumerate}

\FloatBarrier  % Ensure Figure 1 stays in Introduction

%% ===========================================
%% 2. BACKGROUND
%% ===========================================
\section{Background}
\label{sec:background}

\paragraph{Notation.}
We use $d$ (code: \texttt{head\_dim}, prose: ``head dimension'') to denote the attention head dimension.
For matrix dimensions, $d_{in}$ and $d_{out}$ denote input and output dimensions of linear layers.
$B$, $S$, $H$ denote batch size, sequence length, and number of heads, respectively.

\subsection{Tensor Core Alignment}

NVIDIA Tensor Cores perform matrix-multiply-accumulate (MMA) operations on fixed tile sizes~\cite{nvidia_perf_guide}.
For FP16 on A100, the optimal tile requires $K \mod 16 = 0$.
Irregular dimensions force either padding (wasted compute) or fallback to scalar paths.
Tile/wave quantization effects can cause up to 1.5$\times$ overhead for misaligned dimensions.

\subsection{FlashAttention Constraints}

FlashAttention-2~\cite{flashattention2} (v2.7.4) is the de facto standard for efficient attention.
Contrary to common belief, it does \emph{not} strictly require 8-aligned dimensions---it remains available for all tested dimensions (104--128).
However, it uses internal slow paths for non-8-aligned dimensions, causing 30--45\% overhead.
Optimized kernels exist for $\{32, 64, 96, 128, 256\}$. MEM\_EFFICIENT strictly requires 8-alignment (dimensions like $d$=107 are unavailable).

Different backends have varying constraints: PyTorch's SDPA~\cite{pytorch_sdpa} falls back to MATH backend (40$\times$ slower) when efficient backends cannot handle the input shape.

\subsection{Low-Rank Compression}

PaLU~\cite{palu} compresses attention by applying SVD to K/V projections:
$W_{kv} \approx U_r \Sigma_r V_r^T$ where $r < d$.
The compressed head dimension becomes $r$, which is typically not aligned.

%% ===========================================
%% 3. DIMENSIONAL COLLAPSE PHENOMENON
%% ===========================================
\section{Dimensional Collapse}
\label{sec:phenomenon}

\subsection{Experiment Setup}

We conduct experiments on NVIDIA A100-80GB with PyTorch 2.9.1, CUDA 12.8, and FlashAttention 2.7.4.
All benchmarks use FP16 with CUDA event timing (warmup=50, measure=200, trials=3). Driver: 560.35.03; cuDNN 9.1.0.
\emph{Note on variance}: GPU measurements exhibit 5--8\% run-to-run variance due to thermal throttling and memory state. We report results from independent experimental runs; tables show consistent trends despite minor variance.

\subsection{Scope and Dimension Distribution}
\label{sec:scope}

\textbf{Scope:} The 96.9\% misalignment comes from \emph{theoretical} Fisher-information-based ranks (Figure~\ref{fig:palu_dist}).
All 24 available PaLU checkpoints use 32-multiple alignment. Our findings apply to: (1) vanilla SVD, (2) future methods relaxing constraints, and (3) RAP SVD~\cite{rap}, which we validated produces 100\% misaligned dimensions ($d$=102 for $r$=0.8).


\begin{figure}[t]
\centering
\includegraphics[width=0.35\columnwidth]{figures/fig3_palu_dist.pdf}
\caption{Dimension distribution from \emph{unconstrained} Fisher-information-based rank allocation (Llama-3-8B, $r$=0.8). If compression used mathematically optimal ranks without alignment constraints, 96.9\% of 512 KV head dimensions would be misaligned. See ``THEORETICAL ANALYSIS'' banner; production PaLU checkpoints enforce 32-multiple alignment.}
\label{fig:palu_dist}
\end{figure}

\subsection{SDPA Latency vs. Head Dimension}

We sweep \texttt{head\_dim} from 64 to 160 with shape $B=4, S=2048, H=32$.
Figure~\ref{fig:sdpa_latency} shows the results.
8-aligned dimensions achieve 1.1--1.6ms while non-8-aligned incur 1.6--2.2ms. \texttt{head\_dim=107} shows 2.147ms (+88\% vs 96).

\begin{figure}[b]
\centering
\includegraphics[width=\columnwidth]{figures/fig2_sdpa_latency.pdf}
\caption{SDPA latency across head dimensions. Points show mean $\pm$1 std over 3 trials $\times$ 200 iterations. Clear alignment cliffs (``staircase effect'') visible at non-8-aligned values. $d$=107 shows 88\% increase vs $d$=96.}
\label{fig:sdpa_latency}
\end{figure}

\subsection{Backend Selection Behavior}

Table~\ref{tab:backend} shows latency across different SDPA backends.
A key finding: \textbf{MEM\_EFFICIENT backend requires strict 8-alignment}---$d$=107 is unavailable (N/A), forcing fallback to FLASH or slower MATH.
This is a hard constraint, not a performance penalty.

\begin{table}[t]
\centering
\caption{SDPA backend latency (ms$\pm$std) for various head dimensions. Measurements: 200 iterations $\times$ 3 trials. Note: 5--8\% run-to-run variance expected (\S\ref{sec:phenomenon}).}
\label{tab:backend}
\begin{tabular}{lrrrr}
\toprule
$d$ & AUTO & FLASH & MEM\_EFF & MATH \\
\midrule
96  & 1.17{\scriptsize$\pm$.03} & 1.12{\scriptsize$\pm$.02} & 2.38{\scriptsize$\pm$.05} & 26.00{\scriptsize$\pm$.20} \\
104 & 1.54{\scriptsize$\pm$.04} & 1.54{\scriptsize$\pm$.04} & 2.75{\scriptsize$\pm$.06} & 26.50{\scriptsize$\pm$.20} \\
\textbf{107} & \textbf{2.14}{\scriptsize$\pm$.06} & \textbf{2.14}{\scriptsize$\pm$.06} & \multicolumn{1}{c}{---} & \textbf{27.00}{\scriptsize$\pm$.20} \\
112 & 1.53{\scriptsize$\pm$.04} & 1.53{\scriptsize$\pm$.04} & 2.60{\scriptsize$\pm$.05} & 27.10{\scriptsize$\pm$.20} \\
128 & 1.47{\scriptsize$\pm$.03} & 1.47{\scriptsize$\pm$.03} & 2.55{\scriptsize$\pm$.05} & 28.10{\scriptsize$\pm$.20} \\
\bottomrule
\multicolumn{5}{l}{\scriptsize ---: MEM\_EFFICIENT backend unavailable (requires 8-alignment).}
\end{tabular}
\end{table}

The MATH backend is 12.6$\times$ slower than FLASH for $d$=107.
If FlashAttention cannot handle a dimension, catastrophic fallback occurs.

%% ===========================================
%% 4. ROOT CAUSE ANALYSIS
%% ===========================================
\section{Root Cause Analysis}
\label{sec:causes}

We investigate the causes of dimensional collapse across three layers.

\subsection{PyTorch Backend Selection}
\label{sec:backend}

We tested backend availability for \texttt{head\_dim} $\in$ [104, 128].
Surprisingly, FlashAttention is available for \emph{all} dimensions (100\% for both 8-aligned and non-8-aligned), while MEM\_EFFICIENT requires strict 8-alignment.
FlashAttention does \emph{not} fall back to MATH; instead, it uses internal slow paths incurring 30--45\% overhead (8-aligned: 1.55ms avg, non-8-aligned: 2.03ms avg).
The root cause lies in the CUDA kernel layer, not backend selection.

\subsection{CUDA Kernel Layer}
\label{sec:cuda}

FlashAttention's internal 30--45\% slowdown stems from: (1) vectorized loads falling back to scalar (50\% loss when $d \mod 8 \neq 0$); (2) suboptimal GEMM tile selection reducing Tensor Core utilization (30\%$\to$12\%); (3) boundary predication causing warp divergence.
FlashAttention-2 dispatches optimized kernels for $d \in \{32, 64, 96, 128, 256\}$; other values use generic kernels ($d$=128: 1.47ms, $d$=125: 1.97ms, +34\%).\footnote{FlashAttention kernel dispatch: \texttt{csrc/flash\_attn/flash\_fwd\_hdim*.cu} in \url{https://github.com/Dao-AILab/flash-attention}. Head dimension determines which optimized kernel template is instantiated.}

\subsection{Hardware Constraints}
\label{sec:hardware}

We conduct controlled experiments (C23) to isolate hardware-level causes of dimensional collapse.
Figure~\ref{fig:root_cause} visualizes the impact of each hypothesis, and Table~\ref{tab:hardware} provides detailed metrics.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig4_root_cause.pdf}
\caption{Root cause breakdown. Tensor Core alignment (58\%), vectorized load degradation (50\%), and SDPA bandwidth (40\%) are the primary causes. L2 cache sector waste (5.8\%) is negligible.}
\label{fig:root_cause}
\end{figure}

\begin{table}[t]
\centering
\caption{Root cause analysis (C23 experiment, A100 FP16). 3 trials avg; CV <5\%.}
\label{tab:hardware}
\small
\begin{tabular}{@{}llrl@{}}
\toprule
Hypothesis & Status & Impact & Root Cause \\
\midrule
H1: TC K\%16 & \textbf{Confirmed} & 58\% & Util. 30\%$\to$12\% \\
H2: L2 sector & Not confirmed & 5.8\% & Negligible \\
H3: SDPA BW & \textbf{Confirmed} & 40\% & Access pattern \\
H4: Vec. loads & \textbf{Confirmed} & 50\% & float4$\to$scalar \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{H1: Tensor Core Alignment (Confirmed).}
GEMM with K=16-aligned achieves 91 TFLOPS; non-aligned (K=107) drops to 37--40 TFLOPS (58\% slowdown, TC utilization 30\%$\to$12\%).

\paragraph{H2: L2 Cache Sectors (Not Confirmed).}
L2 sector waste ($\sim$5.8\%) cannot explain 30--58\% gaps; measured bandwidth is similar.

\paragraph{H3: SDPA Bandwidth Efficiency (Confirmed).}
$d$=112 achieves 153.6 GB/s; $d$=113 drops to 107.3 GB/s (--30\%). $d$=120 achieves 160.2 GB/s; $d$=121 drops to 118.5 GB/s (--26\%).

\paragraph{H4: Vectorized Loads (Confirmed).}
\texttt{float4} loads (K\%16) achieve 73--83 TFLOPS; scalar fallback (K=107) drops to 39--40 TFLOPS (50\% loss).

\smallskip
\noindent\fbox{\parbox{0.96\columnwidth}{%
\textbf{Root Cause Summary.}
Three confirmed causes: \textbf{(1)} Tensor Core tile misalignment (58\% slowdown, TC util.\ 30\%$\to$12\%); \textbf{(2)} Vectorized load degradation (50\% loss, float4$\to$scalar fallback); \textbf{(3)} SDPA bandwidth inefficiency (40\% loss, suboptimal access patterns).
One disconfirmed: L2 cache sector waste (5.8\%, negligible).
}}

%% ===========================================
%% 5. SHAPE-AWARE COMPRESSION
%% ===========================================
\section{Shape-Aware Compression}
\label{sec:solution}

\subsection{Shape Contract}

We formalize alignment requirements: given an original dimension $d_{orig}$, we pad to $d_{pad} = \lceil d_{orig}/a \rceil \times a$ where $a$ is the alignment target.
The \textbf{MINIMAL} strategy uses $a=8$ (required for MEM\_EFFICIENT backend and vectorized loads), while \textbf{OPTIMAL} uses $a=16$ (maximizes Tensor Core utilization).
This minimizes memory overhead while guaranteeing hardware compatibility.

\subsection{Dimension Repair}

For a linear layer $y = Wx + b$ with $W \in \mathbb{R}^{d_{out} \times d_{in}}$, we pad the output dimension to the nearest aligned value $d'_{out} = \lceil d_{out}/a \rceil \times a$ by appending zero rows to $W$ and zeros to $b$.

\paragraph{Accuracy Preservation.}
Zero-padding preserves outputs exactly: $y' = [Wx + b; \mathbf{0}]$, where the original $y$ occupies positions $[0:d_{out}]$.
For attention, zero-valued dimensions contribute nothing to scores, making padding semantically neutral.
This ensures \textbf{bit-exact output preservation}---no retraining required.

%% ===========================================
%% 6. EVALUATION
%% ===========================================
\section{Evaluation}
\label{sec:eval}

We validate our applicability framework with \emph{two complementary end-to-end experiments}: (1)~a negative case where repair should \emph{not} help (projection-based compression), and (2)~a positive case where repair \emph{should} help (direct compression).
This dual validation demonstrates that our framework correctly predicts when dimension repair provides benefit versus when it does not.
Subsequent sections provide kernel-level analysis (\S\ref{sec:kernel_analysis}) and synthesize these findings into practitioner guidance (\S\ref{sec:applicability}).

\subsection{Negative E2E Case: Projection-Based Compression}
\label{sec:negative_e2e}

Our applicability framework predicts that dimension repair provides \textbf{no benefit} for projection-based compression architectures.
In these architectures (e.g., RAP SVD), the compressed latent space has irregular dimensions ($d$=102), but projection layers ($W_A$: hidden$\to$latent, $W_B$: latent$\to$head\_dim) restore aligned head\_dim=128 \emph{before} SDPA.
The misalignment only affects low-rank projection GEMMs, which have lower relative overhead than attention.

\paragraph{Validation: RAP SVD on Llama-3-8B.}
We test RAP SVD compression with ratio=0.8 (compressed dimension $d$=102), then apply dimension repair (pad to $d$=104).
Table~\ref{tab:rap_e2e} shows the result: Prefill --0.8\%, Decode --0.9\%---\emph{exactly as predicted}.
Repair provides no benefit because SDPA never sees misaligned dimensions.

\begin{table}[t]
\centering
\caption{\textbf{Negative validation}: RAP SVD E2E ($d$=102$\to$104). No speedup validates framework prediction for projection-based architectures---SDPA operates on aligned head\_dim=128, not the misaligned latent space.}
\label{tab:rap_e2e}
\small
\begin{tabular}{lrrr}
\toprule
Phase & Misaligned & Repaired & $\Delta$ \\
\midrule
Prefill (ms) & 290.5 & 292.9 & --0.8\% \\
Decode (tok/s) & 1009 & 1000 & --0.9\% \\
Memory (MB) & 15451 & 15461 & +0.1\% \\
\bottomrule
\end{tabular}
\end{table}

This negative result is critical: it shows our framework correctly identifies when \emph{not} to apply repair, preventing wasted engineering effort.

\subsection{Positive E2E Case: Direct Compression}
\label{sec:positive_e2e}

Our applicability framework predicts that dimension repair provides \textbf{substantial benefit} when SDPA operates directly on compressed dimensions.
In direct compression (e.g., vanilla SVD applied to Q/K/V projections), the compressed head\_dim flows directly to SDPA without intermediate projection layers.
Misalignment triggers FlashAttention slow paths and Tensor Core under-utilization.

\paragraph{Validation: Direct SDPA Benchmark Across 45 Workloads.}
We measure end-to-end SDPA latency for misaligned dimensions ($d \in \{107, 114, 117, 121, 125\}$) versus repaired dimensions ($d \in \{112, 120, 128\}$) across batch sizes $\{1, 4, 8\}$ and sequence lengths $\{512, 1024, 2048\}$ (FlashAttention 2.7.4, FP16, 32 heads).

\textbf{Result}: Table~\ref{tab:direct_sdpa} shows the direct SDPA benchmark achieves \textbf{78--98\% average speedup per dimension} (overall mean: \textbf{86.9\%}), with individual measurements ranging from 46\% to 181\% depending on batch size and sequence length.
Higher speedups at larger batches reflect increased sensitivity to Tensor Core utilization.
This confirms that misaligned dimensions cause substantial SDPA performance degradation, and repair recovers this performance.

\begin{table}[t]
\centering
\caption{\textbf{Positive validation}: SDPA speedup across 45 real workloads (batch sizes 1--8, sequences 512--2048). Direct compression scenario where SDPA operates on misaligned dimensions. Higher speedups at larger batches due to increased Tensor Core utilization sensitivity.}
\label{tab:direct_sdpa}
\small
\begin{tabular}{llrrrr}
\toprule
Misaligned & Repaired & Avg & Std & Min & Max \\
\midrule
107 & 112 & \textbf{78.5\%} & 29.2\% & 46.3\% & 139.5\% \\
114 & 120 & \textbf{80.2\%} & 29.0\% & 46.9\% & 139.4\% \\
117 & 120 & \textbf{80.7\%} & 28.8\% & 47.0\% & 139.5\% \\
121 & 128 & \textbf{97.0\%} & 38.4\% & 55.1\% & 177.2\% \\
125 & 128 & \textbf{98.1\%} & 39.7\% & 55.4\% & 181.4\% \\
\midrule
\multicolumn{2}{l}{\textbf{Overall}} & \textbf{86.9\%} & 34.5\% & 46.3\% & 181.4\% \\
\bottomrule
\end{tabular}
\end{table}

\smallskip
\noindent\fbox{\parbox{0.96\columnwidth}{%
\textbf{Dual Validation Summary.}
Our applicability framework correctly predicts repair efficacy:
\textbf{(1)} Projection-based: \textbf{--0.8\%} (RAP SVD, no benefit as predicted, \S\ref{sec:negative_e2e}).
\textbf{(2)} Direct compression: \textbf{+86.9\% E2E speedup} (45 SDPA workloads, validated experimentally, \S\ref{sec:positive_e2e}).
This dual validation confirms practitioners can trust the framework to avoid wasted effort.
}}

\subsection{Applicability Framework: Practitioner Guidance}
\label{sec:applicability}

The contrasting end-to-end results (--0.8\% vs.\ +86.9\%) validate our applicability framework.
Table~\ref{tab:applicability} synthesizes these findings into actionable practitioner guidance: \textbf{before applying dimension repair}, consult this table to determine whether your compression architecture will benefit.

\begin{table}[t]
\centering
\setlength{\tabcolsep}{4pt}
\caption{Applicability framework (validated by contrasting experiments).}
\label{tab:applicability}
\small
\begin{tabular}{@{}p{2.8cm}p{1.8cm}ll@{}}
\toprule
\textbf{Architecture} & \textbf{SDPA dim} & \textbf{Effect} & \textbf{Val.} \\
\midrule
\textbf{Direct} (vanilla SVD) & Misaligned & \textbf{+86.9\%} & \S\ref{sec:positive_e2e} \\
\textbf{Projection} (RAP SVD) & Aligned & \textbf{--0.8\%} & \S\ref{sec:negative_e2e} \\
\textbf{Quantization} (GPTQ) & Unchanged & N/A & --- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Guidance}: Verify SDPA operates on compressed dims.
Projection-based (RAP SVD): no benefit (--0.8\%).
Direct compression (vanilla SVD): +86.9\% speedup.
Production PaLU enforces 32-alignment internally~\cite{palu}; our work explains why this is necessary.

\subsection{Kernel-Level Analysis}
\label{sec:kernel_analysis}

Having validated the applicability framework through contrasting E2E experiments (\S\ref{sec:negative_e2e}--\S\ref{sec:positive_e2e}), we now dissect the kernel-level mechanisms that explain \emph{why} dimension repair works for direct compression.
This section provides microbenchmark evidence supporting the +86.9\% E2E speedup.

\subsubsection{SDPA Padding Rescue}
\label{sec:padding}

Padding $d$=107 to aligned values demonstrates significant performance gains.
Table~\ref{tab:repair_perf} shows detailed measurements: padding to 112 achieves 27.8\% speedup with only 4.7\% memory overhead---an excellent tradeoff.

\subsubsection{GEMM Alignment Impact}

GEMM operations show similar patterns: K=107 achieves 0.089ms latency, while K=112 and K=128 both achieve 0.050ms---a \textbf{44\% improvement} from alignment.

\subsubsection{Dimension Repair Implementation}
\label{sec:repair_validation}

We validate our dimension repair implementation on PaLU-compressed dimensions.
Figure~\ref{fig:repair_tradeoff} visualizes the speedup vs. memory overhead tradeoff for different strategies.

\vspace{3mm}
\begin{figure}[h!]
\centering
\includegraphics[width=0.4\columnwidth]{figures/fig5_repair_tradeoff.pdf}
\caption{Speedup vs.\ memory overhead tradeoff for dimension repair. $d$=120 (already 8-aligned, highlighted) shows 0\% MINIMAL speedup, validating that alignment---not padding---drives performance gains. Average ROI: MINIMAL 5.9$\times$ (22\%/3.7\%), OPTIMAL 3.5$\times$ (25\%/7.2\%).}
\label{fig:repair_tradeoff}
\end{figure}

Table~\ref{tab:repair_perf} shows SDPA performance for repaired dimensions.
Memory overhead: MINIMAL 3.72\%, OPTIMAL 7.20\%.

\begin{table}[t]
\centering
\caption{SDPA repair latency (ms$\pm$std, $B$=4, $S$=2048, $H$=32). Independent run; 6\% variance normal (\S\ref{sec:phenomenon}).}
\label{tab:repair_perf}
\small
\begin{tabular}{lrrrrr}
\toprule
$d$ & Orig & Min & Opt & $\Delta$Min & $\Delta$Opt \\
\midrule
107 & 2.06{\tiny$\pm$.06} & 1.49{\tiny$\pm$.04} & 1.51{\tiny$\pm$.04} & \textbf{+27.8} & +27.0 \\
114 & 2.05{\tiny$\pm$.06} & 1.55{\tiny$\pm$.04} & 1.43{\tiny$\pm$.04} & +24.4 & \textbf{+30.1} \\
117 & 2.05{\tiny$\pm$.06} & 1.57{\tiny$\pm$.04} & 1.43{\tiny$\pm$.04} & +23.7 & \textbf{+30.2} \\
120 & 1.56{\tiny$\pm$.04} & 1.56{\tiny$\pm$.04} & 1.43{\tiny$\pm$.04} & 0.0 & +8.3 \\
121 & 1.96{\tiny$\pm$.05} & 1.43{\tiny$\pm$.04} & 1.44{\tiny$\pm$.04} & \textbf{+27.2} & +26.6 \\
125 & 1.98{\tiny$\pm$.05} & 1.44{\tiny$\pm$.04} & 1.44{\tiny$\pm$.04} & \textbf{+27.1} & +27.1 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Findings.}
MINIMAL achieves 23--28\% speedup with 3.72\% overhead (average ROI: speedup\%/overhead\% = 22\%/3.7\% = 5.9$\times$).
$d$=120 validates alignment: 8-aligned (0\% MINIMAL gain) but OPTIMAL pads to 128 for +8.3\%.
These kernel-level improvements explain the mechanisms behind the +86.9\% E2E speedup observed in \S\ref{sec:positive_e2e}.

\subsection{Accuracy Preservation}

Zero-padding guarantees \textbf{bit-exact output preservation}: $y'[0:d_{out}] = y$ exactly.
Unit tests confirm identical outputs (30/30 passed).
WikiText-2 perplexity validation on RAP SVD ($r$=0.8, $d$=102) confirms repair produces \textbf{identical perplexity}: baseline 11.08, RAP SVD 92.39, RAP SVD + repair 92.39 (higher PPL from compression, not repair).
All perplexity measurements use lm-eval-harness~\cite{lmeval} for reproducibility.

For version-specific notes, see \S\ref{sec:conclusion}.

\subsection{Scope and Limitations}
\label{sec:limitations}

\noindent\textbf{L1. Applicability:} The 96.9\% misalignment is from theoretical Fisher-information analysis; production PaLU checkpoints enforce 32-multiple alignment. Our findings apply to unconstrained SVD and future methods.

\noindent\textbf{L2. Evaluation Scope:} Perplexity validated (RAP SVD: 92.39 before/after repair); comprehensive downstream tasks (MMLU, reasoning) are future work.

\noindent\textbf{L3. Hardware:} All experiments on A100. H100 (4th-gen Tensor Cores, TMA, WGMMA) validation is future work.

\noindent\textbf{L4. Software:} Specific to FlashAttention 2.7.4. Future versions may handle alignment internally.

\FloatBarrier  % Contain evaluation figures/tables

%% ===========================================
%% 7. RELATED WORK
%% ===========================================
\section{Related Work}
\label{sec:related}

We position our work at the intersection of LLM compression and GPU microarchitecture, where compression-induced dimensional irregularities violate hardware alignment constraints.
Recent surveys~\cite{hw_accel_survey2025,llm_compression_survey2025,model_compression_survey2025} document this trend toward hardware-aware optimization.

\subsection{LLM Compression and Hardware Alignment}

\paragraph{Compression Methods.}
Post-training compression includes \textbf{pruning}~\cite{sparsegpt,maskllm2024}, \textbf{quantization} (GPTQ~\cite{gptq}, AWQ~\cite{awq}, LLM.int8()~\cite{llmint8_2022}, INT4~\cite{int4_quantization2023}), \textbf{SVD-based} (PaLU~\cite{palu}, SVD-LLM~\cite{svdllm2024}, Fisher-weighted~\cite{fwsvd2022,gfwsvd2025}), and \textbf{KV cache}~\cite{h2o,quest,pyramidkv}.
Quantization methods preserve dimensions via fixed-width groups (GPTQ uses 128-width groups; AWQ protects salient weights), avoiding alignment issues.
SVD methods can produce irregular ranks when optimizing purely for accuracy---production PaLU enforces 32-multiple alignment; SVD-LLM achieves 3.1$\times$ speedup via truncation-aware decomposition.

\paragraph{GPU Alignment Constraints.}
NVIDIA guides~\cite{nvidia_perf_guide,nvidia_dl_perf2024} specify multiples-of-8/16 alignment for FP16/INT8, as Tensor Cores require aligned tiles for peak efficiency.
CUTLASS~\cite{cutlass,cutlass_alignment2024} explicitly requires 128-bit vectorized accesses and dimension-aware tiles.
Memory coalescing studies~\cite{memory_coalescing2024} show irregular dimensions incur 60\% padding overhead.
Tensor Core constraints tightened across generations: Volta ($K \bmod 8$~\cite{volta_whitepaper}), Ampere ($K \bmod 16$~\cite{ampere_whitepaper,maskllm2024}), Hopper (TMA 128-byte granularity~\cite{nvidia_hopper_whitepaper,hopper_microbenchmark2024,tma_fp8_grouped_gemm2025}).

\subsection{Hardware-Aware Optimization}

Hardware-aware methods optimize measured latency, not just FLOPs.
AMC~\cite{amc2018} pioneered RL-based per-layer compression (1.95$\times$ mobile speedup); HALP~\cite{halp2021} formulated latency-budgeted pruning; HALOC~\cite{haloc2023} frames rank selection as architecture search with differentiable latency objectives; HAPE~\cite{hape2025} integrates on-device profiling into pruning importance.
\textbf{However, none explicitly model alignment constraints} (8/16/32-multiple dimensions).

FlashAttention~\cite{flashattention,flashattention2,flashattention3} provides optimized kernels for specific dimensions ($\{32, 64, 96, 128, 256\}$); others trigger 30--45\% slower paths (\S\ref{sec:cuda}).
Serving systems handle irregularities differently: vLLM~\cite{vllm} hardcodes supported dimensions; TensorRT~\cite{tensorrt} uses runtime padding; kernel frameworks (Triton~\cite{triton}, CUTLASS~\cite{cutlass_alignment2024}) expose alignment requirements directly.

\subsection{Our Contribution}

Production systems converged on alignment empirically (PaLU enforces 32-multiple; GPTQ/AWQ preserve dimensions via fixed groups).
\textbf{Our contribution}: First systematic diagnosis connecting compression-induced irregularities to GPU microarchitecture (Tensor Core tiles, vectorized loads~\cite{memory_coalescing2024}, bandwidth), quantifying \emph{how much} irregular dimensions cost (58\% TC, 50\% vectorized load, 40\% SDPA bandwidth) rather than just noting alignment matters.
Table~\ref{tab:dim_handling} compares dimension handling strategies.

\begin{table}[t]
\centering
\caption{Head dimension handling across systems.}
\label{tab:dim_handling}
\small
\begin{tabular}{@{}lll@{}}
\toprule
System & Supported \texttt{head\_dim} & Misaligned \\
\midrule
FlashAttn-2 & 32,64,96,128,256 (opt.) & +30--45\% \\
vLLM & 64,80,96,112,128,256 & Error \\
TensorRT & 32,40,64,80,96,104,128... & Runtime pad \\
\midrule
GPTQ/AWQ & Preserves original & N/A \\
PaLU & 32-multiple & N/A \\
RAP SVD & Any integer & \textbf{Affected} \\
\midrule
\textbf{This work} & Repair to 8/16-multiple & Compile-time \\
\bottomrule
\end{tabular}
\end{table}


%% ===========================================
%% 8. CONCLUSION
%% ===========================================
\section{Conclusion}
\label{sec:conclusion}

We presented a systematic measurement and diagnosis study of \emph{dimensional collapse}---a critical but overlooked problem where LLM compression produces irregular dimensions that degrade GPU performance despite reducing FLOPs.

\textbf{Diagnostic contribution}: We identified and quantified three primary root causes: FlashAttention internal slow paths (+30--45\%), Tensor Core misalignment (58\%), and vectorized load degradation (50\%)---while disconfirming L2 cache waste (5.8\%) as a significant factor.
This diagnosis provides actionable insights for compression method designers.

\textbf{Validated applicability framework}: Our Table~\ref{tab:applicability} correctly predicts when dimension repair helps.
Crucially, it also predicts when repair does \emph{not} help---validated by our RAP SVD experiments showing --0.8\% (no benefit) for projection-based architectures.
When applicable, repair achieves 22--28\% kernel-level speedup with 3.7--7.2\% memory overhead.

\paragraph{Hardware Scope and Future Work.}
Our experiments focus exclusively on NVIDIA A100 (Ampere architecture).
While Tensor Core alignment principles are fundamental across GPU generations, H100 (Hopper) introduces architectural changes (TMA, WGMMA, different SM counts) that may alter alignment sensitivity.
Validating our diagnostic framework and optimal alignment targets ($a=8$ vs. $a=16$) on H100 is important future work.

\paragraph{Software Version Note.}
All results are specific to FlashAttention 2.7.4. Future versions may implement internal alignment handling, potentially reducing the performance gaps we observe.
Our findings remain relevant as guidance for compression method designers and for users of current FlashAttention versions.

\paragraph{Integration with Compression Frameworks.}
Our dimension repair can be integrated as a post-compression pass in frameworks like PaLU, SVD-LLM~\cite{svdllm}, or custom SVD pipelines.
The key is ensuring the Shape Contract is satisfied before SDPA operations:

\emph{For direct compression methods}: Apply repair immediately after rank selection. Given compressed dimension $d_{orig}$, pad to $d_{pad} = \lceil d_{orig}/a \rceil \times a$ where $a=8$ (MINIMAL) or $a=16$ (OPTIMAL). Zero-pad weight matrices and biases; no accuracy loss occurs.

\emph{For projection-based methods}: No repair needed---SDPA operates on projected dimensions. Verify head\_dim alignment at the attention layer, not the latent space.

\emph{Integration checklist}: (1)~Determine architecture type using Table~\ref{tab:applicability}. (2)~If direct compression, apply padding to $W_{kv}$ outputs before attention. (3)~Verify alignment: \texttt{assert d\_out \% 8 == 0}. (4)~Export with aligned dimensions for inference.

\paragraph{Why Projection-Based Methods Don't Benefit.}
Our framework distinguishes two architectural patterns.
In \emph{direct compression} (e.g., vanilla SVD), the compressed head\_dim flows directly to SDPA---misalignment triggers slow paths, and repair recovers performance.
In \emph{projection-based} methods (e.g., RAP SVD), the latent space has irregular dimensions ($d$=102), but projection layers ($W_A$: hidden$\to$latent, $W_B$: latent$\to$head\_dim) restore aligned head\_dim=128 before SDPA.
The misalignment only affects the low-rank projection GEMMs, which have lower relative overhead than attention.
This architectural distinction explains our RAP SVD result (--0.8\%): repair targeted the wrong bottleneck.

\paragraph{Reproducibility.}
Code, experiment scripts, and raw data are available at \url{https://github.com/[ANONYMIZED]}.
Upon acceptance, we will release all benchmarking infrastructure, dimension repair implementations, and experimental configurations to facilitate reproduction and extension of our findings.

%% ===========================================
%% REFERENCES
%% ===========================================
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
