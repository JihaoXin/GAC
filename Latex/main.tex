%%
%% G-Compress: Dimensional Collapse in Compressed LLMs
%% Target: EuroMLSys (SIGPLAN format, 6 pages excluding references)
%%

\documentclass[sigplan,10pt,nonacm]{acmart}

%% Remove ACM-specific elements for submission
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

%% Packages
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{tikz}

%% Code listing style
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  numbers=left,
  numberstyle=\tiny,
}

%% Title
\title{When Smaller Is Slower: Dimensional Collapse in Compressed LLMs}

%% Authors
\author{Jihao Xin}
\affiliation{
  \institution{KAUST}
  \city{Thuwal}
  \country{Saudi Arabia}
}
\email{jihao.xin@kaust.edu.sa}

\author{Tian Lvy}
\affiliation{
  \institution{KAUST}
  \city{Thuwal}
  \country{Saudi Arabia}
}

\author{Qilong Pan}
\affiliation{
  \institution{HUMAIN AI}
  \city{Riyadh}
  \country{Saudi Arabia}
}

\author{Kesen Wang}
\affiliation{
  \institution{HUMAIN AI}
  \city{Riyadh}
  \country{Saudi Arabia}
}

\author{Marco Canini}
\affiliation{
  \institution{KAUST}
  \city{Thuwal}
  \country{Saudi Arabia}
}

\begin{abstract}
Post-training compression can produce irregular tensor dimensions (e.g., \texttt{head\_dim=107}) that cause GPU slowdowns despite reducing FLOPs---a phenomenon we term \emph{dimensional collapse}.
On NVIDIA A100, \texttt{head\_dim=107} increases SDPA latency by 88\% vs.\ aligned dimensions.
We identify three root causes: Tensor Core tile misalignment (58\% slowdown), vectorized load degradation (50\% loss), and SDPA bandwidth inefficiency (40\%).
We propose a \emph{dimension repair} pass achieving 22--28\% kernel-level speedup with 3.7--7.2\% memory overhead (3.5--5.9$\times$ ROI for architecture-dependent SDPA scenarios).
Our findings apply to compression methods where SDPA operates directly on misaligned dimensions.
\end{abstract}

\keywords{LLM Compression, GPU Optimization, Tensor Core, Memory Alignment}

\begin{document}

\maketitle

%% ===========================================
%% 1. INTRODUCTION
%% ===========================================
\section{Introduction}
\label{sec:intro}

Large Language Models (LLMs) have achieved remarkable capabilities, but their massive parameter counts pose deployment challenges.
Post-training compression techniques, including pruning and low-rank decomposition, offer promising solutions to reduce memory footprint and computational cost.
However, these techniques often produce models with \emph{irregular tensor dimensions}---values that do not align with hardware-preferred multiples (e.g., 8, 16, 32, 128).

We identify a counterintuitive phenomenon: \textbf{compressed models with fewer FLOPs can be slower than their uncompressed counterparts}.
We term this \emph{dimensional collapse}---a nonlinear performance degradation caused by misalignment between software-defined tensor shapes and hardware-fixed access patterns.

\paragraph{Scope and Applicability.}
Our analysis targets compression methods that \emph{do not} include alignment constraints---such as vanilla SVD, Fisher-information-based rank allocation, or future methods optimizing purely for accuracy.
We verified that all 24 publicly available PaLU checkpoints (ratio 0.5--0.9) use \emph{aligned} dimensions due to internal quantization that rounds ranks to 32-multiples.
Our kernel-level findings apply broadly to \emph{any} misaligned dimensions; end-to-end integration with production systems requires adapting to their factorized structures.

\paragraph{Motivating Example.}
Consider PaLU~\cite{palu}, a state-of-the-art low-rank compression method.
\emph{Theoretical} analysis of Llama-3-8B with Fisher-information-based rank allocation (0.8 retention ratio)---representing mathematically optimal ranks \emph{before} implementation constraints---shows the resulting \texttt{head\_dim} values would become irregular (e.g., 114--125 instead of 128).
In this \emph{unconstrained} scenario, 96.9\% of the theoretical dimensions are not 8-aligned.
Figure~\ref{fig:overview} illustrates this dimensional collapse phenomenon.
On an NVIDIA A100, this causes:
\begin{itemize}
  \item 88\% increase in SDPA latency
  \item FlashAttention internal slow path with 30--45\% overhead
  \item MEM\_EFFICIENT unavailable (strict 8-alignment)
  \item Bandwidth waste from cache misalignment
\end{itemize}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig1_overview.pdf}
\caption{\textbf{Dimensional collapse overview.} (a)~SVD compression produces irregular dimensions (e.g., $d$=107) causing \textbf{+88\% latency} due to GPU alignment violations. (b)~Dimension repair pads to aligned values, recovering \textbf{30\% performance} with only 4.7\% memory overhead.}
\label{fig:overview}
\end{figure}

\paragraph{Contributions.}
This paper makes the following contributions:
\begin{enumerate}
  \item \textbf{Quantification}: We measure the performance impact of irregular dimensions across GEMM and SDPA (\S\ref{sec:phenomenon}).
  \item \textbf{Root Cause Analysis}: We identify the causes across three layers: PyTorch backend selection, CUDA kernel paths, and hardware constraints (\S\ref{sec:causes}).
  \item \textbf{Shape Contract}: We formalize dimension alignment requirements as optimization constraints (\S\ref{sec:solution}).
  \item \textbf{Dimension Repair}: We propose a lightweight post-compression pass that restores alignment (\S\ref{sec:solution}).
  \item \textbf{Evaluation}: Kernel-level experiments demonstrate 22--28\% speedup with 3.7--7.2\% memory overhead---a 3.5--5.9$\times$ return on investment (speedup per unit memory cost) (\S\ref{sec:eval}).
\end{enumerate}

%% ===========================================
%% 2. BACKGROUND
%% ===========================================
\section{Background}
\label{sec:background}

\paragraph{Notation.}
We use $d$ to denote the attention head dimension (also written as \texttt{head\_dim} in code).
For matrix dimensions, $d_{in}$ and $d_{out}$ denote input and output dimensions of linear layers.
$B$, $S$, $H$ denote batch size, sequence length, and number of heads, respectively.

\subsection{Tensor Core Alignment}

NVIDIA Tensor Cores perform matrix-multiply-accumulate (MMA) operations on fixed tile sizes~\cite{nvidia_perf_guide}.
For FP16 on A100, the optimal tile requires $K \mod 16 = 0$.
Irregular dimensions force either padding (wasted compute) or fallback to scalar paths.
Tile/wave quantization effects can cause up to 1.5$\times$ overhead for misaligned dimensions.

\subsection{FlashAttention Constraints}

FlashAttention-2~\cite{flashattention2} (v2.7.4) is the de facto standard for efficient attention.
Contrary to common belief, it does \emph{not} strictly require 8-aligned dimensions---it remains available for all tested dimensions (104--128).
However, it uses internal slow paths for non-8-aligned dimensions, causing 30--45\% overhead.
Optimized kernels exist for $\{32, 64, 96, 128, 256\}$. MEM\_EFFICIENT strictly requires 8-alignment (dimensions like $d$=107 are unavailable).

\textbf{Version Note}: Results are specific to FlashAttention 2.7.4; future versions may implement internal alignment handling.
Different backends have varying constraints: PyTorch's SDPA~\cite{pytorch_sdpa} falls back to MATH backend (40$\times$ slower) when efficient backends cannot handle the input shape.

\subsection{Low-Rank Compression}

PaLU~\cite{palu} compresses attention by applying SVD to K/V projections:
$W_{kv} \approx U_r \Sigma_r V_r^T$ where $r < d$.
The compressed head dimension becomes $r$, which is typically not aligned.

%% ===========================================
%% 3. DIMENSIONAL COLLAPSE PHENOMENON
%% ===========================================
\section{Dimensional Collapse}
\label{sec:phenomenon}

\subsection{Experiment Setup}

We conduct experiments on NVIDIA A100-80GB with PyTorch 2.9.1, CUDA 12.8, and FlashAttention 2.7.4.
All benchmarks use FP16 with CUDA event timing (warmup=50, measure=200, trials=3). Driver: 560.35.03; cuDNN 9.1.0.
\emph{Note on variance}: GPU measurements exhibit 5--8\% run-to-run variance due to thermal throttling and memory state. We report results from independent experimental runs; tables show consistent trends despite minor variance.

\subsection{Scope and Dimension Distribution}
\label{sec:scope}

\textbf{Scope:} The 96.9\% misalignment comes from \emph{theoretical} Fisher-information-based ranks (Figure~\ref{fig:palu_dist}).
All 24 available PaLU checkpoints use 32-multiple alignment. Our findings apply to: (1) vanilla SVD, (2) future methods relaxing constraints, and (3) RAP SVD~\cite{rap}, which we validated produces 100\% misaligned dimensions ($d$=102 for $r$=0.8).


\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig3_palu_dist.pdf}
\caption{Dimension distribution from \emph{unconstrained} Fisher-information-based rank allocation (Llama-3-8B, $r$=0.8). If compression used mathematically optimal ranks without alignment constraints, 96.9\% of 512 KV head dimensions would be misaligned. See ``THEORETICAL ANALYSIS'' banner; production PaLU checkpoints enforce 32-multiple alignment.}
\label{fig:palu_dist}
\end{figure}

\subsection{SDPA Latency vs. Head Dimension}

We sweep \texttt{head\_dim} from 64 to 160 with shape $B=4, S=2048, H=32$.
Figure~\ref{fig:sdpa_latency} shows the results.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig2_sdpa_latency.pdf}
\caption{SDPA latency across head dimensions (Y-axis starts at 0.6ms to emphasize relative differences). Shaded regions show $\pm$1 std over 3 trials $\times$ 200 iterations. Clear alignment cliffs (``staircase effect'') visible at non-8-aligned values. $d$=107 shows 88\% increase vs $d$=96.}
\label{fig:sdpa_latency}
\end{figure}

8-aligned dimensions achieve 1.1--1.6ms while non-8-aligned incur 1.6--2.2ms. \texttt{head\_dim=107} shows 2.147ms (+88\% vs 96).

\subsection{Backend Selection Behavior}

Table~\ref{tab:backend} shows latency across different SDPA backends.
A key finding: \textbf{MEM\_EFFICIENT backend requires strict 8-alignment}---$d$=107 is unavailable (N/A), forcing fallback to FLASH or slower MATH.
This is a hard constraint, not a performance penalty.

\begin{table}[t]
\centering
\caption{SDPA backend latency (ms$\pm$std) for various head dimensions. Measurements: 200 iterations $\times$ 3 trials. Note: 5--8\% run-to-run variance expected (\S\ref{sec:phenomenon}).}
\label{tab:backend}
\begin{tabular}{lrrrr}
\toprule
$d$ & AUTO & FLASH & MEM\_EFF & MATH \\
\midrule
96  & 1.17{\scriptsize$\pm$.03} & 1.12{\scriptsize$\pm$.02} & 2.38{\scriptsize$\pm$.05} & 26.0{\scriptsize$\pm$.2} \\
104 & 1.54{\scriptsize$\pm$.04} & 1.54{\scriptsize$\pm$.04} & 2.75{\scriptsize$\pm$.06} & 26.5{\scriptsize$\pm$.2} \\
\textbf{107} & \textbf{2.14}{\scriptsize$\pm$.06} & \textbf{2.14}{\scriptsize$\pm$.06} & \multicolumn{1}{c}{N/A$^*$} & \textbf{27.0}{\scriptsize$\pm$.2} \\
112 & 1.53{\scriptsize$\pm$.04} & 1.53{\scriptsize$\pm$.04} & 2.60{\scriptsize$\pm$.05} & 27.1{\scriptsize$\pm$.2} \\
128 & 1.47{\scriptsize$\pm$.03} & 1.47{\scriptsize$\pm$.03} & 2.55{\scriptsize$\pm$.05} & 28.1{\scriptsize$\pm$.2} \\
\bottomrule
\multicolumn{5}{l}{\scriptsize $^*$MEM\_EFFICIENT unavailable: requires strict 8-alignment ($d$=107 is not 8-aligned).}
\end{tabular}
\end{table}

The MATH backend is 12.6$\times$ slower than FLASH for $d$=107.
If FlashAttention cannot handle a dimension, catastrophic fallback occurs.

%% ===========================================
%% 4. ROOT CAUSE ANALYSIS
%% ===========================================
\section{Root Cause Analysis}
\label{sec:causes}

We investigate the causes of dimensional collapse across three layers.

\subsection{PyTorch Backend Selection}
\label{sec:backend}

We tested backend availability for \texttt{head\_dim} $\in$ [104, 128].
Surprisingly, FlashAttention is available for \emph{all} dimensions (100\% for both 8-aligned and non-8-aligned), while MEM\_EFFICIENT requires strict 8-alignment.
FlashAttention does \emph{not} fall back to MATH; instead, it uses internal slow paths incurring 30--45\% overhead (8-aligned: 1.55ms avg, non-8-aligned: 2.03ms avg).
The root cause lies in the CUDA kernel layer, not backend selection.

\subsection{CUDA Kernel Layer}
\label{sec:cuda}

FlashAttention's internal 30--45\% slowdown stems from: (1) vectorized loads falling back to scalar (50\% loss when $d \mod 8 \neq 0$); (2) suboptimal GEMM tile selection reducing Tensor Core utilization (30\%$\to$12\%); (3) boundary predication causing warp divergence.
FlashAttention-2 dispatches optimized kernels for $d \in \{32, 64, 96, 128, 256\}$; other values use generic kernels ($d$=128: 1.47ms, $d$=125: 1.97ms, +34\%).\footnote{FlashAttention kernel dispatch: \texttt{csrc/flash\_attn/flash\_fwd\_hdim*.cu} in \url{https://github.com/Dao-AILab/flash-attention}. Head dimension determines which optimized kernel template is instantiated.}

\subsection{Hardware Constraints}
\label{sec:hardware}

We conduct controlled experiments (C23) to isolate hardware-level causes of dimensional collapse.
Figure~\ref{fig:root_cause} visualizes the impact of each hypothesis, and Table~\ref{tab:hardware} provides detailed metrics.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig4_root_cause.pdf}
\caption{Root cause breakdown. Tensor Core alignment (58\%), vectorized load degradation (50\%), and SDPA bandwidth (40\%) are the primary causes. L2 cache sector waste (5.8\%) is negligible.}
\label{fig:root_cause}
\end{figure}

\begin{table}[t]
\centering
\caption{Hardware layer root cause analysis (C23 experiment). Impact measured on A100 with FP16.}
\label{tab:hardware}
\small
\begin{tabular}{@{}llrl@{}}
\toprule
Hypothesis & Status & Impact & Root Cause \\
\midrule
H1: TC K\%16 & \textbf{Confirmed} & 58\% & Util. 30\%$\to$12\% \\
H2: L2 sector & Not confirmed & 5.8\% & Negligible \\
H3: SDPA BW & \textbf{Confirmed} & 40\% & Access pattern \\
H4: Vec. loads & \textbf{Confirmed} & 50\% & float4$\to$scalar \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{H1: Tensor Core Alignment (Confirmed).}
GEMM with K=16-aligned achieves 91 TFLOPS; non-aligned (K=107) drops to 37--40 TFLOPS (58\% slowdown, TC utilization 30\%$\to$12\%).

\paragraph{H2: L2 Cache Sectors (Not Confirmed).}
L2 sector waste ($\sim$5.8\%) cannot explain 30--58\% gaps; measured bandwidth is similar.

\paragraph{H3: SDPA Bandwidth Efficiency (Confirmed).}
$d$=112 achieves 153.6 GB/s; $d$=113 drops to 107.3 GB/s (--30\%). $d$=120 achieves 160.2 GB/s; $d$=121 drops to 118.5 GB/s (--26\%).

\paragraph{H4: Vectorized Loads (Confirmed).}
\texttt{float4} loads (K\%16) achieve 73--83 TFLOPS; scalar fallback (K=107) drops to 39--40 TFLOPS (50\% loss).

\textbf{Summary}: Tensor Core alignment (58\%), vectorized loads (50\%), and SDPA bandwidth (40\%) are primary causes; L2 cache (5.8\%) is negligible.

%% ===========================================
%% 5. SHAPE-AWARE COMPRESSION
%% ===========================================
\section{Shape-Aware Compression}
\label{sec:solution}

\subsection{Shape Contract}

We formalize alignment requirements: given an original dimension $d_{orig}$, we pad to $d_{pad} = \lceil d_{orig}/a \rceil \times a$ where $a$ is the alignment target.
The \textbf{MINIMAL} strategy uses $a=8$ (required for MEM\_EFFICIENT backend and vectorized loads), while \textbf{OPTIMAL} uses $a=16$ (maximizes Tensor Core utilization).
This minimizes memory overhead while guaranteeing hardware compatibility.

\subsection{Dimension Repair}

For a linear layer $y = Wx + b$ with $W \in \mathbb{R}^{d_{out} \times d_{in}}$, we pad the output dimension to the nearest aligned value $d'_{out} = \lceil d_{out}/a \rceil \times a$ by appending zero rows to $W$ and zeros to $b$.

\paragraph{Accuracy Preservation.}
Zero-padding preserves outputs exactly: $y' = [Wx + b; \mathbf{0}]$, where the original $y$ occupies positions $[0:d_{out}]$.
For attention, zero-valued dimensions contribute nothing to scores, making padding semantically neutral.
This ensures \textbf{bit-exact output preservation}---no retraining required.

%% ===========================================
%% 6. EVALUATION
%% ===========================================
\section{Evaluation}
\label{sec:eval}

We validate dimension repair at kernel level (SDPA/GEMM microbenchmarks), demonstrating 25--30\% recovery.
We also contextualize compression benefits with PaLU as an orthogonal study.

\subsection{Padding Rescue Experiment (P1)}

Table~\ref{tab:padding} shows the effect of padding $d$=107 to aligned values.

\begin{table}[t]
\centering
\caption{Padding rescue results for SDPA ($d$=107 logical). Measurements: 200 iterations $\times$ 3 trials. Data consistent with Table~\ref{tab:repair_perf}.}
\label{tab:padding}
\small
\begin{tabular}{lrrr}
\toprule
Phys. $d$ & Mem. Ovhd. & Latency (ms$\pm$std) & Speedup \\
\midrule
107 (base) & 0\% & 2.064{\scriptsize$\pm$.06} & 1.00$\times$ \\
112 & 4.7\% & 1.490{\scriptsize$\pm$.04} & 1.39$\times$ \\
128 & 19.6\% & 1.506{\scriptsize$\pm$.04} & 1.37$\times$ \\
\bottomrule
\end{tabular}
\end{table}

Padding to 112 achieves 27.8\% speedup with only 4.7\% memory overhead---an excellent tradeoff.

\subsection{GEMM Alignment Impact}

GEMM operations show similar patterns: K=107 achieves 0.089ms latency, while K=112 and K=128 both achieve 0.050ms---a \textbf{44\% improvement} from alignment.

\subsection{Dimension Repair Validation (C4)}

We validate our dimension repair implementation on PaLU-compressed dimensions.
Figure~\ref{fig:repair_tradeoff} visualizes the speedup vs. memory overhead tradeoff for different strategies.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig5_repair_tradeoff.pdf}
\caption{Speedup vs.\ memory overhead tradeoff for dimension repair. $d$=120 (already 8-aligned, highlighted) shows 0\% MINIMAL speedup, validating that alignment---not padding---drives performance gains. Average ROI: MINIMAL 5.9$\times$ (22\%/3.7\%), OPTIMAL 3.5$\times$ (25\%/7.2\%).}
\label{fig:repair_tradeoff}
\end{figure}

Table~\ref{tab:repair_perf} shows SDPA performance for repaired dimensions.
Memory overhead: MINIMAL 3.72\%, OPTIMAL 7.20\%.

\begin{table}[t]
\centering
\caption{SDPA latency (ms$\pm$std) before and after dimension repair ($B$=4, $S$=2048, $H$=32). Data from independent run vs.\ Tables~\ref{tab:backend}/\ref{tab:padding}; $\sim$6\% variance is within normal GPU measurement variability (\S\ref{sec:phenomenon}).}
\label{tab:repair_perf}
\begin{tabular}{lrrrrr}
\toprule
$d$ & Original & Minimal & Optimal & $\Delta$Min & $\Delta$Opt \\
\midrule
107 & 2.064{\scriptsize$\pm$.06} & 1.490{\scriptsize$\pm$.04} & 1.506{\scriptsize$\pm$.04} & \textbf{+27.8\%} & +27.0\% \\
114 & 2.049{\scriptsize$\pm$.06} & 1.549{\scriptsize$\pm$.04} & 1.432{\scriptsize$\pm$.04} & +24.4\% & \textbf{+30.1\%} \\
117 & 2.054{\scriptsize$\pm$.06} & 1.567{\scriptsize$\pm$.04} & 1.433{\scriptsize$\pm$.04} & +23.7\% & \textbf{+30.2\%} \\
120 & 1.557{\scriptsize$\pm$.04} & 1.557{\scriptsize$\pm$.04} & 1.428{\scriptsize$\pm$.04} & 0\% & +8.3\% \\
121 & 1.964{\scriptsize$\pm$.05} & 1.430{\scriptsize$\pm$.04} & 1.441{\scriptsize$\pm$.04} & \textbf{+27.2\%} & +26.6\% \\
125 & 1.975{\scriptsize$\pm$.05} & 1.439{\scriptsize$\pm$.04} & 1.439{\scriptsize$\pm$.04} & \textbf{+27.1\%} & +27.1\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Findings.}
MINIMAL achieves 23--28\% speedup with 3.72\% overhead (average ROI = 22\%/3.7\% = 5.9$\times$).
$d$=120 validates alignment: 8-aligned (0\% MINIMAL gain) but OPTIMAL pads to 128 for +8.3\%.

\subsection{Orthogonal Study: PaLU Compression Benefits}

\textbf{Context (not dimension repair)}: We compare Llama-3-8B baseline vs.\ PaLU (ratio=0.7) to contextualize compression benefits (Table~\ref{tab:e2e}).
PaLU achieves \textbf{11.5$\times$ decode speedup} via KV cache compression---this is from \emph{reduced memory footprint}, not dimension alignment.
Since PaLU enforces 32-multiple alignment internally, dimension repair does not apply here.
For methods \emph{without} alignment constraints (e.g., vanilla SVD), kernel-level experiments (C4) suggest additional 22--28\% improvement from repair.

\begin{table}[t]
\centering
\caption{\textbf{PaLU compression benefit (orthogonal to dimension repair)}. The 11.5$\times$ decode speedup comes from reduced KV cache size, not alignment---PaLU enforces 32-multiple alignment internally.}
\label{tab:e2e}
\small
\begin{tabular}{lrrr}
\toprule
Metric & Baseline & PaLU & $\Delta$ \\
\midrule
Prefill (tok/s) & 9870 & 9672 & --2.0\% \\
Decode (tok/s) & 119 & 1371 & +11.5$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Accuracy Preservation}

Zero-padding guarantees \textbf{bit-exact output preservation}: $y'[0:d_{out}] = y$ exactly.
Unit tests confirm identical outputs (30/30 passed).
WikiText-2 perplexity validation on RAP SVD ($r$=0.8, $d$=102, 100\% misaligned) confirms repair produces \textbf{identical perplexity}: baseline 11.08, RAP SVD 92.39, RAP SVD + repair 92.39 (higher PPL from compression, not repair).

\paragraph{E2E Performance Validation.}
We benchmarked RAP SVD inference before/after repair (Table~\ref{tab:rap_e2e}).
Prefill and decode show no speedup (--1.5\%, --0.7\%) because RAP SVD projects to aligned head\_dim=128 before SDPA (\S\ref{sec:limitations}).

\begin{table}[t]
\centering
\caption{RAP SVD E2E validation ($d$=102$\to$104). No speedup because SDPA operates on aligned head\_dim=128.}
\label{tab:rap_e2e}
\small
\begin{tabular}{lrrr}
\toprule
Phase & Misaligned & Repaired & $\Delta$ \\
\midrule
Prefill (ms) & 290.5 & 292.9 & --0.8\% \\
Decode (tok/s) & 1009 & 1000 & --0.9\% \\
Memory (MB) & 15451 & 15461 & +0.1\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Architectural Applicability Analysis}
\label{sec:applicability}

\textbf{Key Finding}: Dimension repair efficacy is \emph{architecture-dependent}.
We validated this through E2E inference benchmarks on RAP SVD~\cite{rap} compressed Llama-3-8B ($d$=102, 100\% misaligned).
The results (Table~\ref{tab:applicability}) provide important architectural guidance.

\begin{table}[t]
\centering
\caption{\textbf{Applicability guidance}: When does dimension repair help? The key factor is where SDPA operates---on compressed or projected dimensions.}
\label{tab:applicability}
\small
\begin{tabular}{@{}llcc@{}}
\toprule
Architecture Type & SDPA head\_dim & Repair Helps? & E2E Validated \\
\midrule
\textbf{Direct compression} & \emph{Misaligned} & \textbf{Yes} (+25--28\%) & Kernel-level \\
(vanilla SVD, head\_dim$\to d$) & (e.g., $d$=107) & & \\
\midrule
\textbf{Projection-based} & \emph{Aligned} & \textbf{No} (--0.8\%) & RAP SVD E2E \\
(RAP SVD, latent$\to$head\_dim) & (e.g., $d$=128) & & \\
\midrule
\textbf{Quantization} & \emph{Unchanged} & N/A & N/A \\
(GPTQ, AWQ) & & & \\
\bottomrule
\end{tabular}
\end{table}

RAP SVD stores compressed representations in latent space ($d$=102), but \emph{SDPA operates on projected head\_dim=128}---which is already aligned.
The misalignment only affects low-rank projection GEMMs ($W_A$: hidden$\to$latent, $W_B$: latent$\to$head\_dim), not attention.
This explains why kernel-level SDPA benchmarks show 25--30\% speedup while RAP SVD E2E shows none: the bottleneck differs by architecture.

\textbf{Practitioner Guidance}: Before applying dimension repair, verify whether SDPA operates directly on compressed dimensions.
If your compression uses a projection layer that restores aligned head\_dim before attention (like RAP SVD), repair provides no E2E benefit.

\subsection{Scope and Limitations}
\label{sec:limitations}

\textbf{(1) Scope}: The 96.9\% misalignment figure is from theoretical Fisher-information analysis; all 24 production PaLU checkpoints enforce 32-multiple alignment.
Our findings apply to methods without alignment constraints (vanilla SVD) or future methods that relax constraints for better compression.

\textbf{(2) Downstream}: Zero-padding preserves outputs exactly; perplexity validated on RAP SVD. Comprehensive task evaluation (MMLU, etc.) is future work.

\textbf{(3) Hardware}: All experiments are on A100. H100+ generalization remains future work.

%% ===========================================
%% 7. RELATED WORK
%% ===========================================
\section{Related Work}
\label{sec:related}

\paragraph{LLM Compression.}
Post-training compression spans multiple paradigms: pruning (SparseGPT~\cite{sparsegpt}), quantization (GPTQ~\cite{gptq}, AWQ~\cite{awq}, QLoRA~\cite{qlora}), low-rank adaptation (LoRA~\cite{lora}), and SVD-based decomposition (PaLU~\cite{palu}, SVD-LLM~\cite{svdllm}, CALDERA~\cite{caldera}).
These methods optimize for accuracy-compression trade-offs but largely ignore hardware alignment.
\emph{Which methods produce misaligned dimensions?} SVD-based approaches (PaLU, vanilla SVD, SVD-LLM) can theoretically produce irregular dimensions. However, production checkpoints often enforce alignment internally. GPTQ and AWQ operate on fixed-width groups (typically 128) and do not alter tensor dimensions. Unstructured pruning (SparseGPT) preserves dimensions but creates irregular sparsity.
Our work targets compression methods that \emph{do not} include alignment constraints, filling a gap in the literature.

\paragraph{KV Cache \& Attention Optimization.}
MQA~\cite{mqa}, GQA~\cite{gqa}, and StreamingLLM~\cite{streaminglm} reduce KV cache while preserving standard dimensions.
FlashAttention~\cite{flashattention,flashattention2} is tuned for $\{32, 64, 96, 128, 256\}$, with cliffs for other values.
SVD-based compression produces irregular dimensions that violate these implicit constraints.

\paragraph{Inference Frameworks.}
TensorRT~\cite{tensorrt}, vLLM~\cite{vllm}, TGI~\cite{tgi}, and FlashInfer~\cite{flashinfer} apply runtime optimizations but typically assume aligned dimensions.
vLLM's FlashAttention backend only supports head sizes in $\{64, 80, 96, 112, 128, 256\}$; unsupported dimensions trigger fallback to slower backends~\cite{pytorch_sdpa}.
TensorRT may perform implicit runtime padding, but this is opaque and incurs per-inference overhead.
Our compile-time approach differs: (1) padding is applied once at model export, not per-inference; (2) alignment is explicit and controllable; (3) frameworks can select optimal kernels knowing true dimensions.
FlashDecoding++~\cite{flashdecoding} shows that different GEMM shapes require different dataflows, with up to 50\% performance variance---further evidence that dimension-aware optimization matters.

\paragraph{Dimension Handling Comparison.}
Table~\ref{tab:dim_handling} summarizes how different systems handle head dimensions.
Our compile-time repair makes alignment explicit and avoids runtime overhead.

\begin{table}[t]
\centering
\caption{Head dimension handling across systems. Supported values shown; others may cause fallback or failure.}
\label{tab:dim_handling}
\small
\begin{tabular}{@{}lll@{}}
\toprule
System & Supported \texttt{head\_dim} & Misaligned handling \\
\midrule
FlashAttn-2 & Optimized: 32,64,96,128,256 & Slow path (+30--45\%) \\
vLLM & 64,80,96,112,128,256 & Error/fallback \\
TensorRT & 32,40,64,80,96,104,128... & Runtime padding \\
\midrule
GPTQ/AWQ & Preserves original dims & N/A (no change) \\
PaLU & 32-multiple (enforced) & N/A (aligned) \\
RAP SVD & Any integer & \textbf{Vulnerable} \\
\midrule
\textbf{This work} & Repair to 8/16-multiple & Compile-time fix \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Positioning.}
Unlike prior accuracy-compression trade-off studies, \textbf{we focus on performance-alignment trade-offs}---compressed models with fewer FLOPs can run slower due to hardware misalignment.

%% ===========================================
%% 8. CONCLUSION
%% ===========================================
\section{Conclusion}
\label{sec:conclusion}

We identified \emph{dimensional collapse}---a critical but overlooked problem where LLM compression produces irregular dimensions that degrade GPU performance despite reducing FLOPs.

\textbf{Key findings}: FlashAttention slow paths (+30--45\%), Tensor Core misalignment (58\%), and vectorized load degradation (50\%) are the primary causes; L2 cache waste (5.8\%) is negligible.
Our dimension repair achieves 22--28\% kernel-level speedup with 3.7--7.2\% memory overhead when SDPA operates directly on misaligned dimensions.

\textbf{Architectural guidance}: Dimension repair is architecture-dependent (Table~\ref{tab:applicability}).
It helps when SDPA directly operates on compressed dimensions, but \emph{not} when compression uses projection layers that restore aligned head\_dim (like RAP SVD).
Practitioners should verify their compression architecture before applying repair.

\textbf{H100 Implications}: While our experiments focus on A100, we expect similar dimensional collapse on H100/H200.
FlashAttention-3 targets Hopper GPUs with optimized kernels for $d \in \{64, 128, 256\}$; misaligned dimensions will likely face similar slow-path penalties.
H100's 4th-generation Tensor Cores use similar tile sizes (m16n8k16 for FP16), making alignment requirements comparable.
Quantitative H100 validation is future work.

\textbf{Integration with compression frameworks}: Our dimension repair can be integrated as a post-compression pass in frameworks like PaLU, SVD-LLM~\cite{svdllm}, or custom SVD pipelines.
The key is ensuring the Shape Contract ($d_{out} \mod 8 = 0$ for MINIMAL, $d_{out} \mod 16 = 0$ for OPTIMAL) is satisfied before SDPA operations.

%% ===========================================
%% REFERENCES
%% ===========================================
\clearpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
