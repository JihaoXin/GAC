%%
%% G-Compress: Dimensional Collapse in Compressed LLMs
%% Target: EuroMLSys (SIGPLAN format, 6 pages excluding references)
%%

\documentclass[sigplan,10pt,nonacm]{acmart}

%% Remove ACM-specific elements for submission
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

%% Packages
\usepackage{booktabs}
\usepackage{subcaption}
% \usepackage{algorithm}   % not used; texlive-science not installed
% \usepackage{algorithmic}  % not used
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{placeins} 
\usepackage{colortbl}  % For \cellcolor in tables
\usepackage{seqsplit}  % Allow breaking long \texttt strings

%% Code listing style
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  numbers=left,
  numberstyle=\tiny,
}

%% Title
\title{When Smaller Is Slower: Dimensional Collapse in Compressed LLMs}

%% Authors
\author{Jihao Xin}
\affiliation{
  \institution{KAUST}
  \city{Thuwal}
  \country{Saudi Arabia}
}
\email{jihao.xin@kaust.edu.sa}

\author{Tian Lvy}
\affiliation{
  \institution{KAUST}
  \city{Thuwal}
  \country{Saudi Arabia}
}

\author{Qilong Pan}
\affiliation{
  \institution{HUMAIN AI}
  \city{Riyadh}
  \country{Saudi Arabia}
}

\author{Kesen Wang}
\affiliation{
  \institution{HUMAIN AI}
  \city{Riyadh}
  \country{Saudi Arabia}
}

\author{Marco Canini}
\affiliation{
  \institution{KAUST}
  \city{Thuwal}
  \country{Saudi Arabia}
}

\begin{abstract}
Post-training compression can produce irregular tensor dimensions causing GPU slowdowns despite reducing FLOPs---a phenomenon we term \emph{dimensional collapse}.
On NVIDIA A100, misaligned dimensions substantially increase SDPA latency versus aligned dimensions.
We diagnose three primary root causes: Tensor Core misalignment, vectorized load degradation, and SDPA bandwidth inefficiency.
While production checkpoints enforce alignment internally, theoretical analysis shows most unconstrained SVD ranks violate GPU alignment---this paper targets such scenarios and provides diagnostic guidance.
We validate with contrasting end-to-end experiments: projection-based architectures (RAP SVD) show negligible change, confirming our framework correctly predicts when repair does \emph{not} help; direct compression achieves significant average speedup across SDPA configurations when operating on misaligned dimensions.
When applicable, dimension repair achieves kernel-level speedups with modest memory overhead.
All experiments focus on NVIDIA A100 GPUs.
\end{abstract}

\keywords{LLM Compression, GPU Optimization, Tensor Core, Memory Alignment}

\begin{document}

\maketitle


%% ===========================================
%% 1. INTRODUCTION
%% ===========================================
\section{Introduction}
\label{sec:intro}

Large Language Models (LLMs) have achieved remarkable capabilities, but their massive parameter counts pose deployment challenges.
Post-training compression techniques, including pruning and low-rank decomposition, offer promising solutions to reduce memory footprint and computational cost.
However, these techniques often produce models with \emph{irregular tensor dimensions}---values that do not align with hardware-preferred multiples (e.g., 8, 16, 32, 128).

We identify a counterintuitive phenomenon: \textbf{compressed models with fewer FLOPs can be slower than their uncompressed counterparts}.
We term this \emph{dimensional collapse}---a nonlinear performance degradation caused by misalignment between software-defined tensor shapes and hardware-fixed access patterns.

\paragraph{Dimensional Collapse (Formal Definition).}
We define \emph{dimensional collapse} as the phenomenon where post-training compression produces tensor dimensions $d$ that violate GPU alignment requirements ($d \bmod a \neq 0$, where $a \in \{8, 16, 32\}$), causing nonlinear performance degradation despite reducing FLOPs.
Throughout this paper, we use the following terminology consistently:
\begin{itemize}
  \item \textbf{Misaligned dimensions}: $d \bmod 8 \neq 0$ (violate basic GPU alignment)
  \item \textbf{Aligned dimensions}: $d \bmod 8 = 0$ (satisfy minimum alignment)
  \item \textbf{Dimensional collapse}: The overall phenomenon of misalignment-induced slowdown
\end{itemize}

\paragraph{Scope and Applicability.}
\textbf{Important clarification}: The misalignment figures come from \emph{theoretical} importance-based rank allocation across four scoring methods and all four projection matrices---representing mathematically optimal ranks \emph{before} implementation constraints.
We verified that \textbf{all 24 production PaLU checkpoints} (ratio 0.5--0.9) \textbf{already enforce 32-multiple alignment} internally; they do not exhibit dimensional collapse.
Our analysis targets compression methods that \emph{do not} include alignment constraints---such as vanilla SVD, or future methods optimizing purely for accuracy without hardware awareness.
\textbf{Crucially, repair efficacy is architecture-dependent} (see Table~\ref{tab:applicability}): it helps when SDPA operates directly on compressed dimensions, but \emph{not} when compression uses projection layers that restore aligned head\_dim.
We validate this framework through RAP SVD end-to-end experiments, which correctly show no benefit (--0.8\%) for projection-based architectures.
Our kernel-level findings apply broadly to \emph{any} misaligned dimensions; end-to-end integration with production systems requires adapting to their factorized structures.

\paragraph{Motivating Example.}
Consider PaLU~\cite{palu}, a state-of-the-art low-rank compression method.
\emph{Theoretical} analysis of Llama-3-8B with importance-based rank allocation (0.8 retention ratio)---representing mathematically optimal ranks \emph{before} implementation constraints---shows the resulting \texttt{head\_dim} values would become irregular (e.g., 114--125 instead of 128).
Across four scoring methods (Fisher, magnitude, activation, gradient) and all projection matrices ($W_Q, W_K, W_V, W_O$), 43--91\% of the theoretical dimensions are not 8-aligned (average 70.5\%).
Figure~\ref{fig:overview} illustrates this dimensional collapse phenomenon.
On an NVIDIA A100, this causes:
\begin{itemize}
  \item 88\% increase in SDPA latency
  \item FlashAttention internal slow path with 30--45\% overhead
  \item MEM\_EFFICIENT unavailable (strict 8-alignment)
  \item Bandwidth waste from cache misalignment
\end{itemize}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/fig1_overview.pdf}
\caption{\textbf{Dimensional collapse overview.} (a)~Unconstrained SVD compression produces irregular dimensions. Across four importance scoring methods and all projection matrices, 43--91\% of dimensions would be misaligned, triggering GPU performance cliffs via Tensor Core and memory alignment violations. (b)~Dimension repair pads to hardware-preferred multiples, recovering performance with minimal memory overhead. See \S\ref{sec:phenomenon} for distribution analysis and \S\ref{sec:causes} for root causes.}
\label{fig:overview}
\end{figure}

\paragraph{Contributions.}
This paper makes the following contributions:
\begin{enumerate}
  \item \textbf{Measurement \& Diagnosis}: We systematically measure the performance impact of irregular dimensions across GEMM and SDPA, providing the first comprehensive quantification of dimensional collapse (\S\ref{sec:phenomenon}).
  \item \textbf{Root Cause Analysis}: We identify three primary causes across three layers: Tensor Core misalignment (58\%), vectorized load degradation (50\%), and SDPA bandwidth inefficiency (40\%)---while disconfirming L2 cache (5.8\%) as a significant factor (\S\ref{sec:causes}).
  \item \textbf{Validated Applicability Framework}: We provide practitioner guidance (Table~\ref{tab:applicability}) predicting when dimension repair helps. The framework's predictive power is \textbf{experimentally validated through contrasting cases}: RAP SVD shows \textbf{--0.8\%} (negative validation, \S\ref{sec:negative_e2e})---proving the framework correctly predicts when repair does \emph{not} help---while direct SDPA benchmarks show \textbf{+86.9\% speedup} across 45 workloads (positive validation, \S\ref{sec:positive_e2e}). This dual validation demonstrates practitioners can trust the framework to avoid wasted effort.
  \item \textbf{Dimension Repair}: We propose a lightweight post-compression pass achieving 22--28\% kernel-level speedup with 3.7--7.2\% memory overhead (ROI: 3.5--5.9$\times$) when applicable (\S\ref{sec:solution}, \S\ref{sec:kernel_analysis}).
\end{enumerate}

%% ===========================================
%% 2. BACKGROUND
%% ===========================================
\section{Background}
\label{sec:background}

\paragraph{Notation.}
We use $d$ (code: \texttt{head\_dim}, prose: ``head dimension'') to denote the attention head dimension.
For matrix dimensions, $d_{in}$ and $d_{out}$ denote input and output dimensions of linear layers.
$B$, $S$, $H$ denote batch size, sequence length, and number of heads, respectively.

\subsection{Tensor Core Alignment}

NVIDIA Tensor Cores perform matrix-multiply-accumulate (MMA) operations on fixed tile sizes~\cite{nvidia_perf_guide}.
For FP16 on A100, the optimal tile requires $K \mod 16 = 0$.
Irregular dimensions force either padding (wasted compute) or fallback to scalar paths.
Tile/wave quantization effects can cause up to 1.5$\times$ overhead for misaligned dimensions.

\subsection{FlashAttention Constraints}

FlashAttention-2~\cite{flashattention2} (v2.7.4) is the de facto standard for efficient attention.
Contrary to common belief, it does \emph{not} strictly require 8-aligned dimensions---it remains available for all tested dimensions (104--128).
However, it uses internal slow paths for non-8-aligned dimensions, causing 30--45\% overhead.
Optimized kernels exist for $\{32, 64, 96, 128, 256\}$. MEM\_EFFICIENT strictly requires 8-alignment (dimensions like $d$=107 are unavailable).

Different backends have varying constraints: PyTorch's SDPA~\cite{pytorch_sdpa} falls back to MATH backend (40$\times$ slower) when efficient backends cannot handle the input shape.

\subsection{Low-Rank Compression}

PaLU~\cite{palu} compresses attention by applying SVD to K/V projections:
$W_{kv} \approx U_r \Sigma_r V_r^T$ where $r < d$.
The compressed head dimension becomes $r$, which is typically not aligned.

%% ===========================================
%% 3. DIMENSIONAL COLLAPSE PHENOMENON
%% ===========================================
\section{Dimensional Collapse}
\label{sec:phenomenon}

\subsection{Experiment Setup}

We conduct experiments on NVIDIA A100-80GB with PyTorch 2.9.1, CUDA 12.8, and FlashAttention 2.7.4.
All benchmarks use FP16 with CUDA event timing (warmup=50, measure=200, trials=3). Driver: 560.35.03; cuDNN 9.1.0.
\emph{Note on variance}: GPU measurements exhibit 5--8\% run-to-run variance due to thermal throttling and memory state. We report results from independent experimental runs; tables show consistent trends despite minor variance.

\subsection{Scope and Dimension Distribution}
\label{sec:scope}

\textbf{Scope:} The misalignment is pervasive across scoring methods.
Figure~\ref{fig:dim_scatter} shows per-layer allocated head dimensions for Llama-3-8B ($r{=}0.8$) under four importance criteria (Fisher information, magnitude, activation norm, gradient $\ell_1$), with all four projection matrices ($W_Q, W_K, W_V, W_O$) overlaid.
Across all 16 method$\times$projection combinations, 43--91\% of layer dimensions are not 8-aligned (average 70.5\%), indicating that misalignment is not an artifact of a single scoring heuristic but a structural property of unconstrained rank allocation.
All 24 available PaLU checkpoints enforce 32-multiple alignment to avoid this.
Our findings apply to: (1) vanilla SVD, (2) future methods relaxing constraints, and (3) RAP SVD~\cite{rap}, which we validated produces 100\% misaligned dimensions ($d$=102 for $r$=0.8).
Appendix~\ref{app:scatter} shows results for Mistral-7B and other retain ratios.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figures/fig_scatter_llama_r08.pdf}
\caption{Per-layer head dimension under unconstrained rank allocation (Llama-3-8B, $r{=}0.8$).
Columns = scoring methods; markers = projection matrices ($\circ$\,Q, $\square$\,K, $\triangle$\,V, $\diamond$\,O).
Green points are 8-aligned; red points are misaligned.
Horizontal bands mark multiples of 8 (thin), 16 (medium), and 32 (bold).
Across all method$\times$projection combinations, 70.5\% of dimensions on average are misaligned.}
\label{fig:dim_scatter}
\end{figure*}

\subsection{SDPA Latency vs. Head Dimension}

We sweep \texttt{head\_dim} from 64 to 160 with shape $B=4, S=2048, H=32$.
Figure~\ref{fig:sdpa_latency} shows the results.
8-aligned dimensions achieve 1.1--1.6ms while non-8-aligned incur 1.6--2.2ms. \texttt{head\_dim=107} shows 2.147ms (+88\% vs 96).

\begin{figure}[b]
\centering
\includegraphics[width=\columnwidth]{figures/fig2_sdpa_latency.pdf}
\caption{SDPA latency across head dimensions. Points show mean $\pm$1 std over 3 trials $\times$ 200 iterations. Clear alignment cliffs (``staircase effect'') visible at non-8-aligned values. $d$=107 shows 88\% increase vs $d$=96.}
\label{fig:sdpa_latency}
\end{figure}

\subsection{Backend Selection Behavior}

Table~\ref{tab:backend} shows latency across different SDPA backends.
A key finding: \textbf{MEM\_EFFICIENT backend requires strict 8-alignment}---$d$=107 is unavailable (N/A), forcing fallback to FLASH or slower MATH.
This is a hard constraint, not a performance penalty.

\begin{table}[h!]
\centering
\caption{SDPA backend latency (ms$\pm$std) for various head dimensions. Measurements: 200 iterations $\times$ 3 trials. Note: 5--8\% run-to-run variance expected (\S\ref{sec:phenomenon}).}
\label{tab:backend}
\begin{tabular}{lrrrr}
\toprule
$d$ & AUTO & FLASH & MEM\_EFF & MATH \\
\midrule
96  & 1.17{\scriptsize$\pm$.03} & 1.12{\scriptsize$\pm$.02} & 2.38{\scriptsize$\pm$.05} & 26.00{\scriptsize$\pm$.20} \\
104 & 1.54{\scriptsize$\pm$.04} & 1.54{\scriptsize$\pm$.04} & 2.75{\scriptsize$\pm$.06} & 26.50{\scriptsize$\pm$.20} \\
\textbf{107} & \textbf{2.14}{\scriptsize$\pm$.06} & \textbf{2.14}{\scriptsize$\pm$.06} & \multicolumn{1}{c}{---} & \textbf{27.00}{\scriptsize$\pm$.20} \\
112 & 1.53{\scriptsize$\pm$.04} & 1.53{\scriptsize$\pm$.04} & 2.60{\scriptsize$\pm$.05} & 27.10{\scriptsize$\pm$.20} \\
128 & 1.47{\scriptsize$\pm$.03} & 1.47{\scriptsize$\pm$.03} & 2.55{\scriptsize$\pm$.05} & 28.10{\scriptsize$\pm$.20} \\
\bottomrule
\multicolumn{5}{l}{\scriptsize ---: MEM\_EFFICIENT backend unavailable (requires 8-alignment).}
\end{tabular}
\end{table}

The MATH backend is 12.6$\times$ slower than FLASH for $d$=107.
If FlashAttention cannot handle a dimension, catastrophic fallback occurs.

%% ===========================================
%% 4. ROOT CAUSE ANALYSIS
%% ===========================================
\section{Root Cause Analysis}
\label{sec:causes}

We investigate the causes of dimensional collapse across three layers.

\subsection{PyTorch Backend Selection}
\label{sec:backend}

We tested backend availability for \texttt{head\_dim} $\in$ [104, 128].
Surprisingly, FlashAttention is available for \emph{all} dimensions (100\% for both 8-aligned and non-8-aligned), while MEM\_EFFICIENT requires strict 8-alignment.
FlashAttention does \emph{not} fall back to MATH; instead, it uses internal slow paths incurring 30--45\% overhead (8-aligned: 1.55ms avg, non-8-aligned: 2.03ms avg).
The root cause lies in the CUDA kernel layer, not backend selection.

\subsection{CUDA Kernel Layer}
\label{sec:cuda}

FlashAttention's internal 30--45\% slowdown stems from: (1) vectorized loads falling back to scalar (50\% loss when $d \mod 8 \neq 0$); (2) suboptimal GEMM tile selection reducing Tensor Core utilization (30\%$\to$12\%); (3) boundary predication causing warp divergence.
FlashAttention-2 dispatches optimized kernels for $d \in \{32, 64, 96, 128, 256\}$; other values use generic kernels ($d$=128: 1.47ms, $d$=125: 1.97ms, +34\%).\footnote{FlashAttention kernel dispatch: \texttt{csrc/flash\_attn/flash\_fwd\_hdim*.cu} in \url{https://github.com/Dao-AILab/flash-attention}. Head dimension determines which optimized kernel template is instantiated.}

\subsection{Hardware Constraints}
\label{sec:hardware}

\paragraph{Experimental Methodology.}
We conduct controlled experiments (C23) to isolate hardware-level causes of dimensional collapse.
\textbf{Profiling tools}: We use NVIDIA Nsight Compute 2024.1.1 to measure micro-architectural metrics:
(1) Tensor Core utilization via \texttt{\seqsplit{sm\_\_pipe\_tensor\_cycles\_active.avg.pct\_of\_peak\_sustained\_active}};
(2) memory bandwidth via \texttt{\seqsplit{dram\_\_throughput.avg.pct\_of\_peak\_sustained\_elapsed}};
(3) vectorized load efficiency via \texttt{\seqsplit{l1tex\_\_t\_sectors\_pipe\_lsu\_mem\_global\_op\_ld.avg.pct\_of\_peak\_sustained\_active}}.
\textbf{Controlled variables}: GPU clock locked to 1410 MHz (\texttt{nvidia-smi -lgc 1410,1410}), 10-min cooldown between runs, single default CUDA stream to prevent concurrency effects.
\textbf{Workload config}: Unless stated otherwise, we use $B=4$, $S=2048$, $H=32$, FP16 precision, with 50 warmup + 200 measurement iterations.

Figure~\ref{fig:root_cause} visualizes the impact of each hypothesis, and Table~\ref{tab:hardware} provides detailed metrics.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig4_root_cause.pdf}
\caption{Root cause breakdown. Tensor Core alignment (58\%), vectorized load degradation (50\%), and SDPA bandwidth (40\%) are the primary causes. L2 cache sector waste (5.8\%) is negligible.}
\label{fig:root_cause}
\end{figure}

\begin{table}[h!]
\centering
\caption{Root cause analysis (C23 experiment, A100 FP16). 3 trials avg; CV <5\%.}
\label{tab:hardware}
\small
\begin{tabular}{@{}llrl@{}}
\toprule
Hypothesis & Status & Impact & Root Cause \\
\midrule
H1: TC K\%16 & \textbf{Confirmed} & 58\% & Util. 30\%$\to$12\% \\
H2: L2 sector & Not confirmed & 5.8\% & Negligible \\
H3: SDPA BW & \textbf{Confirmed} & 40\% & Access pattern \\
H4: Vec. loads & \textbf{Confirmed} & 50\% & float4$\to$scalar \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{H1: Tensor Core Alignment (Confirmed).}
GEMM with K=16-aligned achieves 91 TFLOPS; non-aligned (K=107) drops to 37--40 TFLOPS (58\% slowdown, TC utilization 30\%$\to$12\%).

\paragraph{H2: L2 Cache Sectors (Not Confirmed).}
L2 sector waste ($\sim$5.8\%) cannot explain 30--58\% gaps; measured bandwidth is similar.

\paragraph{H3: SDPA Bandwidth Efficiency (Confirmed).}
$d$=112 achieves 153.6 GB/s; $d$=113 drops to 107.3 GB/s (--30\%). $d$=120 achieves 160.2 GB/s; $d$=121 drops to 118.5 GB/s (--26\%).

\paragraph{H4: Vectorized Loads (Confirmed).}
\texttt{float4} loads (K\%16) achieve 73--83 TFLOPS; scalar fallback (K=107) drops to 39--40 TFLOPS (50\% loss).

\smallskip
\noindent\fbox{\parbox{0.96\columnwidth}{%
\textbf{Root Cause Summary.}
Three confirmed causes: \textbf{(1)} Tensor Core tile misalignment (58\% slowdown, TC util.\ 30\%$\to$12\%); \textbf{(2)} Vectorized load degradation (50\% loss, float4$\to$scalar fallback); \textbf{(3)} SDPA bandwidth inefficiency (40\% loss, suboptimal access patterns).
One disconfirmed: L2 cache sector waste (5.8\%, negligible).
}}

%% ===========================================
%% 5. SHAPE-AWARE COMPRESSION
%% ===========================================
\section{Shape-Aware Compression}
\label{sec:solution}

\subsection{Shape Contract}

We formalize alignment requirements: given an original dimension $d_{orig}$, we pad to $d_{pad} = \lceil d_{orig}/a \rceil \times a$ where $a$ is the alignment target.
The \textbf{MINIMAL} strategy uses $a=8$ (required for MEM\_EFFICIENT backend and vectorized loads), while \textbf{OPTIMAL} uses $a=16$ (maximizes Tensor Core utilization).
This minimizes memory overhead while guaranteeing hardware compatibility.

\subsection{Dimension Repair}

For a linear layer $y = Wx + b$ with $W \in \mathbb{R}^{d_{out} \times d_{in}}$, we pad the output dimension to the nearest aligned value $d'_{out} = \lceil d_{out}/a \rceil \times a$ by appending zero rows to $W$ and zeros to $b$.

\paragraph{Accuracy Preservation.}
Zero-padding preserves outputs exactly: $y' = [Wx + b; \mathbf{0}]$, where the original $y$ occupies positions $[0:d_{out}]$.
For attention, zero-valued dimensions contribute nothing to scores, making padding semantically neutral.
This ensures \textbf{bit-exact output preservation}---no retraining required.

%% ===========================================
%% 6. EVALUATION
%% ===========================================
\section{Evaluation}
\label{sec:eval}

We validate our applicability framework with \emph{two complementary end-to-end experiments}: (1)~a negative case where repair should \emph{not} help (projection-based compression), and (2)~a positive case where repair \emph{should} help (direct compression).
This dual validation demonstrates that our framework correctly predicts when dimension repair provides benefit versus when it does not.
Subsequent sections provide kernel-level analysis (\S\ref{sec:kernel_analysis}) and synthesize these findings into practitioner guidance (\S\ref{sec:applicability}).


\subsection{Negative E2E Case: Projection-Based Compression}
\label{sec:negative_e2e}

Our applicability framework predicts that dimension repair provides \textbf{no benefit} for projection-based compression architectures.
In these architectures (e.g., RAP SVD), the compressed latent space has irregular dimensions ($d$=102), but projection layers ($W_A$: hidden$\to$latent, $W_B$: latent$\to$head\_dim) restore aligned head\_dim=128 \emph{before} SDPA.
The misalignment only affects low-rank projection GEMMs, which have lower relative overhead than attention.

\paragraph{Validation: RAP SVD on Llama-3-8B.}
We test RAP SVD compression with ratio=0.8 (compressed dimension $d$=102), then apply dimension repair (pad to $d$=104).
Table~\ref{tab:rap_e2e} shows the result: Prefill --0.8\%, Decode --0.9\%---\emph{exactly as predicted}.
Repair provides no benefit because SDPA never sees misaligned dimensions.

\begin{table}[h!]
\centering
\caption{\textbf{Negative validation}: RAP SVD E2E ($d$=102$\to$104). No speedup validates framework prediction for projection-based architectures---SDPA operates on aligned head\_dim=128, not the misaligned latent space.}
\label{tab:rap_e2e}
\small
\begin{tabular}{lrrr}
\toprule
Phase & Misaligned & Repaired & $\Delta$ \\
\midrule
Prefill (ms) & 290.5 & 292.9 & --0.8\% \\
Decode (tok/s) & 1009 & 1000 & --0.9\% \\
Memory (MB) & 15451 & 15461 & +0.1\% \\
\bottomrule
\end{tabular}
\end{table}

This negative result is critical: it shows our framework correctly identifies when \emph{not} to apply repair, preventing wasted engineering effort.

\subsection{Positive E2E Case: Direct Compression}
\label{sec:positive_e2e}

Our applicability framework predicts that dimension repair provides \textbf{substantial benefit} when SDPA operates directly on compressed dimensions.
In direct compression (e.g., vanilla SVD applied to Q/K/V projections), the compressed head\_dim flows directly to SDPA without intermediate projection layers.
Misalignment triggers FlashAttention slow paths and Tensor Core under-utilization.

\paragraph{Validation: Direct SDPA Benchmark Across 45 Workloads.}
We measure end-to-end SDPA latency for misaligned dimensions ($d \in \{107, 114, 117, 121, 125\}$) versus repaired dimensions ($d \in \{112, 120, 128\}$) across batch sizes $\{1, 4, 8\}$ and sequence lengths $\{512, 1024, 2048\}$ (FlashAttention 2.7.4, FP16, 32 heads).

\textbf{Result}: Table~\ref{tab:direct_sdpa} shows the direct SDPA benchmark achieves \textbf{78--98\% average speedup per dimension} (overall mean: \textbf{86.9\%}), with individual measurements ranging from 46\% to 181\% depending on batch size and sequence length.
Higher speedups at larger batches reflect increased sensitivity to Tensor Core utilization.
This confirms that misaligned dimensions cause substantial SDPA performance degradation, and repair recovers this performance.

\begin{table}[t]
\centering
\caption{\textbf{Positive validation}: SDPA speedup across 45 real workloads (batch sizes 1--8, sequences 512--2048). Direct compression scenario where SDPA operates on misaligned dimensions. Higher speedups at larger batches due to increased Tensor Core utilization sensitivity.}
\label{tab:direct_sdpa}
\small
\begin{tabular}{llrrrr}
\toprule
Misaligned & Repaired & Avg & Std & Min & Max \\
\midrule
107 & 112 & \textbf{78.5\%} & 29.2\% & 46.3\% & 139.5\% \\
114 & 120 & \textbf{80.2\%} & 29.0\% & 46.9\% & 139.4\% \\
117 & 120 & \textbf{80.7\%} & 28.8\% & 47.0\% & 139.5\% \\
121 & 128 & \textbf{97.0\%} & 38.4\% & 55.1\% & 177.2\% \\
125 & 128 & \textbf{98.1\%} & 39.7\% & 55.4\% & 181.4\% \\
\midrule
\multicolumn{2}{l}{\textbf{Overall}} & \textbf{86.9\%} & 34.5\% & 46.3\% & 181.4\% \\
\bottomrule
\end{tabular}
\end{table}

\smallskip
\noindent\fbox{\parbox{0.96\columnwidth}{%
\textbf{Dual Validation Summary.}
Our applicability framework correctly predicts repair efficacy:
\textbf{(1)} Projection-based: \textbf{--0.8\%} (RAP SVD, no benefit as predicted, \S\ref{sec:negative_e2e}).
\textbf{(2)} Direct compression: \textbf{+86.9\% E2E speedup} (45 SDPA workloads, validated experimentally, \S\ref{sec:positive_e2e}).
This dual validation confirms practitioners can trust the framework to avoid wasted effort.
}}

\subsection{Applicability Framework: Practitioner Guidance}
\label{sec:applicability}

The contrasting end-to-end results (--0.8\% vs.\ +86.9\%) validate our applicability framework.
Table~\ref{tab:applicability} synthesizes these findings into actionable practitioner guidance: \textbf{before applying dimension repair}, consult this table to determine whether your compression architecture will benefit.

\begin{table}[t]
\centering
\setlength{\tabcolsep}{4pt}
\caption{Applicability framework (validated by contrasting experiments).}
\label{tab:applicability}
\small
\begin{tabular}{@{}p{2.8cm}p{1.8cm}ll@{}}
\toprule
\textbf{Architecture} & \textbf{SDPA dim} & \textbf{Effect} & \textbf{Val.} \\
\midrule
\textbf{Direct} (vanilla SVD) & Misaligned & \textbf{+86.9\%} & \S\ref{sec:positive_e2e} \\
\textbf{Projection} (RAP SVD) & Aligned & \textbf{--0.8\%} & \S\ref{sec:negative_e2e} \\
\textbf{Quantization} (GPTQ) & Unchanged & N/A & --- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Guidance}: Verify SDPA operates on compressed dims.
Projection-based (RAP SVD): no benefit (--0.8\%).
Direct compression (vanilla SVD): +86.9\% speedup.
Production PaLU enforces 32-alignment internally~\cite{palu}; our work explains why this is necessary.

\subsection{Kernel-Level Analysis}
\label{sec:kernel_analysis}

Having validated the applicability framework through contrasting E2E experiments (\S\ref{sec:negative_e2e}--\S\ref{sec:positive_e2e}), we now dissect the kernel-level mechanisms that explain \emph{why} dimension repair works for direct compression.
This section provides microbenchmark evidence supporting the +86.9\% E2E speedup.

\subsubsection{SDPA Padding Rescue}
\label{sec:padding}

Padding $d$=107 to aligned values demonstrates significant performance gains.
Table~\ref{tab:repair_perf} shows detailed measurements: padding to 112 achieves 27.8\% speedup with only 4.7\% memory overhead---an excellent tradeoff.

\subsubsection{GEMM Alignment Impact}

GEMM operations show similar patterns: K=107 achieves 0.089ms latency, while K=112 and K=128 both achieve 0.050ms---a \textbf{44\% improvement} from alignment.

\subsubsection{Dimension Repair Implementation}
\label{sec:repair_validation}

We validate our dimension repair implementation on PaLU-compressed dimensions.
Figure~\ref{fig:repair_tradeoff} visualizes the speedup vs. memory overhead tradeoff for different strategies.

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figures/fig5_repair_tradeoff.pdf}
\caption{Speedup vs.\ memory overhead tradeoff for dimension repair. $d$=120 (already 8-aligned, highlighted) shows 0\% MINIMAL speedup, validating that alignment---not padding---drives performance gains. Average ROI: MINIMAL 5.9$\times$ (22\%/3.7\%), OPTIMAL 3.5$\times$ (25\%/7.2\%).}
\label{fig:repair_tradeoff}
\end{figure}

Table~\ref{tab:repair_perf} shows SDPA performance for repaired dimensions.
Memory overhead: MINIMAL 3.72\%, OPTIMAL 7.20\%.

\begin{table}[h!]
\centering
\caption{SDPA repair latency (ms$\pm$std, $B$=4, $S$=2048, $H$=32). Independent run; 6\% variance normal (\S\ref{sec:phenomenon}).}
\label{tab:repair_perf}
\small
\begin{tabular}{lrrrrr}
\toprule
$d$ & Orig & Min & Opt & $\Delta$Min & $\Delta$Opt \\
\midrule
107 & 2.06{\tiny$\pm$.06} & 1.49{\tiny$\pm$.04} & 1.51{\tiny$\pm$.04} & \textbf{+27.8} & +27.0 \\
114 & 2.05{\tiny$\pm$.06} & 1.55{\tiny$\pm$.04} & 1.43{\tiny$\pm$.04} & +24.4 & \textbf{+30.1} \\
117 & 2.05{\tiny$\pm$.06} & 1.57{\tiny$\pm$.04} & 1.43{\tiny$\pm$.04} & +23.7 & \textbf{+30.2} \\
120 & 1.56{\tiny$\pm$.04} & 1.56{\tiny$\pm$.04} & 1.43{\tiny$\pm$.04} & 0.0 & +8.3 \\
121 & 1.96{\tiny$\pm$.05} & 1.43{\tiny$\pm$.04} & 1.44{\tiny$\pm$.04} & \textbf{+27.2} & +26.6 \\
125 & 1.98{\tiny$\pm$.05} & 1.44{\tiny$\pm$.04} & 1.44{\tiny$\pm$.04} & \textbf{+27.1} & +27.1 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Findings.}
MINIMAL achieves 23--28\% speedup with 3.72\% overhead (average ROI: speedup\%/overhead\% = 22\%/3.7\% = 5.9$\times$).
$d$=120 validates alignment: 8-aligned (0\% MINIMAL gain) but OPTIMAL pads to 128 for +8.3\%.
These kernel-level improvements explain the mechanisms behind the +86.9\% E2E speedup observed in \S\ref{sec:positive_e2e}.

\subsection{Accuracy Preservation}

Zero-padding guarantees \textbf{bit-exact output preservation}: $y'[0:d_{out}] = y$ exactly.
Unit tests confirm identical outputs (30/30 passed).
WikiText-2 perplexity validation on RAP SVD ($r$=0.8, $d$=102) confirms repair produces \textbf{identical perplexity}: baseline 11.08, RAP SVD 92.39, RAP SVD + repair 92.39 (higher PPL from compression, not repair).
All perplexity measurements use lm-eval-harness~\cite{lmeval} for reproducibility.

For version-specific notes, see \S\ref{sec:conclusion}.

\subsection{Scope and Limitations}
\label{sec:limitations}

\noindent\textbf{L1. Applicability:} The 43--91\% misalignment is from theoretical importance-based rank allocation (four methods, all projections); production PaLU checkpoints enforce 32-multiple alignment. Our findings apply to unconstrained SVD and future methods.

\noindent\textbf{L2. Evaluation Scope:} Perplexity validated (RAP SVD: 92.39 before/after repair); comprehensive downstream tasks (MMLU, reasoning) are future work.

\noindent\textbf{L3. Hardware:} All experiments on A100. H100 (4th-gen Tensor Cores, TMA, WGMMA) validation is future work.

\noindent\textbf{L4. Software:} Specific to FlashAttention 2.7.4. Future versions may handle alignment internally.


%% ===========================================
%% 7. RELATED WORK
%% ===========================================
\section{Related Work}
\label{sec:related}

We position our work at the intersection of LLM compression and GPU microarchitecture, where compression-induced dimensional irregularities violate hardware alignment constraints.
Recent surveys~\cite{hw_accel_survey2025,llm_compression_survey2025,model_compression_survey2025} document this trend toward hardware-aware optimization.

\subsection{GPU Architecture Evolution}

GPU alignment requirements have evolved across NVIDIA Tensor Core generations~\cite{nvidia_tensor_core_evolution2024}.
\textbf{Volta} (V100, 2017) introduced Tensor Cores with $K \bmod 8$ requirements for FP16 MMA operations~\cite{volta_whitepaper}, using quadpairs of 8 threads with 4$\times$4 tiles.
\textbf{Ampere} (A100, 2020) tightened constraints to $K \bmod 16$ for optimal utilization~\cite{ampere_whitepaper}, adopting warp-level (32 threads) m16n8k16 tiles with BF16 support.
\textbf{Hopper} (H100, 2022) introduced the Tensor Memory Accelerator (TMA) with 128-byte cache-line-aware transfers and warpgroup execution (128 threads)~\cite{nvidia_hopper_whitepaper,hopper_microbenchmark2024}, further emphasizing structured data access.
Recent work on TMA-adaptive GEMM~\cite{tma_fp8_grouped_gemm2025} demonstrates Hopper-specific optimizations achieving 23.8\% memory overhead reduction while satisfying strict TMA alignment.
FlashAttention-3~\cite{flashattention3} achieves 75\% of H100's theoretical peak (740 TFLOPs/s) by targeting specific dimensions (64, 128, 256), but \emph{removes} support for head\_dim 96 and 112 on Hopper---exemplifying the trend toward stricter dimension constraints in newer architectures.

\subsection{LLM Compression and Hardware Alignment}

\paragraph{Compression Methods.}
Post-training compression includes \textbf{pruning}~\cite{sparsegpt,maskllm2024}, \textbf{quantization} (GPTQ~\cite{gptq}, AWQ~\cite{awq}, LLM.int8()~\cite{llmint8_2022}, INT4~\cite{int4_quantization2023}), \textbf{SVD-based} (PaLU~\cite{palu}, SVD-LLM~\cite{svdllm2024}, Fisher-weighted~\cite{fwsvd2022,gfwsvd2025}), and \textbf{KV cache}~\cite{h2o,quest,pyramidkv}.
Quantization methods preserve dimensions via fixed-width groups (GPTQ uses 128-width groups; AWQ protects salient weights), inherently avoiding alignment issues.
SVD methods can produce irregular ranks when optimizing purely for accuracy---SVD-LLM~\cite{svdllm2024} achieves 3.1$\times$ GPU speedup via truncation-aware decomposition; Fisher-weighted SVD~\cite{fwsvd2022} and its Kronecker-decomposed extension~\cite{gfwsvd2025} optimize rank allocation based on parameter importance but do not explicitly enforce alignment.

\paragraph{GPU Alignment Constraints.}
NVIDIA documentation~\cite{nvidia_perf_guide,nvidia_dl_perf2024} specifies multiples-of-8/16 alignment for FP16/INT8, as Tensor Cores require aligned tiles for peak efficiency (TF32: $K \bmod 8$, INT8: $K \bmod 16$).
CUTLASS~\cite{cutlass,cutlass_alignment2024} explicitly requires 128-bit vectorized memory accesses and dimension-aware tiles; wave quantization inefficiency arises when tiles are not divisible by SM counts.
Memory coalescing studies~\cite{memory_coalescing2024} show irregular dimensions cause intra-warp divergence, incurring up to 60\% padding overhead for 3D structures---GPUs combine 32 thread accesses into single 128B transactions only when data is consecutive and aligned.

\subsection{Hardware-Aware Optimization}

Hardware-aware methods optimize measured latency rather than FLOPs alone.
\textbf{AMC}~\cite{amc2018} pioneered RL-based per-layer compression with latency constraints, achieving 1.95$\times$ mobile speedup by substituting FLOPs with actual device measurements.
\textbf{HALP}~\cite{halp2021} formulates structural pruning as latency-budgeted optimization, demonstrating that networks with similar FLOPs can exhibit vastly different latencies due to hardware characteristics.
\textbf{HALOC}~\cite{haloc2023} frames rank selection as neural architecture search with differentiable latency objectives, criticizing prior low-rank methods for ignoring hardware constraints during compression.
\textbf{HAPE}~\cite{hape2025} integrates genuine on-device profiling into pruning importance metrics, moving beyond bare sparsity ratios.
\textbf{Despite this progress, none explicitly model alignment constraints} (8/16/32-multiple dimensions) as first-class optimization objectives---they measure aggregate latency but do not isolate dimensional misalignment as a root cause.

FlashAttention~\cite{flashattention,flashattention2,flashattention3} provides hand-optimized kernels for specific dimensions ($\{32, 64, 96, 128, 256\}$ on Ampere; reduced set on Hopper); others trigger 30--45\% slower generic paths (\S\ref{sec:cuda}).
Serving systems handle irregularities differently: vLLM~\cite{vllm,vllm_dimension_handling2024} hardcodes supported dimensions and raises errors for unsupported values; TensorRT~\cite{tensorrt,tensorrt_padding2024} performs runtime CUDA graph padding to maximize cached graph hit rates (trading minor padding overhead for throughput gains); kernel frameworks (Triton~\cite{triton}, CUTLASS~\cite{cutlass_alignment2024}) expose alignment requirements directly to developers.

\subsection{Why Prior Work Missed Alignment}

Production systems converged on alignment empirically through profiling, but this knowledge remained undocumented.
\textbf{PaLU}~\cite{palu} enforces 32-multiple alignment internally (verified across all 24 released checkpoints), yet the paper does not explain \emph{why}---likely discovered through iterative GPU profiling rather than principled analysis.
\textbf{GPTQ}~\cite{gptq} and \textbf{AWQ}~\cite{awq} sidestep the issue by preserving original dimensions through fixed-width quantization groups (typically 128 elements), avoiding irregular shapes entirely.
\textbf{SVD methods}~\cite{svdllm2024,fwsvd2022,gfwsvd2025} optimize for reconstruction error or task accuracy without hardware-aware rank allocation---when unconstrained, importance-based ranks produce 43--91\% misaligned dimensions across scoring methods and projections (\S\ref{sec:scope}).
Serving systems~\cite{vllm_dimension_handling2024,tensorrt_padding2024} handle misalignment reactively (rejecting inputs or runtime padding) rather than preventing it during compression.

\subsection{Our Contribution}

\textbf{Our contribution}: First systematic diagnosis connecting compression-induced irregularities to GPU microarchitecture (Tensor Core tiles, vectorized loads~\cite{memory_coalescing2024}, bandwidth), quantifying \emph{how much} irregular dimensions cost (58\% TC, 50\% vectorized load, 40\% SDPA bandwidth) rather than just noting alignment matters.
While production systems converged on alignment empirically, we provide the \emph{diagnostic framework} explaining \emph{why} alignment is necessary and \emph{when} repair helps versus when it does not (Table~\ref{tab:applicability}).
This knowledge enables compression method designers to integrate alignment constraints proactively rather than discovering them through trial-and-error profiling.
Table~\ref{tab:dim_handling} compares dimension handling strategies.

\begin{table}[t]
\centering
\caption{Head dimension handling across systems.}
\label{tab:dim_handling}
\small
\begin{tabular}{@{}lll@{}}
\toprule
System & Supported \texttt{head\_dim} & Misaligned \\
\midrule
FlashAttn-2 & 32,64,96,128,256 (opt.) & +30--45\% \\
vLLM & 64,80,96,112,128,256 & Error \\
TensorRT & 32,40,64,80,96,104,128... & Runtime pad \\
\midrule
GPTQ/AWQ & Preserves original & N/A \\
PaLU & 32-multiple & N/A \\
RAP SVD & Any integer & \textbf{Affected} \\
\midrule
\textbf{This work} & Repair to 8/16-multiple & Compile-time \\
\bottomrule
\end{tabular}
\end{table}


%% ===========================================
%% 8. CONCLUSION
%% ===========================================
\section{Conclusion}
\label{sec:conclusion}

We presented the first systematic diagnosis of \emph{dimensional collapse}---GPU performance degradation caused by compression-induced irregular tensor dimensions.
Our contributions span three layers: (1)~quantifying phenomenon severity through controlled SDPA and GEMM benchmarks, (2)~identifying root causes via microarchitectural profiling (Tensor Core misalignment, vectorized load degradation, bandwidth inefficiency), and (3)~providing validated applicability framework predicting when dimension repair helps versus when it does not.

The diagnostic framework enables compression method designers to integrate alignment constraints proactively.
When applicable, dimension repair achieves substantial kernel-level speedups with modest memory overhead.
Our contrasting end-to-end validation---negative case (projection-based architectures) and positive case (direct compression)---confirms practitioners can trust the framework to avoid wasted effort.

\paragraph{Integration Guidance.}
Our dimension repair integrates as a post-compression pass in frameworks like PaLU, SVD-LLM~\cite{svdllm}, or custom SVD pipelines.
For direct compression methods, pad immediately after rank selection: $d_{pad} = \lceil d_{orig}/a \rceil \times a$ where $a=8$ (MINIMAL) or $a=16$ (OPTIMAL).
Zero-pad weight matrices and biases; no accuracy loss occurs.
For projection-based methods, no repair needed---SDPA operates on projected dimensions that are already aligned.

The applicability framework (Table~\ref{tab:applicability}) distinguishes these architectural patterns.
Direct compression flows compressed head\_dim to SDPA, triggering slow paths when misaligned; repair recovers performance.
Projection-based methods use intermediate layers restoring aligned head\_dim before SDPA, making latent misalignment irrelevant to attention bottlenecks.

\paragraph{Scope.}
All experiments focus on NVIDIA A100 with FlashAttention 2.7.4.
H100 validation and comprehensive downstream task evaluation are future work.
While production PaLU enforces alignment internally, our findings explain why this is necessary and guide future compression methods optimizing purely for accuracy.

\paragraph{Reproducibility.}
Code, experiment scripts, and raw data are available at \url{https://github.com/[ANONYMIZED]}.
Upon acceptance, we will release all benchmarking infrastructure, dimension repair implementations, and experimental configurations to facilitate reproduction and extension of our findings.

%% ===========================================
%% REFERENCES
%% ===========================================
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

%% ===========================================
%% APPENDIX
%% ===========================================
\appendix

\section{Dimension Distribution Across Models and Retain Ratios}
\label{app:scatter}

This appendix extends the dimension distribution analysis from \S\ref{sec:scope}.
Figure~\ref{fig:dim_scatter} in the main text shows Llama-3-8B at $r{=}0.8$.
Below we present: (1) Mistral-7B at $r{=}0.8$ (Figure~\ref{fig:scatter_mistral}),
and (2) both models at other retain ratios (Figures~\ref{fig:scatter_llama_ratios}--\ref{fig:scatter_mistral_ratios}).
The misalignment pattern is consistent: regardless of model, scoring method, projection matrix, or retain ratio, unconstrained rank allocation produces 43--91\% non-8-aligned dimensions.

\begin{figure*}[h]
\centering
\begin{minipage}{\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figures/fig_scatter_llama_r05.pdf}\\[2pt]
  \includegraphics[width=\textwidth]{figures/fig_scatter_llama_r06.pdf}\\[2pt]
  \includegraphics[width=\textwidth]{figures/fig_scatter_llama_r07.pdf}\\[2pt]
  \includegraphics[width=\textwidth]{figures/fig_scatter_llama_r09.pdf}
\end{minipage}
\caption{Llama-3-8B dimension distributions at retain ratios 0.5, 0.6, 0.7, 0.9 (top to bottom). Lower ratios produce wider rank spread and higher misalignment.}
\label{fig:scatter_llama_ratios}
\end{figure*}

\begin{figure*}[h]
\centering
\begin{minipage}{\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figures/fig_scatter_mistral_r05.pdf}\\[2pt]
  \includegraphics[width=\textwidth]{figures/fig_scatter_mistral_r06.pdf}\\[2pt]
  \includegraphics[width=\textwidth]{figures/fig_scatter_mistral_r07.pdf}\\[2pt]
  \includegraphics[width=\textwidth]{figures/fig_scatter_mistral_r08.pdf}\\[2pt]
  \includegraphics[width=\textwidth]{figures/fig_scatter_mistral_r09.pdf}
\end{minipage}
\caption{Mistral-7B dimension distributions at retain ratios 0.5, 0.6, 0.7, 0.9 (top to bottom).}
\label{fig:scatter_mistral_ratios}
\end{figure*}

\end{document}
