%%
%% G-Compress: Dimensional Collapse in Compressed LLMs
%% Target: EuroMLSys (SIGPLAN format, 6 pages excluding references)
%%

\documentclass[sigplan,10pt,nonacm]{acmart}

%% Remove ACM-specific elements for submission
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

%% Packages
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}

%% Code listing style
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  numbers=left,
  numberstyle=\tiny,
}

%% Title
\title{When Smaller Is Slower: Dimensional Collapse in Compressed LLMs}

%% Authors
\author{Jihao Xin}
\affiliation{
  \institution{KAUST}
  \city{Thuwal}
  \country{Saudi Arabia}
}
\email{jihao.xin@kaust.edu.sa}

\author{Tian Lvy}
\affiliation{
  \institution{KAUST}
  \city{Thuwal}
  \country{Saudi Arabia}
}

\author{Qilong Pan}
\affiliation{
  \institution{HUMAIN AI}
  \city{Riyadh}
  \country{Saudi Arabia}
}

\author{Kesen Wang}
\affiliation{
  \institution{HUMAIN AI}
  \city{Riyadh}
  \country{Saudi Arabia}
}

\begin{abstract}
Post-training compression techniques for Large Language Models (LLMs) often result in irregular tensor dimensions (e.g., \texttt{head\_dim=107} instead of 128).
We identify a phenomenon called \emph{dimensional collapse}: despite reducing FLOPs, these irregular dimensions cause significant inference slowdowns on modern GPUs.
Our experiments on NVIDIA A100 show that \texttt{head\_dim=107} increases SDPA latency by 88\% compared to \texttt{head\_dim=96}.

We systematically investigate the root causes across three layers.
Contrary to initial assumptions, FlashAttention does \emph{not} fall back to slower backends---instead, it uses an internal slow path with 30--45\% overhead.
At the hardware level, we identify three confirmed root causes: Tensor Core tile alignment (58\% slowdown when K\%16$\neq$0), vectorized load degradation (50\% throughput loss when falling back to scalar loads), and SDPA bandwidth efficiency loss (40\%).
Notably, L2 cache sector waste (5.8\%) is \emph{not} a significant factor.
Based on our findings, we formalize a \emph{Shape Contract} and propose a lightweight \emph{dimension repair} pass.
\textbf{Scope of validation:} We evaluate dimension repair at the kernel level (SDPA and GEMM microbenchmarks), demonstrating 25--30\% performance recovery with only 3.7\% memory overhead.
End-to-end integration with SVD-based compression (e.g., PaLU) requires adapting the repair pass to factorized weight structures ($W = U \cdot V^T$), which we identify as future work.
\end{abstract}

\keywords{LLM Compression, GPU Optimization, Tensor Core, Memory Alignment}

\begin{document}

\maketitle

%% ===========================================
%% 1. INTRODUCTION
%% ===========================================
\section{Introduction}
\label{sec:intro}

Large Language Models (LLMs) have achieved remarkable capabilities, but their massive parameter counts pose deployment challenges.
Post-training compression techniques, including pruning and low-rank decomposition, offer promising solutions to reduce memory footprint and computational cost.
However, these techniques often produce models with \emph{irregular tensor dimensions}---values that do not align with hardware-preferred multiples (e.g., 8, 16, 32, 128).

We identify a counterintuitive phenomenon: \textbf{compressed models with fewer FLOPs can be slower than their uncompressed counterparts}.
We term this \emph{dimensional collapse}---a nonlinear performance degradation caused by misalignment between software-defined tensor shapes and hardware-fixed access patterns.

\paragraph{Motivating Example.}
Consider PaLU~\cite{palu}, a state-of-the-art low-rank compression method that reduces attention head dimensions through SVD.
When compressing Llama-3-8B with a 0.8 retention ratio, the resulting \texttt{head\_dim} values become irregular (e.g., 114--125 instead of 128).
Our analysis shows that 96.9\% of the compressed dimensions are not 8-aligned.
Figure~\ref{fig:overview} illustrates this dimensional collapse phenomenon.
On an NVIDIA A100, this causes:
\begin{itemize}
  \item 88\% increase in SDPA latency
  \item FlashAttention internal slow path with 30--45\% overhead
  \item MEM\_EFFICIENT unavailable (strict 8-alignment)
  \item Bandwidth waste from cache misalignment
\end{itemize}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig1_overview.pdf}
\caption{Dimensional collapse in compressed LLMs. Post-training compression (e.g., PaLU) produces irregular head dimensions (114--125) that violate GPU alignment requirements, causing performance cliffs despite reduced FLOPs.}
\label{fig:overview}
\end{figure}

\paragraph{Contributions.}
This paper makes the following contributions:
\begin{enumerate}
  \item \textbf{Quantification}: We measure the performance impact of irregular dimensions across GEMM and SDPA (\S\ref{sec:phenomenon}).
  \item \textbf{Root Cause Analysis}: We identify the causes across three layers: PyTorch backend selection, CUDA kernel paths, and hardware constraints (\S\ref{sec:causes}).
  \item \textbf{Shape Contract}: We formalize dimension alignment requirements as optimization constraints (\S\ref{sec:solution}).
  \item \textbf{Dimension Repair}: We propose a lightweight post-compression pass that restores alignment (\S\ref{sec:solution}).
  \item \textbf{Evaluation}:
  \begin{itemize}
    \item \emph{Validated}: Kernel-level experiments on SDPA and GEMM microbenchmarks demonstrate 25--30\% speedup with 3.7--4.7\% memory overhead.
    \item \emph{Future Work}: End-to-end integration with SVD-based compression requires adapting to factorized weight structures ($W = U \cdot V^T$).
  \end{itemize}
  (\S\ref{sec:eval}).
\end{enumerate}

%% ===========================================
%% 2. BACKGROUND
%% ===========================================
\section{Background}
\label{sec:background}

\paragraph{Notation.}
We use $d$ (also written as \texttt{head\_dim} in code) to denote the attention head dimension.
In tables and figures, we use ``D=\textit{value}'' (e.g., D=107) as shorthand for $d=\textit{value}$.
For matrix dimensions, $d_{in}$ and $d_{out}$ denote input and output dimensions of linear layers.
$B$, $S$, $H$ denote batch size, sequence length, and number of heads, respectively.

\subsection{Tensor Core Alignment}

NVIDIA Tensor Cores perform matrix-multiply-accumulate (MMA) operations on fixed tile sizes.
For FP16 on A100, the optimal tile requires $K \mod 16 = 0$.
Irregular dimensions force either padding (wasted compute) or fallback to scalar paths.

\subsection{FlashAttention Constraints}

FlashAttention-2~\cite{flashattention2} (v2.7.4) is the de facto standard for efficient attention.
Contrary to common belief, it does \emph{not} strictly require 8-aligned dimensions---it remains available for all tested dimensions (104--128).
However, it uses internal slow paths for non-8-aligned dimensions, causing 30--45\% overhead.
Optimized kernels exist for $\{32, 64, 96, 128, 256\}$. MEM\_EFFICIENT strictly requires 8-alignment.

\subsection{Low-Rank Compression}

PaLU~\cite{palu} compresses attention by applying SVD to K/V projections:
$W_{kv} \approx U_r \Sigma_r V_r^T$ where $r < d$.
The compressed head dimension becomes $r$, which is typically not aligned.

%% ===========================================
%% 3. DIMENSIONAL COLLAPSE PHENOMENON
%% ===========================================
\section{Dimensional Collapse}
\label{sec:phenomenon}

\subsection{Experiment Setup}

We conduct experiments on NVIDIA A100-80GB with PyTorch 2.9.1, CUDA 12.8, and FlashAttention 2.7.4.
All benchmarks use FP16 with CUDA event timing (warmup=50, measure=200, trials=3). Driver: 560.35.03; cuDNN 9.1.0.

\subsection{Compression Produces Misaligned Dimensions}

We analyze PaLU-compressed Llama-3-8B (retention ratio 0.8).
Figure~\ref{fig:palu_dist} shows the dimension distribution after compression.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig3_palu_dist.pdf}
\caption{Distribution of head dimensions after PaLU compression (Llama-3-8B, r=0.8). 96.9\% of the 512 KV heads have misaligned dimensions. Only D=120 (3.1\%) is 8-aligned; none are 16-aligned.}
\label{fig:palu_dist}
\end{figure}

\subsection{SDPA Latency vs. Head Dimension}

We sweep \texttt{head\_dim} from 64 to 160 with shape $B=4, S=2048, H=32$.
Figure~\ref{fig:sdpa_latency} shows the results.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig2_sdpa_latency.pdf}
\caption{SDPA latency across head dimensions. Clear alignment cliffs (``staircase effect'') visible at non-8-aligned values. D=107 shows 88\% increase vs D=96. The FLASH backend uses internal slow paths for misaligned dimensions.}
\label{fig:sdpa_latency}
\end{figure}

Key observations:
\begin{itemize}
  \item 8-aligned dimensions (72, 80, 88, 96, ...) achieve 1.1--1.6 ms
  \item Non-8-aligned dimensions (65--71, 97--103, ...) incur 1.6--2.2 ms
  \item \texttt{head\_dim=107}: 2.147 ms (+88\% vs 96)
\end{itemize}

\subsection{Backend Selection Behavior}

Table~\ref{tab:backend} shows latency across different SDPA backends.

\begin{table}[t]
\centering
\caption{SDPA backend latency (ms) for various head dimensions. MEM\_EFFICIENT fails for D=107.}
\label{tab:backend}
\begin{tabular}{lrrrr}
\toprule
head\_dim & AUTO & FLASH & MEM\_EFF & MATH \\
\midrule
96  & 1.17 & 1.12 & 2.38 & 26.0 \\
104 & 1.54 & 1.54 & 2.75 & 26.5 \\
\textbf{107} & \textbf{2.14} & \textbf{2.14} & FAIL & \textbf{27.0} \\
112 & 1.53 & 1.53 & 2.60 & 27.1 \\
128 & 1.47 & 1.47 & 2.55 & 28.1 \\
\bottomrule
\end{tabular}
\end{table}

The MATH backend is 12.6$\times$ slower than FLASH for D=107.
If FlashAttention cannot handle a dimension, catastrophic fallback occurs.

%% ===========================================
%% 4. ROOT CAUSE ANALYSIS
%% ===========================================
\section{Root Cause Analysis}
\label{sec:causes}

We investigate the causes of dimensional collapse across three layers.

\subsection{PyTorch Backend Selection}
\label{sec:backend}

\paragraph{Initial Hypothesis.}
PyTorch's SDPA automatically selects attention backends based on input shapes.
We initially hypothesized that non-8-aligned dimensions would trigger fallback to slower backends.

\paragraph{Experimental Verification (C21).}
We tested backend availability for \texttt{head\_dim} $\in$ [104, 128].
Surprisingly, FlashAttention is available for \emph{all} tested dimensions, including non-8-aligned values.

\begin{table}[t]
\centering
\caption{Backend availability across head dimensions (C21 experiment).}
\label{tab:backend_avail}
\begin{tabular}{lrr}
\toprule
Backend & 8-aligned & Non-8-aligned \\
\midrule
FLASH & 100\% (8/8) & 100\% (42/42) \\
MEM\_EFFICIENT & 100\% (8/8) & 0\% (0/42) \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Finding.}
FlashAttention does \emph{not} fall back to MATH for non-8-aligned dimensions.
Instead, it uses an internal slow path that incurs 30--45\% overhead:

\begin{itemize}
  \item 8-aligned (D=104,112,120,128): avg 1.55 ms
  \item Non-8-aligned (D=105--111,113--119,...): avg 2.03 ms (+31\%)
\end{itemize}

This ``staircase effect'' (Figure~\ref{fig:sdpa_latency}) shows that latency jumps discretely at alignment boundaries, not gradually with dimension size.
The root cause lies in the CUDA kernel layer, not backend selection.

\subsection{CUDA Kernel Layer}
\label{sec:cuda}

Since backend selection is not the root cause, we investigate FlashAttention's internal kernel behavior.
The 30--45\% slowdown on non-8-aligned dimensions stems from several factors:

\begin{enumerate}
  \item \textbf{Vectorized loads}: FlashAttention uses \texttt{float4} (128-bit) loads when $d \mod 8 = 0$. Non-aligned dimensions require scalar loads or predicated vector loads with 50\% throughput loss.
  \item \textbf{GEMM tile selection}: CUTLASS kernels select different tile sizes based on dimension alignment. Suboptimal tiles reduce occupancy and Tensor Core utilization (30\%$\to$12\%).
  \item \textbf{Predication}: Non-aligned dimensions cause boundary predicates, increasing warp divergence.
  \item \textbf{Internal padding}: FlashAttention may pad internally, less efficiently than explicit pre-padding.
\end{enumerate}

These factors compound to create the observed ``staircase effect'' where latency jumps discretely at 8-alignment boundaries.

\paragraph{FlashAttention Kernel Dispatch.}
FlashAttention-2~\cite{flashattention2} dispatches to optimized kernels for D $\in \{32, 64, 96, 128, 256\}$.
For other D values, it uses generic kernels with: (1) predicated loads, (2) dynamic shared memory, and (3) suboptimal tiling. D=128 achieves 1.47ms while D=125 (with predicates) takes 1.97ms (+34\%).

\subsection{Hardware Constraints}
\label{sec:hardware}

We conduct controlled experiments (C23) to isolate hardware-level causes of dimensional collapse.
Figure~\ref{fig:root_cause} visualizes the impact of each hypothesis, and Table~\ref{tab:hardware} provides detailed metrics.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig4_root_cause.pdf}
\caption{Root cause breakdown. Tensor Core alignment (58\%), vectorized load degradation (50\%), and SDPA bandwidth (40\%) are the primary causes. L2 cache sector waste (5.8\%) is negligible.}
\label{fig:root_cause}
\end{figure}

\begin{table}[t]
\centering
\caption{Hardware layer root cause analysis (C23 experiment). Impact measured on A100 with FP16.}
\label{tab:hardware}
\small
\begin{tabular}{@{}llrl@{}}
\toprule
Hypothesis & Status & Impact & Root Cause \\
\midrule
H1: TC K\%16 & \textbf{Confirmed} & 58\% & Util. 30\%$\to$12\% \\
H2: L2 sector & Not confirmed & 5.8\% & Negligible \\
H3: SDPA BW & \textbf{Confirmed} & 40\% & Access pattern \\
H4: Vec. loads & \textbf{Confirmed} & 50\% & float4$\to$scalar \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{H1: Tensor Core Alignment (Confirmed).}
We measure GEMM throughput for shape (8192, 8192, K) with varying K alignment.
16-aligned K achieves 91.1 TFLOPS, 8-aligned achieves 76.8 TFLOPS, while non-aligned (e.g., K=107) drops to 37--40 TFLOPS---a 58\% slowdown.
Tensor Core utilization decreases from $\sim$30\% to $\sim$12\%.

\paragraph{H2: L2 Cache Sectors (Not Confirmed).}
L2 cache operates in 32-byte sectors.
Non-aligned dimensions cause $\sim$5.8\% sector waste, but measured bandwidth actually increases slightly for non-aligned dimensions (214.6 vs 209.3 GB/s).
This cannot explain the 30--58\% performance gap.

\paragraph{H3: SDPA Bandwidth Efficiency (Confirmed).}
SDPA bandwidth efficiency strongly correlates with 8-alignment:
\begin{itemize}
  \item D=112: 1.529 ms, 153.6 GB/s
  \item D=113: 2.208 ms, 107.3 GB/s (\textbf{--30\%})
  \item D=120: 1.571 ms, 160.2 GB/s
  \item D=121: 2.142 ms, 118.5 GB/s (\textbf{--26\%})
\end{itemize}

\paragraph{H4: Vectorized Loads (Confirmed).}
FlashAttention uses \texttt{float4} (128-bit) loads when dimensions permit.
Table~\ref{tab:vectorize} shows the impact of load type on GEMM throughput.

\begin{table}[t]
\centering
\caption{Vectorized load patterns and GEMM throughput (TFLOPS).}
\label{tab:vectorize}
\begin{tabular}{llll}
\toprule
Load Type & Alignment & K examples & TFLOPS \\
\midrule
float4 & K\%16==0 & 112, 128 & 73--83 \\
float4 & K\%8==0 & 104, 120 & 68--77 \\
float2 & K\%4==0 & 108, 116 & 61--71 \\
\textbf{scalar} & none & 105, 107 & \textbf{39--40} \\
\bottomrule
\end{tabular}
\end{table}

Non-aligned dimensions fall back to scalar loads, causing 50\% throughput loss.

\paragraph{Root Cause Summary.}
The performance hierarchy is:
(1) Tensor Core tile alignment (58\% impact, K\%16),
(2) Vectorized load degradation (50\% impact, K\%8),
(3) SDPA bandwidth efficiency (40\% impact).
L2 cache waste (5.8\%) is negligible.

%% ===========================================
%% 5. SHAPE-AWARE COMPRESSION
%% ===========================================
\section{Shape-Aware Compression}
\label{sec:solution}

\subsection{Shape Contract}

We formalize alignment requirements as a constraint optimization problem:
\begin{equation}
\begin{aligned}
\text{minimize} \quad & \text{memory\_overhead}(d_{pad}) \\
\text{s.t.} \quad & d_{pad} \mod 8 = 0 \\
& d_{pad} \geq d_{orig}
\end{aligned}
\end{equation}

For optimal Tensor Core utilization, prefer $d_{pad} \mod 16 = 0$.

\subsection{Dimension Repair Algorithm}

\begin{algorithm}[t]
\caption{Dimension Repair for Linear Layers}
\label{alg:repair}
\begin{algorithmic}
\REQUIRE $W \in \mathbb{R}^{d_{out} \times d_{in}}$: weight matrix
\REQUIRE $b \in \mathbb{R}^{d_{out}}$ (optional): bias vector
\REQUIRE $strategy$: alignment strategy (minimal, optimal)
\REQUIRE $alignment$: target alignment (8 or 16)
\STATE $d'_{out} \gets \lceil d_{out} / alignment \rceil \times alignment$
\STATE $W' \in \mathbb{R}^{d'_{out} \times d_{in}} \gets \mathbf{0}$
\STATE $W'[0:d_{out}, :] \gets W$ \COMMENT{Copy original weights}
\IF{$b$ exists}
  \STATE $b' \in \mathbb{R}^{d'_{out}} \gets \mathbf{0}$
  \STATE $b'[0:d_{out}] \gets b$ \COMMENT{Copy original bias}
\ENDIF
\IF{$b$ exists}
  \RETURN $W'$, $b'$
\ELSE
  \RETURN $W'$
\ENDIF
\end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:repair} shows our dimension repair procedure.
For a linear layer $y = Wx + b$, we pad the output dimension from $d_{out}$ to $d'_{out}$ by appending zero rows to $W$ and zeros to $b$.
The ``minimal'' strategy uses $alignment=8$, while ``optimal'' uses $alignment=16$.

\paragraph{Accuracy Preservation.}
Zero-padding preserves model outputs exactly.
For input $x \in \mathbb{R}^{d_{in}}$, the padded layer computes:
\[
y' = W'x + b' = \begin{bmatrix} Wx + b \\ \mathbf{0} \end{bmatrix} = \begin{bmatrix} y \\ \mathbf{0} \end{bmatrix}
\]
The original output $y$ occupies positions $[0:d_{out}]$, while positions $[d_{out}:d'_{out}]$ contain zeros.
Downstream layers that consume $y'$ must either: (1) slice the valid dimensions, or (2) use attention masks that ignore padded positions.
For attention mechanisms, zero-valued key/query dimensions contribute zero to attention scores, making padding semantically neutral.
This ensures \textbf{bit-exact output preservation}---no retraining or fine-tuning is required.

%% ===========================================
%% 6. EVALUATION
%% ===========================================
\section{Evaluation}
\label{sec:eval}

\paragraph{Scope of Validation.}
We evaluate dimension repair at two levels:
(1) \textbf{Kernel-level} (SDPA and GEMM microbenchmarks): fully validated, demonstrating 25--30\% performance recovery;
(2) \textbf{End-to-end LLM inference}: we compare baseline vs.\ PaLU-compressed models to show compression benefits, but dimension repair integration with PaLU's SVD structure ($W = U \cdot V^T$) remains future work.
The E2E speedups reported (\S\ref{sec:eval}.4) are from compression, not dimension repair.

\subsection{Padding Rescue Experiment (P1)}

Table~\ref{tab:padding} shows the effect of padding D=107 to aligned values.

\begin{table}[t]
\centering
\caption{Padding rescue results for SDPA (D=107 logical).}
\label{tab:padding}
\small
\begin{tabular}{lrrr}
\toprule
Phys. D & Mem. Ovhd. & Latency (ms) & Speedup \\
\midrule
107 (base) & 0\% & 2.192 & 1.00$\times$ \\
112 & 4.7\% & 1.523 & 1.44$\times$ \\
128 & 19.6\% & 1.445 & 1.52$\times$ \\
\bottomrule
\end{tabular}
\end{table}

Padding to 112 achieves 30.5\% speedup with only 4.7\% memory overhead---an excellent tradeoff.

\subsection{GEMM Alignment Impact}

GEMM operations show similar patterns:

\begin{table}[t]
\centering
\caption{GEMM latency (M=4096, N=4096) for different K dimensions.}
\label{tab:gemm}
\begin{tabular}{lrr}
\toprule
K dimension & Latency (ms) & vs K=107 \\
\midrule
107 & 0.089 & baseline \\
112 & 0.050 & 1.78$\times$ faster \\
128 & 0.050 & 1.78$\times$ faster \\
\bottomrule
\end{tabular}
\end{table}

Aligning K from 107 to 112 improves GEMM performance by 44\%.

\subsection{Dimension Repair Validation (C4)}

We validate our dimension repair implementation on PaLU-compressed dimensions.
Figure~\ref{fig:repair_tradeoff} visualizes the speedup vs. memory overhead tradeoff for different strategies.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig5_repair_tradeoff.pdf}
\caption{Per-dimension speedup vs. memory overhead for different repair strategies. Each point represents a PaLU dimension (D=107--125). MINIMAL (circles) achieves 6.9$\times$ average ROI, OPTIMAL (squares) provides 4.0$\times$ ROI. Dashed lines show iso-ROI curves.}
\label{fig:repair_tradeoff}
\end{figure}

Table~\ref{tab:repair_memory} shows the memory overhead of different repair strategies.

\begin{table}[t]
\centering
\caption{Memory overhead analysis for PaLU dimension repair (512 KV heads).}
\label{tab:repair_memory}
\begin{tabular}{llr}
\toprule
Strategy & Alignment Target & Memory Overhead \\
\midrule
MINIMAL & mod 8 & 3.72\% \\
OPTIMAL & mod 16 & 7.20\% \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:repair_perf} shows SDPA performance for repaired dimensions.

\begin{table}[t]
\centering
\caption{SDPA latency (ms) before and after dimension repair ($B$=4, $S$=2048, $H$=32). Note: D=107 baseline (2.064ms) differs from Table~\ref{tab:padding} (2.192ms) due to run-to-run variance ($\sim$6\%), which is within normal GPU measurement variability.}
\label{tab:repair_perf}
\begin{tabular}{lrrrrr}
\toprule
D & Original & Minimal & Optimal & $\Delta$Min & $\Delta$Opt \\
\midrule
107 & 2.064 & 1.490 & 1.506 & \textbf{+27.8\%} & +27.0\% \\
114 & 2.049 & 1.549 & 1.432 & +24.4\% & \textbf{+30.1\%} \\
117 & 2.054 & 1.567 & 1.433 & +23.7\% & \textbf{+30.2\%} \\
120 & 1.557 & 1.557 & 1.428 & 0\% & +8.3\% \\
121 & 1.964 & 1.430 & 1.441 & \textbf{+27.2\%} & +26.6\% \\
125 & 1.975 & 1.439 & 1.439 & \textbf{+27.1\%} & +27.1\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Findings.}
(1) MINIMAL achieves 23--28\% speedup with only 3.72\% memory overhead.
(2) D=120 (already 8-aligned) shows no improvement with MINIMAL, validating our hypothesis.
(3) OPTIMAL provides additional gains for 8-aligned dimensions (D=120: +8.3\%) by targeting mod-16.
(4) The speedup-to-overhead ratio (ROI) is 6.9$\times$ for MINIMAL, 4.0$\times$ for OPTIMAL.

\paragraph{PaLU Dimension Mapping.}
Table~\ref{tab:repair_map} shows how MINIMAL strategy repairs PaLU dimensions.

\begin{table}[t]
\centering
\caption{MINIMAL strategy repairs PaLU dimensions to nearest 8-aligned value.}
\label{tab:repair_map}
\begin{tabular}{cccc}
\toprule
Original & Repaired & Original & Repaired \\
\midrule
114 $\rightarrow$ 120 & (+6) & 121 $\rightarrow$ 128 & (+7) \\
116 $\rightarrow$ 120 & (+4) & 122 $\rightarrow$ 128 & (+6) \\
117 $\rightarrow$ 120 & (+3) & 123 $\rightarrow$ 128 & (+5) \\
118 $\rightarrow$ 120 & (+2) & 124 $\rightarrow$ 128 & (+4) \\
120 $\rightarrow$ 120 & (0) & 125 $\rightarrow$ 128 & (+3) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{End-to-End LLM Inference (C5)}

\noindent\fbox{\parbox{0.95\columnwidth}{
\textbf{Scope Note:} The results in this subsection show \emph{PaLU compression} benefits (reduced KV cache), \textbf{not} dimension repair speedups.
Our repair pass was not applied because PaLU's SVD factorization ($W_{kv} = U \cdot V^T$) requires specialized adaptation---see \S\ref{sec:solution} for discussion.
}}
\vspace{0.5em}

We evaluate end-to-end inference performance on Llama-3-8B comparing baseline and PaLU-compressed (ratio=0.7) variants on A100 80GB (PyTorch 2.9.1, CUDA 12.8).
Figure~\ref{fig:e2e} visualizes the prefill and decode throughput comparison.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig6_e2e.pdf}
\caption{End-to-end LLM inference results. \emph{See Scope Note above for interpretation.}}
\label{fig:e2e}
\end{figure}

\begin{table}[t]
\centering
\caption{End-to-end LLM inference metrics. \emph{See Scope Note above for interpretation.}}
\label{tab:e2e}
\begin{tabular}{lrrr}
\toprule
Metric & Baseline & PaLU & $\Delta$ \\
\midrule
Prefill (tok/s) & 9870 & 9672 & --2.0\% \\
\textbf{Decode (tok/s)} & \textbf{119} & \textbf{1371} & \textbf{+11.5$\times$} \\
Memory (MB) & 19003 & 18896 & --0.6\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Finding.}
PaLU compression achieves \textbf{11.5$\times$ decode throughput improvement} (Table~\ref{tab:e2e}).
The decode phase is memory-bound, and reducing KV cache size directly improves throughput.
Despite the 30--45\% overhead from misaligned dimensions (identified in \S\ref{sec:causes}), the compression benefit dominates in this memory-bound scenario.

\paragraph{Why Repair Is Not Applied.}
PaLU decomposes K/V projections as $W_{kv} = U \cdot V^T$, where $U \in \mathbb{R}^{d_{model} \times r}$ and $V^T \in \mathbb{R}^{r \times d_{head}}$.
The irregular dimension $r$ (e.g., 114--125) resides \emph{inside} the decomposition, not at layer boundaries.
Our current repair pass targets standard linear layers; adapting it to modify $U$ matrices while preserving the SVD structure requires additional engineering.

\paragraph{Expected Repair Impact.}
Based on our kernel-level validation (\S\ref{sec:eval}.3), we estimate that integrating alignment constraints into PaLU could yield:
\begin{itemize}
  \item \textbf{Prefill}: +15--25\% speedup (compute-bound, benefits from Tensor Core alignment)
  \item \textbf{Decode}: +5--10\% speedup (memory-bound, limited benefit from alignment)
\end{itemize}
Two approaches: (1) constrained SVD with aligned rank, or (2) post-compression $U$ padding.

\subsection{Accuracy Preservation}

Zero-padding guarantees \textbf{bit-exact output preservation}.
We provide both theoretical and empirical evidence:

\paragraph{Theoretical Guarantee (Forward-Pass Equivalence).}
For a linear layer $y = Wx + b$ with $W \in \mathbb{R}^{d_{out} \times d_{in}}$, the padded layer uses $W' \in \mathbb{R}^{d'_{out} \times d_{in}}$ where $W'[0:d_{out}, :] = W$ and $W'[d_{out}:d'_{out}, :] = \mathbf{0}$.
The output $y' = W'x + b'$ satisfies:
\begin{equation}
y'[0:d_{out}] = W'[0:d_{out}, :] \cdot x + b'[0:d_{out}] = Wx + b = y
\end{equation}
Thus the original output is exactly preserved in the first $d_{out}$ positions.
For attention mechanisms, zero-valued key/query dimensions contribute zero to attention scores ($q \cdot k^T$), making padding semantically neutral.
This mathematical equivalence guarantees \textbf{no accuracy degradation}---no retraining or fine-tuning is required.
\textbf{Formal statement}: Let $f_{\theta}(x)$ denote the original model and $f_{\theta'}(x)$ the repaired model. For any input $x$, we have $f_{\theta}(x) = \text{slice}(f_{\theta'}(x), [0:d_{out}])$, ensuring functional equivalence.

\paragraph{Empirical Verification.}
We verify that repaired linear layers produce \textbf{identical outputs} (within floating-point precision) for the original dimensions (30/30 unit tests passed).

\paragraph{Limitations.}
Our evaluation has three main limitations:

\textbf{(1) Accuracy validation scope (Primary Gap):} While unit tests confirm bit-exact preservation (30/30 passed) and our theoretical analysis proves forward-pass equivalence, we have not conducted comprehensive perplexity evaluation (e.g., WikiText-2) or downstream task assessment (e.g., HellaSwag, PIQA).
\emph{Why we expect zero degradation:} Zero-padding adds only $\mathbf{0}$ entries that contribute nothing to matrix multiplications or attention scores. The mathematical proof (Equation~3) guarantees $f_{\theta}(x) = \text{slice}(f_{\theta'}(x))$ for \emph{any} input---this is deterministic, not probabilistic.
\emph{What perplexity would add:} Large-scale evaluation would demonstrate this guarantee holds across diverse input distributions at full model scale, ruling out any unforeseen numerical edge cases.
This validation is our primary identified gap and highest-priority future work.

\textbf{(2) End-to-end repair integration:} The kernel-level speedups (25--30\%) are validated on SDPA and GEMM microbenchmarks but not yet integrated into PaLU's SVD decomposition.
PaLU represents $W_{kv} = U \cdot V^T$ where irregular dimensions reside in $U$ matrices; adapting our repair pass requires additional engineering to modify $U$ while preserving the factorization structure.

\textbf{(3) Estimated repair impact:} The expected prefill (+15--25\%) and decode (+5--10\%) improvements in \S\ref{sec:eval}.4 are extrapolated from kernel experiments, not measured on complete models.

%% ===========================================
%% 7. RELATED WORK
%% ===========================================
\section{Related Work}
\label{sec:related}

\paragraph{LLM Compression.}
Post-training compression for LLMs has attracted significant attention.
SparseGPT~\cite{sparsegpt} achieves one-shot pruning with minimal accuracy loss.
GPTQ~\cite{gptq} and AWQ~\cite{awq} enable 4-bit quantization while preserving model quality.
Low-rank decomposition methods like PaLU~\cite{palu} compress KV projections via SVD.
These methods optimize for accuracy-compression trade-offs but largely ignore hardware alignment constraints.
Our work reveals that this oversight causes significant performance penalties (30--58\% slowdown).

\paragraph{KV Cache Optimization.}
MQA~\cite{mqa} and GQA~\cite{gqa} reduce KV cache by sharing heads.
StreamingLLM~\cite{streaminglm} maintains fixed-size caches for infinite-length generation.
These approaches modify attention but preserve standard head dimensions.
In contrast, SVD-based compression like PaLU produces irregular per-head dimensions that violate GPU alignment.

\paragraph{GPU Kernel Optimization.}
FlashAttention~\cite{flashattention,flashattention2} revolutionized attention computation with IO-aware algorithms.
However, FlashAttention's optimized kernels are tuned for specific head dimensions ($\{32, 64, 96, 128, 256\}$), with performance cliffs for other values.
CUTLASS~\cite{cutlass} provides templated GEMM kernels that select tile sizes based on dimension alignment.
Tensor Core utilization depends heavily on K-dimension alignment ($K \mod 16 = 0$).
Our Shape Contract formalizes these implicit requirements, enabling compression-aware kernel selection.

\paragraph{Inference Optimization Frameworks.}
TensorRT~\cite{tensorrt} applies layer fusion and kernel auto-tuning, including implicit padding for misaligned dimensions.
However, TensorRT's runtime padding differs from our approach:
(1) it is \emph{opaque}---users cannot control the strategy;
(2) it may not recognize irregular dimensions from compression;
(3) runtime padding incurs per-inference overhead, whereas compile-time repair enables better kernels.
vLLM~\cite{vllm} and TGI optimize LLM serving but assume aligned dimensions.
Our Shape Contract makes alignment requirements \emph{explicit}.

\paragraph{Positioning of This Work.}
Unlike prior work that focuses on accuracy-compression trade-offs, \textbf{we focus on performance-alignment trade-offs}.
We identify dimensional collapse as a critical but overlooked problem: \textbf{compressed models may have fewer FLOPs yet run slower} due to hardware misalignment.
Our contribution complements compression research by providing Shape Contracts and dimension repair to recover lost GPU efficiency.

%% ===========================================
%% 8. CONCLUSION
%% ===========================================
\section{Conclusion}
\label{sec:conclusion}

We identified dimensional collapse as a critical but overlooked problem in LLM compression.
Through systematic analysis, we traced the root causes across software and hardware layers:
\begin{itemize}
  \item FlashAttention's internal slow path for non-8-aligned dimensions (+30--45\%)
  \item Tensor Core tile alignment: K\%16 requirement causes 58\% slowdown when violated
  \item Vectorized load degradation: float4$\rightarrow$scalar fallback causes 50\% throughput loss
  \item SDPA bandwidth efficiency: 40\% loss for non-8-aligned head dimensions
  \item L2 cache sector waste: only 5.8\%, \emph{not} a significant factor
\end{itemize}

Notably, PyTorch backend selection does \emph{not} cause fallback for most irregular dimensions---the performance penalty occurs within FlashAttention itself.

Our Shape Contract and dimension repair approach provide a practical solution:
\begin{itemize}
  \item MINIMAL strategy: 3.72\% memory overhead for 25--28\% \textbf{kernel-level} speedup (6.9$\times$ ROI)
  \item OPTIMAL strategy: 7.20\% memory overhead for 27--30\% \textbf{kernel-level} speedup (4.0$\times$ ROI)
  \item 96.9\% of PaLU dimensions benefit from repair
  \item Simple post-compression pass, no model retraining required
\end{itemize}

\textbf{Note on end-to-end validation:} Kernel-level speedups (25--30\%) are validated on SDPA and GEMM microbenchmarks.
Integrating dimension repair into PaLU's SVD decomposition ($W = U \cdot V^T$) requires adapting to the factorized structure; this is left as future work.

End-to-end evaluation shows PaLU achieves 11.5$\times$ decode speedup despite dimension misalignment, demonstrating that compression benefits can outweigh alignment penalties in memory-bound phases.
However, combining compression with dimension repair promises further gains, particularly in compute-bound prefill (estimated +15--25\%).

We advocate for compression-aware kernel design and alignment-aware compression algorithms.

\paragraph{Future Work.}
We identify three directions for future research:
\begin{enumerate}
  \item \textbf{SVD Integration}: Adapting dimension repair to low-rank decomposition structures ($W = U \cdot V^T$), either through constrained SVD or post-compression $U$ padding.
  \item \textbf{Perplexity Validation}: Comprehensive accuracy evaluation (WikiText-2, downstream tasks) to empirically confirm our theoretical guarantee of zero accuracy degradation.
  \item \textbf{Hardware Generalization (H100+)}: Our A100 analysis reveals alignment requirements tied to mma.sync tile sizes (K\%16). On H100 and newer architectures:
  \begin{itemize}
    \item TMA (Tensor Memory Accelerator) imposes different granularity for global-to-shared memory transfers
    \item WGMMA instructions operate on 64$\times$64 warpgroup tiles, suggesting K\%64 may become optimal
    \item Different SM counts and memory hierarchy may shift the relative impact of our identified root causes
  \end{itemize}
  Preliminary profiling would validate whether our Shape Contract generalizes or requires architecture-specific tuning.
\end{enumerate}

%% ===========================================
%% REFERENCES
%% ===========================================
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
