%%
%% GAC: When Smaller Is Slower â€” Dimensional Misalignment in Compressed LLMs
%% Target: EuroMLSys 2026 (SIGPLAN format, 6 pages excluding references)
%%

\documentclass[sigplan,10pt,nonacm]{acmart}

%% Remove ACM-specific elements for submission
\settopmatter{printacmref=false,authorsperrow=5}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

%% Packages
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{positioning,decorations.pathreplacing,fit,backgrounds}
\usepackage{placeins}
\usepackage{colortbl}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\hfuzz=0pt  % Strict: report ALL overfull boxes

%% Compact spacing
\setlength{\textfloatsep}{8pt plus 2pt minus 2pt}
\setlength{\floatsep}{8pt plus 2pt minus 2pt}
\setlength{\intextsep}{6pt plus 2pt minus 2pt}
\setlength{\abovecaptionskip}{4pt}
\setlength{\belowcaptionskip}{2pt}

%% Colors matching slides
\definecolor{cblue}{RGB}{55,131,187}
\definecolor{cred}{RGB}{211,63,73}
\definecolor{cgreen}{RGB}{56,158,92}
\definecolor{corange}{RGB}{230,159,0}

%% Title
\title{Why Smaller Is Slower?\\ Dimensional Misalignment in Compressed LLMs}

%% Authors
\author{Jihao Xin}
\affiliation{
  \institution{KAUST}
  \country{Saudi Arabia}
}

\author{Tian Lyu}
\affiliation{
  \institution{KAUST}
  \country{Saudi Arabia}
}

\author{Qilong Pan}
\affiliation{
  \institution{HUMAIN AI}
  \country{Saudi Arabia}
}

\author{Kesen Wang}
\affiliation{
  \institution{HUMAIN AI}
  \country{Saudi Arabia}
}

\author{Marco Canini}
\affiliation{
  \institution{KAUST}
  \country{Saudi Arabia}
}

\begin{abstract}
Post-training compression reduces LLM parameter counts but often produces irregular tensor dimensions that degrade GPU performance---a phenomenon we call \emph{dimensional misalignment}.
We present a full-stack analysis of this effect, identifying root causes at three levels: framework (PyTorch backend dispatch), library (cuBLAS kernel selection), and hardware (TensorCore and memory alignment).
We propose \textbf{GAC} (GPU-Aligned Compression), a general framework that wraps any compressor and selects GPU-Aligned dimensions via multi-choice knapsack optimization.
On Llama-3-8B with two representative compressors---ASVD (SVD factorization) and LLM-Pruner (structured pruning)---GAC achieves 100\% dimension alignment with up to 1.5$\times$ speedup while preserving model quality.
\end{abstract}

\keywords{LLM Compression, GPU Optimization}

\begin{document}

\maketitle


%% ===========================================
%% 1. INTRODUCTION
%% ===========================================
\section{Introduction}
\label{sec:intro}

Large Language Models (LLMs) achieve strong capabilities but their scale poses deployment challenges~\cite{llama3}.
Post-training compression reduces model size which can be categorized into three categories: \emph{quantization} (reduced precision)~\cite{gptq,awq}, \emph{sparsification} (zeroing weights)~\cite{sparsegpt,wanda}, and \emph{dimension reduction} (low-rank factorization, structured pruning, KV cache eviction)~\cite{asvd,palu,llmpruner,h2o,pyramidkv}.
We focus on \textbf{dimension reduction}: it alters the tensor shapes that feed the dominant inference operators, which often produce \emph{irregular} dimensions (e.g., reduced rank from 128 to 107), which is inefficient in GPU's execution, and leads to a counterintuitive outcome: \emph{compressed models with \textbf{smaller} parameters can run \textbf{slower} than uncompressed models}. We term this \textbf{dimensional misalignment}.
We summarize three classes of compression algorithms that lead to dimensional misalignment: \textbf{(i)}~low-rank factorization (decomposing weight matrices into two smaller matrices $W \approx A \cdot B$, truncating inner dimension),~\cite{asvd,palu,svdllm2024}; \textbf{(ii)}~structured pruning (removing neurons or heads, reducing output dimension),~\cite{llmpruner,sparsegpt,wanda}; \textbf{(iii)}~token eviction (dropping or compressing keys/values, reducing effective sequence length),~\cite{h2o,pyramidkv}.

Existing compression methods predominantly trade off \emph{size and accuracy}: given a compression ratio, they maximize preserved accuracy while ignoring how chosen dimensions interact with hardware.
We propose \textbf{GAC} (GPU-Aligned Compression), a new compression \emph{paradigm} that imposes hardware alignment constraints on top of existing dimension-reducing compressors: it wraps any such compressor and selects hardware-aligned dimensions under the same parameter budget, restoring speed without changing the compressor's accuracy-oriented design.

\noindent \textbf{Contributions.}\\
\textbf{(1)}~We identify the \emph{dimensional misalignment} problem (\S\ref{sec:misalignment}).\\
\textbf{(2)}~We conduct a systematic full-stack analysis (\S\ref{sec:analysis}).\\
\textbf{(3)}~GAC formalizes the dimension selection as a constrained optimization with a dynamic programming solver (\S\ref{sec:gac}).\\
\textbf{(4)}~Preliminary evaluation on Llama-3-8B shows up to 1.5$\times$ speedup (\S\ref{sec:eval}).


%% ===========================================
%% 2. DIMENSIONAL MISALIGNMENT
%% ===========================================
\section{Motivation: Dimensional Misalignment}
\label{sec:misalignment}
\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/scatter_1x4_meta_llama_3_8b_instruct_r0.8.pdf}
  \caption{Llama-3-8B at $\rho=20\%$. Shape: $\circ{=}$Q Head, $\square{=}$K Head, $\triangle{=}$V Head; color: \textcolor{cgreen}{green}=8-aligned, \textcolor{cred}{red}=misaligned.}
  \label{fig:dim_scatter}
  \vspace{-0.3cm}
  \end{figure*}


Existing LLM compression aims to preserve accuracy under a given compression ratio $\rho$.
Formally, given the set of pretrained parameters $\mathcal{W}$, we seek a compressed set $\mathcal{W}'$ that minimizes expected loss subject to the size constraint:
\begin{equation}
\min_{\mathcal{W}'} \; \mathbb{E}_{(x,y)\sim\mathcal{D}}[\mathcal{L}(\mathcal{W}'; x, y)] \quad \text{s.t.} \quad 1-|\mathcal{W}'|/|\mathcal{W}| \leq \rho.
\label{eq:compression_target}
\end{equation}
Here $\mathcal{D}$ is the data distribution, $\mathcal{L}$ is the loss, $|\mathcal{W}'|$ and $|\mathcal{W}|$ denote total parameter counts. We denote $B = (1-\rho)|\mathcal{W}|$ as the parameter budget.

Different parameters have different compression sensitivities, e.g., early and late layers are often more critical than middle layers---so budget cannot be allocated uniformly.
Existing methods proceed in two steps.
First, for each parameter $W_i$, compute an \textbf{importance score} $s_i$ using a proxy (Table~\ref{tab:importance_scores}); a higher $s_i$ reflects higher sensitivity thus should retain more parameters in $W_i$.
Second, allocate the budget $B$ by assigning each $W_i$ a dimension $d_i$ thus that more important $W_i$ receive larger $d_i$:
\begin{equation}
\{d_i\} = \arg\max_{\{d_i\}} \sum_i s_i \cdot |W_i| \quad \text{s.t.} \quad |\mathcal{W}'| \leq B, \quad d_i \geq 0.
\label{eq:budget_allocation}
\end{equation}
Here $d_i$ is the compressed dimension (e.g., inner rank or output width, varies by compression method), $|W_i|$ is the parameter count given dimension $d_i$, $|\mathcal{W}'| = \sum_i |W_i|$ is the total parameter count of the compressed model, and $B$ is the parameter budget.
Because $s_i$ and the optimum $\{d_i\}$ are continuous, the resulting dimensions are typically \emph{irregular} (e.g., 107, 108, 109) and often violate GPU alignment requirements (e.g.\ $d \bmod 8 = 0$), triggering backend fallbacks and kernel switches that erase the expected speedup from fewer parameters and FLOPs.

We demonstrate the dimensional misalignment problem with a real-world example: PaLU~\cite{palu}.
We use \textbf{8-alignment} ($d \bmod 8 = 0$) as an example of alignment constraints. When dimensions are not 8-aligned, latency can increase by up to 90\% (Figure~\ref{fig:sdpa_latency}); we detail this analysis in \S\ref{sec:analysis}.
Figure~\ref{fig:palu_dist} shows that a large fraction of layers end up misaligned (e.g., 78\% in this setup).
For generality, we summarize the mainstream importance score proxies into four categories (Table~\ref{tab:importance_scores}).
We empirically measure unconstrained dimension allocation on Llama-3-8B at $\rho=20\%$ with all four proxies, Figure~\ref{fig:dim_scatter} demonstrates the dimensional misalignment consistently occurs across different methods.
\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig3_palu_dist.png}
\caption{Llama-3-8B after PaLU compression at $\rho=20\%$.}
\label{fig:palu_dist}
\end{figure}

\begin{table}[t]
\centering
\caption{Mainstream importance score for budget allocation.}
\label{tab:importance_scores}
\small
\setlength{\tabcolsep}{2.5pt}
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Method} & \textbf{Score} & \textbf{Intuition} & \textbf{Works} \\
\midrule
Magnitude & $\|W_i\|_F$ & Weight norm & SVD-LLM~\cite{svdllm2024} \\
Activation & $\|X_i\|_F$ & Input mag.\  & ASVD~\cite{asvd} \\
Gradient & $\left|\frac{\partial \mathcal{L}}{\partial h_i} \cdot h_i\right|$ & Taylor exp.\  & \cite{taylor_pruning} \\
Fisher & $\mathrm{tr}(\mathbf{F}_i)$ & Loss curvature & PaLU \cite{palu} \\
\bottomrule
\end{tabular}
\end{table}


%% ===========================================
%% 3. FULL-STACK ANALYSIS
%% ===========================================

\section{Full-Stack Analysis}
\label{sec:analysis}
We analyze where and why dimensional misalignment causes slowdowns on an NVIDIA A100-80GB with PyTorch 2.9.1, CUDA 12.8, FP16.
Latency is measured with CUDA events (50 warmup, 200 measurement iterations, 3 trials).
Root causes fall into three layers (Figure~\ref{fig:fullstack_overview}): Framework, Library, and Hardware. We detail each layer in the following subsections.
\begin{figure}[t]
  \centering
  \begin{tikzpicture}[
      box/.style={draw=gray!60, rounded corners=5pt, minimum width=2.0cm, minimum height=0.8cm,
                  line width=0.8pt, font=\sffamily\footnotesize\bfseries, align=center, inner sep=4pt},
      >=stealth, arrow/.style={->, line width=1.2pt, gray!55},
      lbl/.style={font=\sffamily\scriptsize, text=gray!40!black, above=1pt}
    ]
    \node[box, fill=cblue!18] (fw) at (0,0) {Framework};
    \node[box, fill=corange!18] (lib) at (3.0,0) {Library};
    \node[box, fill=cred!18] (hw) at (6.0,0) {Hardware};
    \draw[arrow] (fw) -- (lib) node[midway, lbl] {Dispatch};
    \draw[arrow] (lib) -- (hw) node[midway, lbl] {Execute};
  \end{tikzpicture}
  \caption{Hierarchy of the analysis' three layers.}
  \label{fig:fullstack_overview}
  \end{figure}

\subsection{Framework Layer}
\label{sec:framework}
Existing DL frameworks such as PyTorch and TensorFlow dispatch an operation to different backends.
For example, a matrix multiply \texttt{A@B} in PyTorch may run via cuBLAS or via a Triton kernel depending on the shape and hardware, where the dispatching mechanism is hidden from the user.
In this section, we exemplify this issue via PyTorch's \textbf{SDPA} (Scaled Dot-Product Attention), the core attention mechanism:
\begin{equation}
\mathrm{Attention}(Q,K,V) = \mathrm{softmax}(Q K^\top / \sqrt{d_k}) V
\end{equation}
where $Q$, $K$, $V$ are the query, key, and value matrices and $d_k$ is the per-head dimension.
When one calls the SDPA API \footnote{\texttt{torch.nn.functional.scaled\_dot\_product\_attention(Q, K, V)}} , PyTorch may select an optimized implementation (e.g., FlashAttention) or fall back to a naive eager implementation (the ``Math'' backend).

We measured SDPA latency with inputs $Q,K,V$ of shape $(B, S, H, d)$: batch $B{=}4$, sequence length $S{=}2048$, number of heads $H{=}32$, and we sweep the per-head dimension $d$ from 64 to 256 (full sweep details in Appendix~\ref{app:fa2_templates}).
We observed a staircase pattern (Figure~\ref{fig:sdpa_latency}).
First, multiples of 8 are faster: e.g., $d{=}129$ incurs $\sim$90\% higher latency than $d{=}128$.
Profiling shows that PyTorch uses FlashAttention only when $d \bmod 8 = 0$; otherwise it falls back to the Math kernel.
Second, among 8-aligned dimensions, $d \bmod 32 = 0$ forms another performance boundary: FlashAttention-2 uses dimension-specific templates, with tile sizes changing every 32 dimensions.
In the figure, alternating background shades denote template regions; labels such as ``$t{=}96$, $128{\times}64$'' indicate the template size $t$ and the $B_r {\times} B_c$ tile shape for that region.
\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig2_sdpa_latency.pdf}
  \caption{PyTorch SDPA latency across dimensions.}
  \label{fig:sdpa_latency}
  \end{figure}


\subsection{Library Layer}
\label{sec:library}

Linear algebra is dispatched to libraries (e.g., cuBLAS), where the same GEMM can be served by different kernels depending on dimensions.
We examine this via GEMM $C {=} A {\cdot} B$ with $A \in \mathbb{R}^{M \times K}$ and $B \in \mathbb{R}^{K \times N}$.
We measured GeMM latency with two of $(M, N, K)$ fixed at typical LLM sizes ($M{=}N{=}2048$, $K{=}128$) and the third dimension swept from 50\% to 100\%.
Figure~\ref{fig:gemm_alignment} shows the results.
First, $K$ and $N$ exhibit a clear alignment effect: when the swept dimension satisfies $d \bmod 8 = 0$, latency is lower (e.g., $K$ aligned $\sim$20\,$\mu$s vs.\ misaligned 22--26\,$\mu$s, up to 30\% penalty).
Second, $M$ and $N$ show \emph{kernel-switching cliffs}: at certain boundaries (e.g., $M{=}1728 \to 1729$), latency jumps (e.g., $\sim$30\%).
We profiled with Nsight Compute to explain this: when $d \bmod 8 = 0$, cuBLAS invokes its native optimized kernel; otherwise it uses a CUTLASS-generated kernel, which is further divided into align2 (fetching 2 elements at a time) or align1.
Table~\ref{tab:cublas_tiers} summarizes the three tiers.
GeMV ($M{=}1$) exhibits a similar but smaller penalty (${\sim}$12\% on $K$, ${\sim}$4\% on $N$; Appendix~\ref{app:gemv}), consistent with GeMV being memory-bound rather than compute-bound.
\begin{table}[t]
\centering
\caption{cuBLAS GEMM kernel tiers (Nsight Compute).}
\label{tab:cublas_tiers}
\small
\setlength{\tabcolsep}{2pt}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}clll@{}}
\toprule
\textbf{Tier} & \textbf{Condition} & \textbf{Kernel} & \textbf{MMA Instr.} \\
\midrule
\rowcolor{cgreen!10} 1 & $d \bmod 8 = 0$ & cuBLAS-native sm80 & \texttt{mma.m16n8k16} \\
\rowcolor{corange!10} 2 & $d \bmod 2 = 0$ & CUTLASS sm80 align2 & \texttt{mma.m16n8k16} \\
\rowcolor{cred!10} 3 & odd & CUTLASS sm75 align1 & \texttt{mma.m16n8k8} \\
\bottomrule
\end{tabular}%
}
\end{table}

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figures/fig_gemm_alignment.pdf}
\caption{GeMM latency with dimension sweep.}
\label{fig:gemm_alignment}
\end{figure*}

\subsection{Hardware Layer}
\label{sec:hardware}
Beyond framework and library dispatch, misaligned dimensions also cause inefficiency from the hardware level.
We use Nsight Compute profiling to isolate two mechanisms.
\textbf{(1)~Tensor Core}: The A100 MMA instruction \texttt{mma.m16n8k16} processes tiles of $16{\times}8{\times}16$ fp16 elements; dimensions not divisible by these tile sizes leave partial tiles underutilized.
A throughput sweep near $K,N{=}4096$ confirms: aligned dimensions reach 160--175 TFLOPS while misaligned ones drop to 50--110 TFLOPS, with period-16 in $K$ and period-8 in $N$ matching the tile shape (Figure~\ref{fig:hw_alignment}a,b).
\textbf{(2)~Memory}: The A100 L2 cache operates in 32-byte sectors; for FP16 this requires $K \bmod 16 = 0$ for full utilization.
Misaligned accesses show ${\sim}$2$\times$ bandwidth loss in microbenchmarks (Figure~\ref{fig:hw_alignment}c).

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figures/fig_hw_alignment.pdf}
\caption{Hardware-level alignment penalties (sweep near 4096): (a,b)~Tensor Core throughput, (c)~L2 cache bandwidth.}
\label{fig:hw_alignment}
\end{figure*}

\paragraph{Summary.} Table~\ref{tab:constraints} summarizes all constraints.
The minimum requirement across all layers is $d \bmod 8 = 0$; stricter alignment (mod~16, mod~32) yields further gains.
These penalties compound: a single misaligned dimension can trigger a backend fallback, a suboptimal kernel, and underutilized tiles simultaneously.

\begin{table}[t]
\centering
\caption{Full-stack alignment constraint summary.}
\label{tab:constraints}
\small
\setlength{\tabcolsep}{2.5pt}
\begin{tabular}{@{}lllr@{}}
\toprule
\textbf{Level} & \textbf{Mechanism} & \textbf{Constraint} & \textbf{Penalty} \\
\midrule
\rowcolor{cblue!8} Framework & SDPA backend & $d$\%$8{=}0$ & ${\sim}$90\% \\
\rowcolor{cblue!8} Framework & FA2 template & $d$\%$32{=}0$ & ${\sim}$30\% \\
\midrule
\rowcolor{corange!8} Library & cuBLAS GeMM & $K$/$N$\%$8{=}0$ & ${\sim}$90\% \\
\rowcolor{corange!8} Library & cuBLAS GeMV & $K$\%$8{=}0$ & ${\sim}$12\% \\
\midrule
\rowcolor{cred!8} Hardware & TC MMA & $K$\%16, $N$\%8 & ${\sim}$70\% \\
\rowcolor{cred!8} Hardware & L2 sectors & $K$\%$16{=}0$ & ${\sim}$50\% \\
\bottomrule
\end{tabular}
\end{table}

%% ===========================================
%% 4. GAC: ALIGNMENT-AWARE COMPRESSION
%% ===========================================
\section{GAC: GPU-Aligned Compression}
\label{sec:gac}

\input{figures/fig_gac_framework.tex}

To bridge the gap between compression and alignment, we propose \textbf{GAC} (GPU-Aligned Compression), a paradigm that makes budget allocation system-aware so that fewer parameters translate into real speedup.
GAC wraps any dimension-reducing compressor as a post-processing pass, re-selecting dimensions to satisfy alignment constraint.
Given an LLM + a compressor +  a compression budget, the pipeline produces a 100\%-aligned compressed model in three steps (Figure~\ref{fig:gac_framework}):\\
\textbf{(1)~Unconstrained Compression}---Compress the model with misaligned dimensions $\{d_i^*\}$ and importance scores $\{s_i\}$.\\
\textbf{(2)~Dimension Sweep}---For each misaligned weight, sweep near $d_i^*$ and retain only hardware-friendly candidates.\\
\textbf{(3)~Constrained Optimization}---Formulate a knapsack problem and solve via dynamic programming (DP).

\subsection{Problem Formulation}

Given $n$ projections with importance scores $\{s_i\}$, ideal (unconstrained) dimensions $\{d_i^*\}$, and total parameter budget $B$, we seek aligned dimensions $\{d_i\}$ that maximize importance-weighted information preservation:
\begin{equation}
\max_{\{d_i\}} \sum_{i=1}^{n} s_i \cdot (d_i - d_i^*) \;\;\text{s.t.}\;\; \sum_{i} d_i \cdot g_i \leq B,\;\; d_i \in C_i
\label{eq:gac}
\end{equation}
where $g_i$ is the group size for projection $i$ and $C_i$ is the set of hardware-friendly candidates.
The objective is \emph{asymmetric}: rounding up ($d_i > d_i^*$) yields positive value (information preserved), while rounding down yields negative value (information lost), weighted by each layer's importance score.
High-importance (sensitive) layers naturally receive larger dimensions; low-importance (insensitive) layers absorb the cost.

\subsection{Candidate Generation via Dimension Sweep}

Rather than hard-coding alignment to fixed multiples (e.g., mod~8 or mod~32), GAC generates candidates empirically.
For each projection, we profile GeMM and SDPA latency at dimensions near $d_i^*$ and retain only those that avoid performance cliffs.
This produces $|C_i| \approx 11$--$17$ candidates per projection.
Cliff dimensions---where cuBLAS switches kernel tier or SDPA falls back---are automatically excluded.
For example, given an ideal dimension $d^*{=}107.3$, the sweep produces $C{=}\{96, 104, 112, 128\}$, excluding 107 because it triggers a cuBLAS performance cliff; the DP then picks $d{=}104$, saving budget for more sensitive layers.
Because the sweep is hardware-specific, GAC adapts to different GPU architectures without manual tuning.

\subsection{Multi-Choice Knapsack DP}

We solve Eq.~\ref{eq:gac} via dynamic programming.
For each candidate $d_{ij} \in C_i$, define value $v_{ij} = s_i \cdot (d_{ij} - d_i^*)$ and weight $w_{ij} = d_{ij} \cdot g_i$.
The DP recurrence is:
\[
D[i][b] = \max_{j} \left\{ D[i{-}1][b - w_{ij}] + v_{ij} \right\}
\]
Complexity: $O(n \cdot |C| \cdot B')$ where $B' = B/(a \cdot g)$ is the quantized budget ($a$: alignment granularity, $g$: group size).
In practice, with $n{=}224$ projections and $|C| \approx 15$, the DP runs in under one second on CPU.

Algorithm~\ref{alg:gac} gives the complete procedure.

\begin{algorithm}[t]
\caption{GAC algorithm ($<$1\,s on CPU for $n{=}224$).}
\label{alg:gac}
\small
\begin{algorithmic}[1]
\Require Pre-trained LLM, compressor, budget $B$
\Ensure Aligned dimensions $\{d_i\}_{i=1}^n$ with $d_i \in C_i$
\Statex \textit{Phase 1: Unconstrained Compression}
\State Run compressor $\to$ ideal dims $\{d_i^*\}$, scores $\{s_i\}$
\State Profile GeMM/SDPA $\to$ constraints (\S\ref{sec:analysis})
\Statex \textit{Phase 2: Candidate Generation}
\For{each projection $i = 1, \ldots, n$}
  \State Sweep dims near $d_i^*$; measure latency
  \State $C_i \gets$ dims avoiding perf.\ cliffs
\EndFor
\Statex \textit{Phase 3: Multi-Choice Knapsack DP}
\State $B' \gets B / (a \cdot g)$
\State $D[0..n][0..B'] \gets -\infty$;\; $D[0][0] \gets 0$
\For{$i = 1$ to $n$}
  \For{each $d_{ij} \in C_i$}
    \State $v_{ij} \gets s_i \cdot (d_{ij} {-} d_i^*)$;\; $w_{ij} \gets d_{ij} \cdot g_i$
    \For{$b = w_{ij}$ to $B'$}
      \State $D[i][b] \gets \max\!\bigl(D[i][b],\; D[i{-}1][b{-}w_{ij}] + v_{ij}\bigr)$
    \EndFor
  \EndFor
\EndFor
\State Backtrack from $\arg\max_b D[n][b]$
\State \Return $\{d_i\}$
\end{algorithmic}
\end{algorithm}


%% ===========================================
%% 5. EVALUATION
%% ===========================================
\section{Evaluation}
\label{sec:eval}

\subsection{Setup}

\paragraph{Model and hardware.}
Llama-3-8B~\cite{llama3} (8.03B parameters) on NVIDIA A100-80GB, PyTorch 2.9.1, CUDA 12.8, FP16.
All latency is prefill (compute-bound GeMM); we focus on prefill because it is where alignment most affects performance.

\paragraph{Compression methods.}
We evaluate two representative methods that change tensor dimensions in orthogonal ways:
\textbf{(1)~ASVD}~\cite{asvd}: activation-aware SVD across all $n{=}224$ projections ($W {\to} A {\cdot} B$), rank allocated by PPL-calibrated sensitivity; 15\% compression.
\textbf{(2)~LLM-Pruner}~\cite{llmpruner}: coupled structured pruning of MLP intermediate dimensions across 28/32 layers; 15\% compression.

\paragraph{Strategies.}
For each compressor: \emph{Unaligned} (default output with irregular dimensions) and \emph{GAC} (dimensions aligned, $d \bmod 8 = 0$).

\paragraph{Metrics.}
Alignment (\% of projections where $d \bmod 8 = 0$), perplexity (WikiText-2, 2048-token blocks), accuracy (average of PIQA and HellaSwag zero-shot), and prefill latency ($S{=}1024$, batch${=}1$, 30 measurements after warmup).

\subsection{Main Results}

Table~\ref{tab:main_results} presents the end-to-end comparison.

\begin{table}[t]
\centering
\caption{Main results on Llama-3-8B (15\% compression, A100-80GB, $S{=}1024$).}
\label{tab:main_results}
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{@{}lcrcc@{}}
\toprule
\textbf{Method} & \textbf{Align} & \textbf{PPL}$\downarrow$ & \textbf{Acc} & \textbf{Prefill (ms)} \\
\midrule
\rowcolor{gray!10} Baseline & 100\% & 6.14 & 0.72 & 99.6 \\
\midrule
ASVD (Unaln.) & 5\% & 34.7 & 0.38 & 100.5\,{\scriptsize\textcolor{cred}{+1\%}} \\
ASVD (GAC) & 100\% & \textbf{31.3} & 0.39 & \textbf{67.1}\,{\scriptsize\textcolor{cgreen}{$-$33\%}} \\
\midrule
Pruner (Unaln.) & 83\% & 9.88 & 0.67 & 137.7\,{\scriptsize\textcolor{cred}{+38\%}} \\
Pruner (GAC) & 100\% & \textbf{9.87} & 0.67 & \textbf{88.0}\,{\scriptsize\textcolor{cgreen}{$-$12\%}} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{ASVD: SVD factorization.}
The unconstrained allocation produces 95\% misaligned dimensions.
Despite reducing parameters by 15\%, the misaligned model shows \emph{no prefill speedup} (100.5\,ms vs 99.6\,ms baseline)---the compression benefit is entirely consumed by alignment overhead.
GAC restores alignment to 100\%, yielding a 1.5$\times$ prefill speedup (67.1\,ms, $-$33\% vs baseline).
Notably, GAC also improves perplexity (31.3 vs 34.7) because the DP rounds sensitive layers \emph{up}, preserving information where it matters most.

\paragraph{LLM-Pruner: structured pruning.}
The pruned model retains 83\% alignment (most MLP dimensions happen to be near multiples of 8), yet still suffers a +38\% prefill penalty---demonstrating that even partial misalignment triggers significant overhead.
GAC eliminates the penalty entirely, achieving a net 12\% speedup over the uncompressed baseline.
Quality is preserved: PPL and accuracy are nearly identical between Unaligned and GAC variants.

\paragraph{Rank allocation visualization.}
Figure~\ref{fig:gac_ranks} shows per-layer dimension allocations for $W_K$ and $W_V$ projections under three strategies.
For $W_K$ (top), GAC DP deviates from both Unaligned and Round-to-8: it rounds \emph{up} for high-importance layers (e.g., layers 14--16) and rounds down for insensitive ones, exploiting the asymmetric objective.
For $W_V$ (bottom), most layers retain the full rank (512), with GAC maintaining alignment at the few compressed layers (18--26).

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_gac_ranks.pdf}
\caption{Per-layer rank allocation for $W_K$ (top) and $W_V$ (bottom).}
\label{fig:gac_ranks}
\end{figure}

\subsection{Prefill Latency Scaling}

The alignment penalty grows with sequence length.
Figure~\ref{fig:prefill_scaling} shows LLM-Pruner prefill latency at four sequence lengths for the three configurations.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_prefill_scaling.pdf}
\caption{LLM-Pruner prefill latency across sequence lengths.}
\label{fig:prefill_scaling}
\end{figure}

The penalty grows from +19\% ($S{=}128$) to +38\% ($S{=}1024$): longer sequences push GeMMs deeper into the compute-bound regime, where Tensor Core utilization---and thus alignment (\S\ref{sec:hardware})---dominates.
At $S{=}128$ the model is still partially memory-bound, hiding some misalignment cost; by $S{=}1024$ the penalty is fully exposed.
GAC maintains ${\sim}$12\% speedup over the \emph{uncompressed} baseline for $S \geq 256$, showing that aligned compression delivers both smaller models and faster inference.

\subsection{Discussion}

\paragraph{Why compression does not always speed up inference.}
ASVD decomposes each weight $W$ into two smaller matrices $A$ and $B$, halving FLOPs per projection but doubling the number of GeMM calls.
When dimensions are misaligned, each GeMM triggers Tier~2/3 kernels (Table~\ref{tab:cublas_tiers}) with lower throughput, negating the FLOP reduction entirely---hence the +1\% ``speedup'' of unaligned ASVD.
LLM-Pruner reduces MLP intermediate dimensions but produces sizes like 5931, 6054, and 6778, which fall outside cuBLAS's optimal kernel range.
Even the 83\% naturally-aligned dimensions cannot prevent the +38\% penalty because the remaining 17\% misaligned layers dominate the critical path.

\paragraph{GAC as a general paradigm.}
GAC operates at \emph{compression time}, requiring no runtime overhead, no architecture changes, and no extra memory---the DP runs in $<$1\,s on CPU.
The dimension sweep is hardware-specific: on A100, it identifies cliffs at SDPA template boundaries and cuBLAS kernel transitions; on H100 with TMA and FA3~\cite{flashattention3}, the cliffs shift accordingly.
For compression libraries, GAC adds negligible computation as a post-processing pass; for serving systems, it eliminates the need for runtime padding (TensorRT~\cite{tensorrt}) or dimension rejection (vLLM~\cite{vllm}).


%% ===========================================
%% 6. RELATED WORK AND DISCUSSION
%% ===========================================
\section{Related Work and Discussion}
\label{sec:related}

\textbf{LLM compression.}
SVD-based methods~\cite{asvd,svdllm2024,palu,fwsvd2022,gfwsvd2025} allocate per-layer dimensions via importance scores, producing irregular dimensions as a by-product.
Structured pruning~\cite{sparsegpt,wanda,llmpruner} and KV cache compression~\cite{h2o,quest,pyramidkv} similarly alter tensor shapes without considering hardware alignment.
Quantization~\cite{gptq,awq} preserves dimensions via fixed-width groups, naturally avoiding misalignment.
All dimension-altering compressors are \emph{hardware-agnostic}: they optimize accuracy under a size budget but ignore how chosen dimensions interact with the GPU execution stack (\S\ref{sec:analysis}).
GAC is complementary---it post-processes any such compressor's output to restore alignment.

\textbf{Hardware-aware optimization.}
HALP~\cite{halp2021} formulates CNN pruning as latency-budgeted optimization using end-to-end timing; HALOC~\cite{haloc2023} uses a differentiable latency predictor for CNN low-rank compression.
Both treat latency as a \emph{black-box} signal---they optimize aggregate runtime without isolating \emph{why} certain dimensions are slow, are tied to specific architectures, and offer no parameter-budget guarantee.
GAC instead identifies root causes (framework dispatch, kernel selection, and hardware tile alignment; \S\ref{sec:analysis}) and constrains the search space directly, guaranteeing both alignment and the parameter budget.

\textbf{Serving-side mitigations.}
Serving systems handle misalignment \emph{reactively}.
FlashAttention-2 pads to the next template (${\sim}$30\% overhead); vLLM~\cite{vllm} rejects unsupported head dimensions; TensorRT~\cite{tensorrt} pads at runtime.
These add overhead but cannot change the model.
GAC prevents misalignment at \emph{compression time}, eliminating runtime workarounds.

\textbf{Limitations and future work.}
Our evaluation covers two compressors, one model (Llama-3-8B), and one GPU (A100).
Alignment constraints grow stricter across GPU generations~\cite{nvidia_tensor_core_evolution2024}: Hopper introduces TMA with 128-byte transfers~\cite{nvidia_hopper_whitepaper}, and FlashAttention-3~\cite{flashattention3} drops support for \texttt{head\_dim} 96 and 112---suggesting that misalignment penalties will only increase on newer hardware.
Integrating GAC directly into compression libraries, extending to composed techniques (e.g., SVD + quantization), and adding a lightweight post-GAC finetuning pass are natural next steps.


%% ===========================================
%% REFERENCES
%% ===========================================
\clearpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}


%% ===========================================
%% APPENDIX
%% ===========================================
\appendix

\section{FA2 Template Tiers and Dimension Sweep}
\label{app:fa2_templates}

We sweep \texttt{head\_dim} from 64 to 256 with $B{=}4$, $S{=}2048$, $H{=}32$.
FlashAttention-2 selects the smallest template $t \geq d$; tile width $B_c$ halves at boundaries (e.g., $128 \to 64 \to 32$), so crossing $d{=}128 \to 129$ causes $\sim$90\% latency increase.
Non-8-aligned dimensions trigger the MATH fallback (e.g., $d{=}107$ incurs 2.14\,ms, +40\% vs $d{=}112$ at 1.53\,ms on Flash).
Table~\ref{tab:fa2_templates} quantifies the tiers.

\begin{table}[h]
\centering
\caption{FA2 template tiers and performance ($B{=}4$, $S{=}2048$, $H{=}32$).}
\label{tab:fa2_templates}
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{@{}llrrr@{}}
\toprule
Region & Template & $B_r \times B_c$ & Latency & vs.\ $t{=}64$ \\
\midrule
$d{=}64$ & 64 & 128$\times$128 & 0.74\,ms & 1.0$\times$ \\
$d \in (64,96]$ & 96 & 128$\times$64 & 1.12\,ms & 1.5$\times$ \\
$d \in (96,128]$ & 128 & 128$\times$64 & 1.47\,ms & 2.0$\times$ \\
$d \in (128,160]$ & 160 & 128$\times$32 & 2.00\,ms & 2.7$\times$ \\
$d \in (160,256]$ & 192--256 & 128$\times$32 & 2.3--2.9\,ms & 3--4$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\section{GeMV Alignment Sweep}
\label{app:gemv}

Figure~\ref{fig:gemv_alignment} shows GeMV latency ($M{=}1$) with stride-1 sweeps near 4096 for $K$ and $N$.
The alignment penalty is smaller than GeMM (${\sim}$12\% on $K$, ${\sim}$4\% on $N$), consistent with GeMV being memory-bound.

\begin{figure*}[h]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig_gemv_alignment.pdf}
\caption{GeMV latency ($M{=}1$, stride-1 sweep near 4096).}
\label{fig:gemv_alignment}
\end{figure*}

\section{Dimension Distribution of Evaluated Methods}
\label{app:dims}

Figure~\ref{fig:dim_distribution} shows per-layer dimension distributions for ASVD (224 projections) and LLM-Pruner (28 pruned MLP layers).
ASVD dimensions range from 300 to 3,185 with 0\% initial alignment; LLM-Pruner from 5,931 to 14,336 with 83\% alignment.

\begin{figure*}[h]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig_dim_distribution.pdf}
\caption{Per-layer dimension distributions for ASVD (top) and LLM-Pruner (bottom).}
\label{fig:dim_distribution}
\end{figure*}


\section{SVD Rank Distribution Across Compression Ratios}
\label{app:scatter_ratios}

The misalignment problem persists across different compression ratios.
Figure~\ref{fig:scatter_ratios} shows dimension scatter plots for Llama-3-8B under unconstrained SVD allocation at five compression levels ($\rho=0.5$, 0.6, 0.7, 0.8, 0.9) using Fisher importance scores.
At every ratio, a substantial fraction (40--80\%) of dimensions are misaligned, confirming that dimensional misalignment is inherent to importance-based rank allocation, not an artifact of aggressive compression.

\begin{figure*}[h]
\centering
\begin{subfigure}[t]{\textwidth}
\includegraphics[width=\textwidth]{figures/scatter_1x4_meta_llama_3_8b_instruct_r0.5.pdf}
\caption{Llama-3-8B, $r$=0.5 (50\% compression)}
\end{subfigure}

\vspace{4pt}
\begin{subfigure}[t]{\textwidth}
\includegraphics[width=\textwidth]{figures/scatter_1x4_meta_llama_3_8b_instruct_r0.7.pdf}
\caption{Llama-3-8B, $r$=0.7 (30\% compression)}
\end{subfigure}

\vspace{4pt}
\begin{subfigure}[t]{\textwidth}
\includegraphics[width=\textwidth]{figures/scatter_1x4_meta_llama_3_8b_instruct_r0.9.pdf}
\caption{Llama-3-8B, $r$=0.9 (10\% compression)}
\end{subfigure}

\vspace{4pt}
\begin{subfigure}[t]{\textwidth}
\includegraphics[width=\textwidth]{figures/scatter_1x4_mistral_7b_v0_3_r0.8.pdf}
\caption{Mistral-7B, $\rho=0.8$ (cross-model)}
\end{subfigure}
\caption{SVD rank distributions across compression ratios and models.}
\label{fig:scatter_ratios}
\end{figure*}

\end{document}
