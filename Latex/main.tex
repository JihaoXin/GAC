%%
%% G-Compress: Dimensional Collapse in Compressed LLMs
%% Target: EuroMLSys (SIGPLAN format, 6 pages excluding references)
%%

\documentclass[sigplan,10pt,nonacm]{acmart}

%% Remove ACM-specific elements for submission
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

%% Packages
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{tikz}

%% Code listing style
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  numbers=left,
  numberstyle=\tiny,
}

%% Title
\title{When Smaller Is Slower: Dimensional Collapse in Compressed LLMs}

%% Authors
\author{Jihao Xin}
\affiliation{
  \institution{KAUST}
  \city{Thuwal}
  \country{Saudi Arabia}
}
\email{jihao.xin@kaust.edu.sa}

\author{Tian Lvy}
\affiliation{
  \institution{KAUST}
  \city{Thuwal}
  \country{Saudi Arabia}
}

\author{Qilong Pan}
\affiliation{
  \institution{HUMAIN AI}
  \city{Riyadh}
  \country{Saudi Arabia}
}

\author{Kesen Wang}
\affiliation{
  \institution{HUMAIN AI}
  \city{Riyadh}
  \country{Saudi Arabia}
}

\author{Marco Canini}
\affiliation{
  \institution{KAUST}
  \city{Thuwal}
  \country{Saudi Arabia}
}

\begin{abstract}
Post-training compression techniques for Large Language Models (LLMs) often result in irregular tensor dimensions (e.g., \texttt{head\_dim=107} instead of 128).
We identify a phenomenon called \emph{dimensional collapse}: despite reducing FLOPs, these irregular dimensions cause significant inference slowdowns on modern GPUs.
Our experiments on NVIDIA A100 show that \texttt{head\_dim=107} increases SDPA latency by 88\% compared to \texttt{head\_dim=96}.

We systematically investigate the root causes across three layers.
Contrary to initial assumptions, FlashAttention does \emph{not} fall back to slower backends---instead, it uses an internal slow path with 30--45\% overhead.
At the hardware level, we identify three confirmed root causes: Tensor Core tile alignment (58\% slowdown when K\%16$\neq$0), vectorized load degradation (50\% throughput loss when falling back to scalar loads), and SDPA bandwidth efficiency loss (40\%).
Notably, L2 cache sector waste (5.8\%) is \emph{not} a significant factor.
Based on our findings, we formalize a \emph{Shape Contract} and propose a lightweight \emph{dimension repair} pass.
\textbf{Scope of validation:} We evaluate dimension repair at the kernel level (SDPA and GEMM microbenchmarks), demonstrating 25--30\% performance recovery with only 3.7\% memory overhead.
End-to-end integration with SVD-based compression (e.g., PaLU) requires adapting the repair pass to factorized weight structures ($W = U \cdot V^T$), which we identify as future work.
\end{abstract}

\keywords{LLM Compression, GPU Optimization, Tensor Core, Memory Alignment}

\begin{document}

\maketitle

%% ===========================================
%% 1. INTRODUCTION
%% ===========================================
\section{Introduction}
\label{sec:intro}

Large Language Models (LLMs) have achieved remarkable capabilities, but their massive parameter counts pose deployment challenges.
Post-training compression techniques, including pruning and low-rank decomposition, offer promising solutions to reduce memory footprint and computational cost.
However, these techniques often produce models with \emph{irregular tensor dimensions}---values that do not align with hardware-preferred multiples (e.g., 8, 16, 32, 128).

We identify a counterintuitive phenomenon: \textbf{compressed models with fewer FLOPs can be slower than their uncompressed counterparts}.
We term this \emph{dimensional collapse}---a nonlinear performance degradation caused by misalignment between software-defined tensor shapes and hardware-fixed access patterns.

\paragraph{Motivating Example.}
Consider PaLU~\cite{palu}, a state-of-the-art low-rank compression method that reduces attention head dimensions through SVD.
Theoretical analysis of Llama-3-8B with Fisher-information-based rank allocation (0.8 retention ratio) shows the resulting \texttt{head\_dim} values would become irregular (e.g., 114--125 instead of 128).
In this scenario, 96.9\% of the compressed dimensions are not 8-aligned.\footnote{This analysis uses theoretical optimal ranks from Fisher information~\cite{palu}. Production PaLU implementations may apply quantization constraints that enforce alignment; our dimension repair addresses cases where such constraints are absent.}
Figure~\ref{fig:overview} illustrates this dimensional collapse phenomenon.
On an NVIDIA A100, this causes:
\begin{itemize}
  \item 88\% increase in SDPA latency
  \item FlashAttention internal slow path with 30--45\% overhead
  \item MEM\_EFFICIENT unavailable (strict 8-alignment)
  \item Bandwidth waste from cache misalignment
\end{itemize}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig1_overview.pdf}
\caption{Dimensional collapse in compressed LLMs. Post-training compression (e.g., PaLU) produces irregular head dimensions (114--125) that violate GPU alignment requirements, causing performance cliffs despite reduced FLOPs.}
\label{fig:overview}
\end{figure}

\paragraph{Contributions.}
This paper makes the following contributions:
\begin{enumerate}
  \item \textbf{Quantification}: We measure the performance impact of irregular dimensions across GEMM and SDPA (\S\ref{sec:phenomenon}).
  \item \textbf{Root Cause Analysis}: We identify the causes across three layers: PyTorch backend selection, CUDA kernel paths, and hardware constraints (\S\ref{sec:causes}).
  \item \textbf{Shape Contract}: We formalize dimension alignment requirements as optimization constraints (\S\ref{sec:solution}).
  \item \textbf{Dimension Repair}: We propose a lightweight post-compression pass that restores alignment (\S\ref{sec:solution}).
  \item \textbf{Evaluation}:
  \begin{itemize}
    \item \emph{Validated}: Kernel-level experiments on SDPA and GEMM microbenchmarks demonstrate 25--30\% speedup with 3.7--4.7\% memory overhead.
    \item \emph{Future Work}: End-to-end integration with SVD-based compression requires adapting to factorized weight structures ($W = U \cdot V^T$).
  \end{itemize}
  (\S\ref{sec:eval}).
\end{enumerate}

%% ===========================================
%% 2. BACKGROUND
%% ===========================================
\section{Background}
\label{sec:background}

\paragraph{Notation.}
We use $d$ to denote the attention head dimension (also written as \texttt{head\_dim} in code).
For matrix dimensions, $d_{in}$ and $d_{out}$ denote input and output dimensions of linear layers.
$B$, $S$, $H$ denote batch size, sequence length, and number of heads, respectively.

\subsection{Tensor Core Alignment}

NVIDIA Tensor Cores perform matrix-multiply-accumulate (MMA) operations on fixed tile sizes.
For FP16 on A100, the optimal tile requires $K \mod 16 = 0$.
Irregular dimensions force either padding (wasted compute) or fallback to scalar paths.

\subsection{FlashAttention Constraints}

FlashAttention-2~\cite{flashattention2} (v2.7.4) is the de facto standard for efficient attention.
Contrary to common belief, it does \emph{not} strictly require 8-aligned dimensions---it remains available for all tested dimensions (104--128).
However, it uses internal slow paths for non-8-aligned dimensions, causing 30--45\% overhead.
Optimized kernels exist for $\{32, 64, 96, 128, 256\}$. MEM\_EFFICIENT strictly requires 8-alignment.
\emph{Note: Results are specific to FlashAttention 2.7.4; future versions may implement internal alignment handling that changes these behaviors.}

\subsection{Low-Rank Compression}

PaLU~\cite{palu} compresses attention by applying SVD to K/V projections:
$W_{kv} \approx U_r \Sigma_r V_r^T$ where $r < d$.
The compressed head dimension becomes $r$, which is typically not aligned.

%% ===========================================
%% 3. DIMENSIONAL COLLAPSE PHENOMENON
%% ===========================================
\section{Dimensional Collapse}
\label{sec:phenomenon}

\subsection{Experiment Setup}

We conduct experiments on NVIDIA A100-80GB with PyTorch 2.9.1, CUDA 12.8, and FlashAttention 2.7.4.
All benchmarks use FP16 with CUDA event timing (warmup=50, measure=200, trials=3). Driver: 560.35.03; cuDNN 9.1.0.

\subsection{Compression Produces Misaligned Dimensions}

We analyze the theoretical dimension distribution from Fisher-information-based rank allocation for Llama-3-8B (retention ratio 0.8), which represents optimal SVD compression without implementation-specific constraints.
Figure~\ref{fig:palu_dist} shows the dimension distribution.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig3_palu_dist.pdf}
\caption{Distribution of head dimensions from Fisher-information-based rank allocation (Llama-3-8B, r=0.8). 96.9\% of the 512 theoretical KV head dimensions are misaligned. Only $d$=120 (3.1\%) is 8-aligned. \textbf{Note:} Production implementations may use quantization to enforce alignment; this distribution represents unconstrained SVD compression.}
\label{fig:palu_dist}
\end{figure}

\subsection{SDPA Latency vs. Head Dimension}

We sweep \texttt{head\_dim} from 64 to 160 with shape $B=4, S=2048, H=32$.
Figure~\ref{fig:sdpa_latency} shows the results.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig2_sdpa_latency.pdf}
\caption{SDPA latency across head dimensions (shaded regions show $\pm$1 std over 3 trials $\times$ 200 iterations). Clear alignment cliffs (``staircase effect'') visible at non-8-aligned values. $d$=107 shows 88\% increase vs $d$=96. The FLASH backend uses internal slow paths for misaligned dimensions.}
\label{fig:sdpa_latency}
\end{figure}

8-aligned dimensions achieve 1.1--1.6ms while non-8-aligned incur 1.6--2.2ms. \texttt{head\_dim=107} shows 2.147ms (+88\% vs 96).

\subsection{Backend Selection Behavior}

Table~\ref{tab:backend} shows latency across different SDPA backends.

\begin{table}[t]
\centering
\caption{SDPA backend latency (ms$\pm$std) for various head dimensions. MEM\_EFFICIENT fails for D=107. Measurements: 200 iterations $\times$ 3 trials.}
\label{tab:backend}
\begin{tabular}{lrrrr}
\toprule
$d$ & AUTO & FLASH & MEM\_EFF & MATH \\
\midrule
96  & 1.17{\scriptsize$\pm$.03} & 1.12{\scriptsize$\pm$.02} & 2.38{\scriptsize$\pm$.05} & 26.0{\scriptsize$\pm$.2} \\
104 & 1.54{\scriptsize$\pm$.04} & 1.54{\scriptsize$\pm$.04} & 2.75{\scriptsize$\pm$.06} & 26.5{\scriptsize$\pm$.2} \\
\textbf{107} & \textbf{2.14}{\scriptsize$\pm$.06} & \textbf{2.14}{\scriptsize$\pm$.06} & --- & \textbf{27.0}{\scriptsize$\pm$.2} \\
112 & 1.53{\scriptsize$\pm$.04} & 1.53{\scriptsize$\pm$.04} & 2.60{\scriptsize$\pm$.05} & 27.1{\scriptsize$\pm$.2} \\
128 & 1.47{\scriptsize$\pm$.03} & 1.47{\scriptsize$\pm$.03} & 2.55{\scriptsize$\pm$.05} & 28.1{\scriptsize$\pm$.2} \\
\bottomrule
\end{tabular}
\end{table}

The MATH backend is 12.6$\times$ slower than FLASH for $d$=107.
If FlashAttention cannot handle a dimension, catastrophic fallback occurs.

%% ===========================================
%% 4. ROOT CAUSE ANALYSIS
%% ===========================================
\section{Root Cause Analysis}
\label{sec:causes}

We investigate the causes of dimensional collapse across three layers.

\subsection{PyTorch Backend Selection}
\label{sec:backend}

We tested backend availability for \texttt{head\_dim} $\in$ [104, 128].
Surprisingly, FlashAttention is available for \emph{all} dimensions (100\% for both 8-aligned and non-8-aligned), while MEM\_EFFICIENT requires strict 8-alignment.
FlashAttention does \emph{not} fall back to MATH; instead, it uses internal slow paths incurring 30--45\% overhead (8-aligned: 1.55ms avg, non-8-aligned: 2.03ms avg).
The root cause lies in the CUDA kernel layer, not backend selection.

\subsection{CUDA Kernel Layer}
\label{sec:cuda}

FlashAttention's internal 30--45\% slowdown stems from: (1) vectorized loads falling back to scalar (50\% loss when $d \mod 8 \neq 0$); (2) suboptimal GEMM tile selection reducing Tensor Core utilization (30\%$\to$12\%); (3) boundary predication causing warp divergence.
FlashAttention-2 dispatches optimized kernels for $d \in \{32, 64, 96, 128, 256\}$; other values use generic kernels ($d$=128: 1.47ms, $d$=125: 1.97ms, +34\%).

\subsection{Hardware Constraints}
\label{sec:hardware}

We conduct controlled experiments (C23) to isolate hardware-level causes of dimensional collapse.
Figure~\ref{fig:root_cause} visualizes the impact of each hypothesis, and Table~\ref{tab:hardware} provides detailed metrics.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig4_root_cause.pdf}
\caption{Root cause breakdown. Tensor Core alignment (58\%), vectorized load degradation (50\%), and SDPA bandwidth (40\%) are the primary causes. L2 cache sector waste (5.8\%) is negligible.}
\label{fig:root_cause}
\end{figure}

\begin{table}[t]
\centering
\caption{Hardware layer root cause analysis (C23 experiment). Impact measured on A100 with FP16.}
\label{tab:hardware}
\small
\begin{tabular}{@{}llrl@{}}
\toprule
Hypothesis & Status & Impact & Root Cause \\
\midrule
H1: TC K\%16 & \textbf{Confirmed} & 58\% & Util. 30\%$\to$12\% \\
H2: L2 sector & Not confirmed & 5.8\% & Negligible \\
H3: SDPA BW & \textbf{Confirmed} & 40\% & Access pattern \\
H4: Vec. loads & \textbf{Confirmed} & 50\% & float4$\to$scalar \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{H1: Tensor Core Alignment (Confirmed).}
GEMM with K=16-aligned achieves 91 TFLOPS; non-aligned (K=107) drops to 37--40 TFLOPS (58\% slowdown, TC utilization 30\%$\to$12\%).

\paragraph{H2: L2 Cache Sectors (Not Confirmed).}
L2 sector waste ($\sim$5.8\%) cannot explain 30--58\% gaps; measured bandwidth is similar.

\paragraph{H3: SDPA Bandwidth Efficiency (Confirmed).}
$d$=112 achieves 153.6 GB/s; $d$=113 drops to 107.3 GB/s (--30\%). $d$=120 achieves 160.2 GB/s; $d$=121 drops to 118.5 GB/s (--26\%).

\paragraph{H4: Vectorized Loads (Confirmed).}
\texttt{float4} loads (K\%16) achieve 73--83 TFLOPS; scalar fallback (K=107) drops to 39--40 TFLOPS (50\% loss).

\textbf{Summary}: Tensor Core alignment (58\%), vectorized loads (50\%), and SDPA bandwidth (40\%) are primary causes; L2 cache (5.8\%) is negligible.

%% ===========================================
%% 5. SHAPE-AWARE COMPRESSION
%% ===========================================
\section{Shape-Aware Compression}
\label{sec:solution}

\subsection{Shape Contract}

We formalize alignment requirements as a constraint optimization problem:
\begin{equation}
\begin{aligned}
\text{minimize} \quad & \text{memory\_overhead}(d_{pad}) \\
\text{s.t.} \quad & d_{pad} \mod 8 = 0 \\
& d_{pad} \geq d_{orig}
\end{aligned}
\end{equation}

For optimal Tensor Core utilization, prefer $d_{pad} \mod 16 = 0$.

\subsection{Dimension Repair}

For a linear layer $y = Wx + b$ with $W \in \mathbb{R}^{d_{out} \times d_{in}}$, we pad the output dimension to the nearest aligned value $d'_{out} = \lceil d_{out}/a \rceil \times a$ by appending zero rows to $W$ and zeros to $b$.
The \textbf{MINIMAL} strategy uses $a=8$, while \textbf{OPTIMAL} uses $a=16$.

\paragraph{Accuracy Preservation.}
Zero-padding preserves outputs exactly: $y' = [Wx + b; \mathbf{0}]$, where the original $y$ occupies positions $[0:d_{out}]$.
For attention, zero-valued dimensions contribute nothing to scores, making padding semantically neutral.
This ensures \textbf{bit-exact output preservation}---no retraining required.

%% ===========================================
%% 6. EVALUATION
%% ===========================================
\section{Evaluation}
\label{sec:eval}

We validate dimension repair at kernel level (SDPA/GEMM microbenchmarks), demonstrating 25--30\% recovery.
E2E results show compression benefits; repair integration remains future work.

\subsection{Padding Rescue Experiment (P1)}

Table~\ref{tab:padding} shows the effect of padding $d$=107 to aligned values.

\begin{table}[t]
\centering
\caption{Padding rescue results for SDPA ($d$=107 logical). Measurements: 200 iterations $\times$ 3 trials.}
\label{tab:padding}
\small
\begin{tabular}{lrrr}
\toprule
Phys. $d$ & Mem. Ovhd. & Latency (ms$\pm$std) & Speedup \\
\midrule
107 (base) & 0\% & 2.192{\scriptsize$\pm$.07} & 1.00$\times$ \\
112 & 4.7\% & 1.523{\scriptsize$\pm$.04} & 1.44$\times$ \\
128 & 19.6\% & 1.445{\scriptsize$\pm$.04} & 1.52$\times$ \\
\bottomrule
\end{tabular}
\end{table}

Padding to 112 achieves 30.5\% speedup with only 4.7\% memory overhead---an excellent tradeoff.

\subsection{GEMM Alignment Impact}

GEMM operations show similar patterns: K=107 achieves 0.089ms latency, while K=112 and K=128 both achieve 0.050ms---a \textbf{44\% improvement} from alignment.

\subsection{Dimension Repair Validation (C4)}

We validate our dimension repair implementation on PaLU-compressed dimensions.
Figure~\ref{fig:repair_tradeoff} visualizes the speedup vs. memory overhead tradeoff for different strategies.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig5_repair_tradeoff.pdf}
\caption{Per-dimension speedup vs. memory overhead for different repair strategies. Each labeled point represents a specific dimension from the theoretical PaLU distribution: $d$=107, 114, 117, 120, 121, 125. MINIMAL (blue circles, pad to mod-8) achieves 6.9$\times$ average ROI; OPTIMAL (orange squares, pad to mod-16) provides 4.0$\times$ ROI. Note: $d$=120 (already 8-aligned) shows 0\% MINIMAL speedup, validating our hypothesis.}
\label{fig:repair_tradeoff}
\end{figure}

Table~\ref{tab:repair_memory} shows the memory overhead of different repair strategies.

\begin{table}[t]
\centering
\caption{Memory overhead analysis for PaLU dimension repair (512 KV heads).}
\label{tab:repair_memory}
\begin{tabular}{llr}
\toprule
Strategy & Alignment Target & Memory Overhead \\
\midrule
MINIMAL & mod 8 & 3.72\% \\
OPTIMAL & mod 16 & 7.20\% \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:repair_perf} shows SDPA performance for repaired dimensions.

\begin{table}[t]
\centering
\caption{SDPA latency (ms$\pm$std) before and after dimension repair ($B$=4, $S$=2048, $H$=32). Note: $d$=107 baseline (2.064$\pm$.06ms) differs from Table~\ref{tab:padding} (2.192$\pm$.07ms) due to run-to-run variance ($\sim$6\%), within normal GPU measurement variability.}
\label{tab:repair_perf}
\begin{tabular}{lrrrrr}
\toprule
$d$ & Original & Minimal & Optimal & $\Delta$Min & $\Delta$Opt \\
\midrule
107 & 2.064{\scriptsize$\pm$.06} & 1.490{\scriptsize$\pm$.04} & 1.506{\scriptsize$\pm$.04} & \textbf{+27.8\%} & +27.0\% \\
114 & 2.049{\scriptsize$\pm$.06} & 1.549{\scriptsize$\pm$.04} & 1.432{\scriptsize$\pm$.04} & +24.4\% & \textbf{+30.1\%} \\
117 & 2.054{\scriptsize$\pm$.06} & 1.567{\scriptsize$\pm$.04} & 1.433{\scriptsize$\pm$.04} & +23.7\% & \textbf{+30.2\%} \\
120 & 1.557{\scriptsize$\pm$.04} & 1.557{\scriptsize$\pm$.04} & 1.428{\scriptsize$\pm$.04} & 0\% & +8.3\% \\
121 & 1.964{\scriptsize$\pm$.05} & 1.430{\scriptsize$\pm$.04} & 1.441{\scriptsize$\pm$.04} & \textbf{+27.2\%} & +26.6\% \\
125 & 1.975{\scriptsize$\pm$.05} & 1.439{\scriptsize$\pm$.04} & 1.439{\scriptsize$\pm$.04} & \textbf{+27.1\%} & +27.1\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Findings.}
We define \emph{ROI} (Return on Investment) as the ratio of speedup percentage to memory overhead percentage: $\text{ROI} = \Delta\text{speedup} / \Delta\text{memory}$.
MINIMAL achieves 23--28\% speedup with 3.72\% overhead (6.9$\times$ ROI, i.e., $\approx$26\%/3.72\%).
$d$=120 (already 8-aligned) shows no MINIMAL improvement, validating our hypothesis.
OPTIMAL provides additional gains ($d$=120: +8.3\%) at 4.0$\times$ ROI.

\subsection{End-to-End Compression Baseline}

\noindent\textbf{Important Scope Note:} The following results demonstrate PaLU \emph{compression} benefits (reduced KV cache size), \textbf{not} the effect of our proposed dimension repair.
Integrating dimension repair into PaLU's SVD structure ($W = U \cdot V^T$) requires adapting to factorized weight representations, which remains future work.
The available PaLU checkpoints use internal quantization constraints that already enforce alignment, making this a baseline comparison.

We compare Llama-3-8B baseline vs.\ PaLU-compressed (ratio=0.7) on A100 80GB.
Figure~\ref{fig:e2e} shows prefill/decode throughput.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig6_e2e.pdf}
\caption{End-to-end LLM inference: PaLU \emph{compression} benefits (not dimension repair). The 11.5$\times$ decode speedup results from reduced KV cache size, demonstrating that compression benefits dominate despite potential misalignment overhead.}
\label{fig:e2e}
\end{figure}

\begin{table}[t]
\centering
\caption{End-to-end LLM inference: PaLU \emph{compression} benefits (not dimension repair). Measurements: 200 iterations $\times$ 3 trials.}
\label{tab:e2e}
\begin{tabular}{lrrr}
\toprule
Metric & Baseline & PaLU & $\Delta$ \\
\midrule
Prefill (tok/s) & 9870{\scriptsize$\pm$120} & 9672{\scriptsize$\pm$115} & --2.0\% \\
\textbf{Decode (tok/s)} & \textbf{119}{\scriptsize$\pm$3} & \textbf{1371}{\scriptsize$\pm$28} & \textbf{+11.5$\times$} \\
Memory (MB) & 19003 & 18896 & --0.6\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Finding.}
PaLU achieves \textbf{11.5$\times$ decode throughput} (Table~\ref{tab:e2e}) by reducing KV cache size.
This demonstrates that compression benefits dominate in memory-bound decode scenarios.
\textbf{Note:} The tested PaLU checkpoint uses internal alignment constraints, so these results do not include misalignment overhead.
For compression methods without such constraints, our kernel-level experiments (\S\ref{sec:eval}.1--3) suggest dimension repair could recover an additional 25--30\% performance.

\subsection{Accuracy Preservation}

Zero-padding guarantees \textbf{bit-exact output preservation}.
For a linear layer $y = Wx + b$, the padded output $y' = W'x + b'$ satisfies $y'[0:d_{out}] = y$---the original output is exactly preserved.
For attention, zero-valued dimensions contribute nothing to scores, making padding semantically neutral.
Unit tests confirm identical outputs (30/30 passed).

\paragraph{Limitations.}
(1) \emph{Accuracy scope}: While unit tests and theory confirm bit-exact preservation, comprehensive perplexity (WikiText-2) and downstream evaluation remain future work.
(2) \emph{E2E integration}: Kernel-level speedups (25--30\%) are not yet integrated into PaLU's SVD structure.
(3) \emph{Estimated impact}: Expected prefill (+15--25\%) and decode (+5--10\%) gains are extrapolated from kernel experiments.

%% ===========================================
%% 7. RELATED WORK
%% ===========================================
\section{Related Work}
\label{sec:related}

\paragraph{LLM Compression.}
Post-training compression methods---SparseGPT~\cite{sparsegpt} (pruning), GPTQ~\cite{gptq}/AWQ~\cite{awq} (quantization), and PaLU~\cite{palu} (low-rank)---optimize for accuracy-compression trade-offs but largely ignore hardware alignment.
Our work reveals this causes 30--58\% performance penalties.

\paragraph{KV Cache \& Attention Optimization.}
MQA~\cite{mqa}, GQA~\cite{gqa}, and StreamingLLM~\cite{streaminglm} reduce KV cache while preserving standard dimensions.
FlashAttention~\cite{flashattention,flashattention2} is tuned for $\{32, 64, 96, 128, 256\}$, with cliffs for other values.
SVD-based compression produces irregular dimensions that violate these implicit constraints.

\paragraph{Inference Frameworks.}
TensorRT~\cite{tensorrt}, vLLM~\cite{vllm}, and TGI apply runtime optimizations but assume aligned dimensions.
TensorRT's implicit padding is opaque and incurs per-inference overhead; our compile-time repair enables better kernel selection.

\paragraph{Positioning.}
Unlike prior accuracy-compression trade-off studies, \textbf{we focus on performance-alignment trade-offs}---compressed models with fewer FLOPs can run slower due to hardware misalignment.

%% ===========================================
%% 8. CONCLUSION
%% ===========================================
\section{Conclusion}
\label{sec:conclusion}

We identified \emph{dimensional collapse} as a critical but overlooked problem in LLM compression.
Root causes span three layers: FlashAttention internal slow paths (+30--45\%), Tensor Core tile alignment (58\% slowdown when K\%16$\neq$0), and vectorized load degradation (50\% throughput loss).
Notably, L2 cache waste (5.8\%) is \emph{not} significant.

Our Shape Contract and dimension repair provide a practical solution: MINIMAL strategy achieves 25--28\% kernel-level speedup with only 3.72\% memory overhead (6.9$\times$ ROI).
96.9\% of PaLU dimensions benefit; no retraining required.

End-to-end evaluation shows PaLU achieves 11.5$\times$ decode speedup despite misalignment.
Integrating repair into SVD structures ($W = U \cdot V^T$), comprehensive perplexity validation, and H100+ generalization remain future work.

%% ===========================================
%% REFERENCES
%% ===========================================
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
